./hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestImmutableScan.java:    LOG.debug("Compare all getters of scan and scanCopy.");
./hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestImmutableScan.java:        LOG.debug("Comparing return values of method: {}", method);
./hbase-client/src/main/java/org/apache/hadoop/hbase/ClientMetaTableAccessor.java:      LOG.debug("Scanning META" + " starting at row=" + Bytes.toStringBinary(scan.getStartRow()) +
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/token/ClientTokenUtil.java:        LOG.debug("Obtained token " + token.getKind().toString() + " for user " +
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSelector.java:            LOG.debug("Returning token "+ident);
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSelector.java:    LOG.debug("No matching token found");
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:          LOG.debug("Have sent token of size " + saslToken.length + " from initSASLContext.");
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:            LOG.debug("Server asks us to fall back to simple auth.");
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:          LOG.debug("Will read input token of size " + saslToken.length
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:            LOG.debug("Will send token of size " + saslToken.length + " from initSASLContext.");
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:            LOG.debug("Will read input token of size " + saslToken.length
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:          LOG.debug("Sasl connection failed: ", e);
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:        LOG.debug("SASL client context established. Negotiated QoP: "
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:      LOG.debug("reading next wrapped RPC packet");
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:        LOG.debug("unwrapping token of length:" + rpcBuf.length);
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java:        LOG.debug("wrapping token of length:" + len);
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/EncryptionUtil.java:      LOG.debug(msg);
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/EncryptionUtil.java:        LOG.debug("Unable to unwrap key with current master key '" + masterKeyName + "'");
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/GssSaslClientAuthenticationProvider.java:    LOG.debug("Setting up Kerberos RPC to server={}", serverPrincipal);
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslClientAuthenticationProvider.java:        LOG.debug("SASL client callback: setting username: {}", userName);
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslClientAuthenticationProvider.java:        LOG.debug("SASL client callback: setting userPassword");
./hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslClientAuthenticationProvider.java:        LOG.debug("SASL client callback: setting realm: {}", rc.getDefaultText());
./hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java:        LOG.debug("Skip exceedThrottleQuota row-key when parse quota result");
./hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java:    LOG.debug(
./hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java:      LOG.debug("Close zookeeper connection {} to {}", getId(), connectString);
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcDuplexHandler.java:        LOG.debug("Unknown callId: " + id + ", skipping over this response of " + whatIsLeftToRead
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcConnection.java:    LOG.debug("Using {} authentication for service={}, sasl={}",
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java:            LOG.debug("call write error for {}", call.toShortString());
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java:          LOG.debug("Received exception in connection setup.\n" +
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java:          LOG.debug("Received exception in connection setup.\n" +
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java:          LOG.debug("Exception encountered while connecting to the server", ex);
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java:        LOG.debug("Not trying to connect to " + remoteId.getAddress()
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java:        LOG.debug("Connecting to " + remoteId.getAddress());
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java:        LOG.debug("Length of response for connection header:" + readSize);
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java:      LOG.debug("Codec=" + this.codec + ", compressor=" + this.compressor + ", tcpKeepAlive="
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java:        LOG.debug("Not trying to connect to " + remoteId.getAddress()
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java:      LOG.debug("Stopping rpc client");
./hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/FailedServers.java:      LOG.debug(
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java:        LOG.debug("Start fetching {} from registry", type);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java:            LOG.debug("Failed to fetch {} from registry", type, error);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java:          LOG.debug("The fetched {} is {}", type, value);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java:    LOG.debug("Add entry to stale cache for table {} with startKey {}, {}",
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java:      LOG.debug("Entry for table {} with startKey {}, {} times out", tablename, entry.getKey(),
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java:      LOG.debug("Lookup {} goes to primary region", row);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java:        LOG.debug("Lookup {} goes to primary meta", row);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java:        LOG.debug("Lookup {} goes to primary meta", row);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java:          LOG.debug("clean entry {}, {} from stale cache", entry.getKey(), entry.getValue());
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java:    LOG.debug("Refreshed replica count {}", newNumOfReplicas);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClusterStatusListener.java:      LOG.debug("Channel bindAddress={}, networkInterface={}, INA={}", bindAddress, ni, ina);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:      LOG.debug("The future is already done, canceled={}, give up retrying", future.isCancelled());
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorOverAsyncBufferedMutator.java:      LOG.debug("Flush failed, you should get an exception thrown to your code", e);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java:          LOG.debug("The newnly fetch region {} is different from the old one {} for row '{}'," +
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java:      LOG.debug("The fetched location of '{}', row='{}', locateType={} is {}", tableName,
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java:      LOG.debug("table={}, state={}", state.getTableName(), state.getState(), se);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java:        nameOrEncodedName2State.forEach((k, v) -> LOG.debug("region={}, state={}", k, v));
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java:      LOG.debug(toCommaDelimitedString(encodedRegionNames), se);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java:      LOG.debug(toCommaDelimitedString(encodedRegionNames), se);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java:      LOG.debug(toCommaDelimitedString(
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java:      LOG.debug("Failed to run HBCK chore", se);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java:      LOG.debug("Try updating {} , the old value is {}, error={}", loc, oldLoc,
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java:      LOG.debug("The actual exception when updating {} is {}", loc,
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java:      LOG.debug("Will not update {} because the exception is null or not the one we care about",
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java:      LOG.debug("Try updating {} with the new location {} constructed by {}", loc, newLoc,
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java:      LOG.debug("Try removing {} from cache", loc);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java:    LOG.debug(stackBuilder.toString());
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java:          LOG.debug("The fetched master address is {}", addr);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/MasterAddressRefresher.java:          LOG.debug("Attempting to refresh master address end points.");
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/MasterAddressRefresher.java:          LOG.debug("Finished refreshing master end points. {}", newMasters);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/MasterAddressRefresher.java:          LOG.debug("Interrupted during wait, aborting refresh-masters-thread.", e);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/MasterAddressRefresher.java:          LOG.debug("Error populating latest list of masters.", e);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocator.java:    LOG.debug("Clear meta cache for {}", tableName);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocator.java:    LOG.debug("Clear meta cache for {}", serverName);
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java:      LOG.debug(String.format("0x%x", System.identityHashCode(this)) +
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java:      LOG.debug(String.format("0x%x", System.identityHashCode(this)) + " resume prefetching");
./hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java:                LOG.debug("Table " + tableName + " has " + notDeployedRegions.size() + " regions");
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/util/RecoverLeaseFSUtils.java:                LOG.debug("isFileClosed not available");
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:      LOG.debug("No decryptEncryptedDataEncryptionKey method in DFSClient," +
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:          LOG.debug("Found relocated ByteString class from hadoop-thirdparty." +
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:          LOG.debug("Did not find relocated ByteString class from hadoop-thirdparty." +
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:          LOG.debug("Shaded LiteralByteString from hadoop-thirdparty is found.");
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:            LOG.debug("com.google.protobuf.LiteralByteString found.");
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:      LOG.debug(
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:        LOG.debug(
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:        LOG.debug("SASL client skipping handshake in unsecured configuration for addr = " + addr
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:        LOG.debug("SASL client skipping handshake in secured configuration with "
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:        LOG.debug("SASL client skipping handshake in secured configuration with "
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:        LOG.debug(
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java:        LOG.debug("SASL client skipping handshake in secured configuration with no SASL "
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java:      LOG.debug("Hadoop 3.3 and above shades protobuf.");
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java:      LOG.debug("Hadoop 3.2 and below use unshaded protobuf.", e);
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java:      LOG.debug("ClientProtocol::create wrong number of arguments, should be hadoop 3.2 or below");
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java:      LOG.debug("ClientProtocol::create wrong number of arguments, should be hadoop 2.x");
./hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java:      LOG.debug("can not find SHOULD_REPLICATE flag, should be hadoop 2.x", e);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestRemoteBackup.java:        LOG.debug("Wrote " + NB_ROWS_IN_FAM3 + " rows into family3");
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:        LOG.debug("Merge backup images " + bids);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:          LOG.debug("Merge Job finished:" + result);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:          LOG.debug("Renamed "+ backupDirPath +" to "+ tmpBackupDir);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:    LOG.debug("writing " + ADD_ROWS + " rows to " + table1);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:    LOG.debug("written " + ADD_ROWS + " rows to " + table1);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:    LOG.debug("written " + ADD_ROWS + " rows to " + table2);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:        LOG.debug("Expected :"+ e.getMessage());
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:    LOG.debug("After incremental restore: " + hTable.getDescriptor());
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupMergeWithFailures.java:    LOG.debug("f1 has " + TEST_UTIL.countRows(hTable, famName) + " rows");
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("writing " + ADD_ROWS + " rows to " + table1);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("written " + ADD_ROWS + " rows to " + table1);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("written " + NB_ROWS_MOB + " rows to " + table1 + " to Mob enabled CF");
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("written " + 5 + " rows to " + table2);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:        LOG.debug("region is not splittable, because " + e);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("split finished in =" + (endSplitTime - startSplitTime));
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("Restoring full " + backupIdFull);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("After incremental restore: " + hTable.getDescriptor());
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("f1 has " + countFamName + " rows");
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("f2 has " + countFam2Name + " rows");
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackup.java:      LOG.debug("mob has " + countMobName + " rows");
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithBulkLoad.java:    LOG.debug("bulk loading into " + testName);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithBulkLoad.java:    LOG.debug("bulk loading into " + testName);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java:    LOG.debug("writing " + ADD_ROWS + " rows to " + table1);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java:    LOG.debug("written " + ADD_ROWS + " rows to " + table1);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java:    LOG.debug("written " + ADD_ROWS + " rows to " + table2);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java:    LOG.debug("After incremental restore: " + hTable.getDescriptor());
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupMerge.java:    LOG.debug("f1 has " + countRows + " rows");
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupBase.java:        LOG.debug("For incremental backup, current table set is "
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupBase.java:        LOG.debug("snapshot copy for " + backupId);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupBase.java:      LOG.debug(Objects.toString(it.next().getPath()));
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/master/TestBackupLogCleaner.java:      LOG.debug("WAL list after full backup");
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/master/TestBackupLogCleaner.java:      LOG.debug("+++WAL: " + fs.getPath().toString());
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestBackupStatusProgress.java:    LOG.debug(info.getShortDescription());
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithFailures.java:    LOG.debug("writing " + ADD_ROWS + " rows to " + table1);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithFailures.java:    LOG.debug("written " + ADD_ROWS + " rows to " + table1);
./hbase-backup/src/test/java/org/apache/hadoop/hbase/backup/TestIncrementalBackupWithFailures.java:    LOG.debug("written " + 5 + " rows to " + table2);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:      LOG.debug("Backup session " + backupInfo.getBackupId() + " has been started.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:    LOG.debug("Trying to delete snapshot for full backup.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:      LOG.debug("Trying to delete snapshot: " + snapshotName);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:      LOG.debug("Deleting the snapshot " + snapshotName + " for backup " + backupInfo.getBackupId()
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:        LOG.debug("Delete log files of exporting snapshot: " + file.getPath().getName());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:      LOG.debug("Trying to cleanup up target dir. Current backup phase: "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:            LOG.debug("Cleaning up uncompleted backup data at " + targetDirPath.toString()
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:            LOG.debug("No data has been copied to " + targetDirPath.toString() + ".");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:            LOG.debug(tableDir.toString() + " is empty, remove it.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:        LOG.debug("Delete log files of DistCp: " + file.getPath().getName());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java:      LOG.debug("Backup " + backupInfo.getBackupId() + " finished: " + backupCompleteData);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java:          LOG.debug("Delete backup info " + p + " for " + backupInfo.getBackupId());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java:        LOG.debug(numDeleted + " bulk loaded files out of " + map.size() + " were deleted");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java:    LOG.debug("Remove " + tn + " from " + info.getBackupId() + " tables="
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java:        LOG.debug("Delete backup info " + info.getBackupId());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java:    LOG.debug("GetAffectedBackupInfos for: " + backupInfo.getBackupId() + " table=" + tn);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java:          LOG.debug("GetAffectedBackupInfos for: " + backupInfo.getBackupId() + " table=" + tn
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/FullTableBackupClient.java:    LOG.debug("There are " + (int) numOfSnapshots + " snapshots to be copied.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/FullTableBackupClient.java:        LOG.debug("Setting snapshot copy job name to : " + jobname);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/FullTableBackupClient.java:      LOG.debug("Copy snapshot " + args[1] + " to " + args[3]);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/FullTableBackupClient.java:      LOG.debug("snapshot copy for " + backupId);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java:              LOG.debug("copying archive " + archive + " to " + tgt);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java:    LOG.debug(newlyArchived.size() + " files have been archived.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java:      LOG.debug("For incremental backup, current table set is "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java:      LOG.debug("Incremental copy HFiles is starting. dest=" + backupDest);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java:        LOG.debug("Setting incremental copy HFiles job name to : " + jobname);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java:      LOG.debug("Incremental copy HFiles from " + StringUtils.join(files, ',')
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:      LOG.debug("Added log cleaner: {}. Added master procedure manager: {}."
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:      LOG.debug("Added region procedure manager: {}. Added region observer: {}",
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:    LOG.debug("Getting the direct ancestors of the current backup {}", backupInfo.getBackupId());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:      LOG.debug("Current backup is a full backup, no direct ancestor for it.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:          LOG.debug("Met the backup boundary of the current table set:");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:            LOG.debug("  BackupID={}, BackupDir={}", image1.getBackupId(),  image1.getRootDir());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:          LOG.debug("Current backup has an incremental backup ancestor, "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:          LOG.debug(
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java:    LOG.debug("Got {} ancestors for the current backup.", ancestors.size());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java:      LOG.debug("Loading manifest from: " + backupPath.toString());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java:          LOG.debug("Loaded manifest instance from manifest file: "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java:    LOG.debug("Backup image " + image1.getBackupId() + " can cover " + image2.getBackupId());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java:    LOG.debug("Full image set can cover image " + image.getBackupId());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:      LOG.debug("StartCode " + savedStartCode + "for backupID " + backupInfo.getBackupId());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:      LOG.debug("StartCode " + savedStartCode + "for backupID " + backupInfo.getBackupId());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:    LOG.debug("In getLogFilesForNewBackup()\n" + "olderTimestamps: " + olderTimestamps
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:        LOG.debug("currentLogFile: " + log.getPath().toString());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:            LOG.debug("Skip hbase:meta log file: " + log.getPath().getName());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:          LOG.debug("Known hosts (from newestTimestamps):");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:            LOG.debug(s);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:          LOG.debug("Skip .meta log file: " + currentLogFile);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalBackupManager.java:          LOG.debug("Skip .meta log file: " + path.getName());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java:    LOG.debug("restoreStage finished");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:    LOG.debug("Backup table {} is not present and available, waiting for it to become so",
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:    LOG.debug("Backup table {} exists and available", tableName);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:          LOG.debug("found bulk loaded file : " + tbl + " " + Bytes.toString(fam) + " " + path);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:      LOG.debug("write bulk load descriptor to backup " + tabName + " with " + finalPaths.size()
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:      LOG.debug("written " + puts.size() + " rows for bulk load of " + tabName);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:      LOG.debug(
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:      LOG.debug("written " + puts.size() + " rows for bulk load of " + tabName);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:        LOG.debug("orig deleting the row: " + Bytes.toString(row));
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:      LOG.debug("deleted " + rows.size() + " original bulkload rows");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:          LOG.debug("found orig " + path + " for " + fam + " of table " + region);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:    LOG.debug("Start new backup exclusive operation");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:    LOG.debug("Finish backup exclusive operation");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:      tables.forEach(table -> LOG.debug(Objects.toString(table)));
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:      files.forEach(file -> LOG.debug("add :" + file));
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:        LOG.debug(
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:    LOG.debug("Restoring " + BackupSystemTable.getTableNameAsString(conf) + " from snapshot");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:        LOG.debug("Done restoring backup system table");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:    LOG.debug("Deleting " + BackupSystemTable.getSnapshotName(conf) + " from the system");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:        LOG.debug("Done deleting backup system table snapshot");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:      LOG.debug("writing raw bulk path " + file + " for " + table + " " + Bytes.toString(region));
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java:    LOG.debug("bulk row string " + rowStr + " region " + parts[idx]);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java:        LOG.debug("Attempting to copy table info for:" + table + " target: " + target +
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java:        LOG.debug("Finished copying tableinfo.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java:        LOG.debug("Starting to write region info for table " + table);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java:        LOG.debug("Finished writing region info for table " + table);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java:      LOG.debug("Delete log files: " + file.getPath().getName());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java:      LOG.debug("Trying to cleanup up target dir : " + backupInfo.getBackupId());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java:          LOG.debug(tableDir.toString() + " is empty, remove it.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java:      LOG.debug("Folder tableArchivePath: " + tableArchivePath.toString() + " does not exists");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java:        LOG.debug("Found descriptor " + tableDescriptor + " through " + incrBackupId);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java:      LOG.debug("Retrieved descriptor: " + tableDescriptor + " thru " + lastIncrBackupId);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java:          LOG.debug("Found no table descriptor in the snapshot dir, previous schema would be lost");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java:          LOG.debug("find table descriptor but no archive dir for table " + tableName
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java:      LOG.debug("Parsing region dir: " + regionDir);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java:        LOG.debug("Parsing family dir [" + familyDir.toString() + " in region [" + regionDir + "]");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java:            LOG.debug("Trying to figure out region boundaries hfile=" + hfile + " first="
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupHFileCleaner.java:        LOG.debug("Got " + ioe + " when closing connection");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/regionserver/LogRollBackupSubprocedurePool.java:    LOG.debug("Waiting for backup procedure to finish.");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/regionserver/LogRollBackupSubprocedure.java:        LOG.debug("DRPC started: " + rss.getServerName());
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/regionserver/LogRollBackupSubprocedure.java:      LOG.debug("log roll took " + (EnvironmentEdgeManager.currentTime() - start));
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java:      LOG.debug("Backup is not enabled. Check your {} setting",
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java:          LOG.debug("Found log file in backup system table, deleting: {}", wal);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java:          LOG.debug("Did not find this log in backup system table, keeping: {}", wal);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java:      LOG.debug("Found -overwrite option in restore command, "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java:      LOG.debug("Found -check option in restore command, "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupObserver.java:      LOG.debug("skipping recording bulk load in postBulkLoadHFile since backup is disabled");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupObserver.java:      LOG.debug("skipping recording bulk load in preCommitStoreFile since backup is disabled");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java:      LOG.debug("add incremental job :" + hfileOutPath + " from " + inputDirs);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java:      LOG.debug("success configuring load incremental job");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:    LOG.debug("Backup progress data \"" + backupProgressData
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:            LOG.debug("Backup progress data updated to backup system table: \"Progress: "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:        LOG.debug("Backup progress data updated to backup system table: \"Progress: "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:      LOG.debug("DistCp job-id: " + jobID + " completed: " + job.isComplete() + " "
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:      LOG.debug(Objects.toString(ctrs));
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:        LOG.debug("Doing SNAPSHOT_COPY");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:        LOG.debug("Doing COPY_TYPE_DISTCP");
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:        LOG.debug("DistCp options: " + Arrays.toString(options));
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java:      LOG.debug("Killed copy job " + id);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceRestoreJob.java:      LOG.debug("Restore " + (fullBackupRestore ? "full" : "incremental")
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceRestoreJob.java:            LOG.debug("Restoring HFiles from directory " + bulkOutputPath);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceRestoreJob.java:        LOG.debug("Restore Job finished:" + result);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java:      LOG.debug("Merge backup images " + bids);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java:        LOG.debug("Merge Job finished:" + result);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java:        LOG.debug("Renamed "+ backupDirPath +" to "+ tmpBackupDir);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java:        LOG.debug("MoveData from "+ fst.getPath() +" to "+ dest+" result="+ result);
./hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java:          LOG.debug("File: " + fileBackupDirPath + " does not exist.");
./hbase-replication/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationStateBasic.java:        LOG.debug("ConnectedPeerStatus was " + !status + " but expected " + status
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java:    LOG.debug("Found dead servers:" + result);
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:        LOG.debug("Creating {} with data {}", wal, Bytes.toStringBinary(logOffset));
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:          LOG.debug("Didn't find a RegionServer that replicates, won't prevent deletions.");
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:        LOG.debug("Peer {} not found in hfile reference queue.", peerNode);
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:    LOG.debug("Adding hfile references {} in queue {}", pairs, peerNode);
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:    LOG.debug("The multi list size for adding hfile references in zk for node {} is {}",
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:    LOG.debug("Removing hfile references {} from queue {}", files, peerNode);
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:    LOG.debug("The multi list size for removing hfile references in zk for node {} is {}",
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:          LOG.debug("Didn't find any peers with hfile references, won't prevent deletions.");
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java:        LOG.debug("Replication hfile references node cversion changed from %d to %d, retry = %d",
./hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationUtils.java:      LOG.debug("Interrupted while sleeping between retries");
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java:    LOG.debug("Current leader index is "+currentLeader.getIndex());
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java:    LOG.debug("Stored leader index in ZK is "+storedIndex);
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java:    LOG.debug("New leader index is "+currentLeader.getIndex());
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java:    LOG.debug("Stored leader index in ZK is "+storedIndex);
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java:    LOG.debug("New leader index is "+currentLeader.getIndex());
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java:    LOG.debug("Stored leader index in ZK is "+storedIndex);
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKUtil.java:        LOG.debug(e.toString(), e);
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKNodeTracker.java:        LOG.debug("nodeDeleted(" + path + ")");
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKNodeTracker.java:        LOG.debug("nodeCreated(" + path + ")");
./hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKNodeTracker.java:        LOG.debug("nodeDataChanged(" + path + ")");
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java:          LOG.debug("Try starting again because there is no data from {}", node);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java:          LOG.debug("Node {} now exists, resetting a watcher", node);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java:                LOG.debug("Node " + path + " already deleted. Assuming a " +
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java:              LOG.debug("Node {} already deleted, retry={}", path, isRetry);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java:    LOG.debug("Retry, connectivity issue (JVM Pause?); quorum={},exception{}=", quorumServers, e);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/DeletionListener.java:      LOG.debug("Processing delete on {}", pathToWatch);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:          LOG.debug("Encountered InterruptedException when closing {}", this.recoverableZooKeeper);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:      LOG.debug("Checking znode ACLs");
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:        LOG.debug("ACL is empty");
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:            LOG.debug(String.format("permissions for '%s' are not correct: have 0x%x, want 0x%x",
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:            LOG.debug(String.format("permissions for '%s' are not correct: have 0x%x, want 0x%x",
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:              LOG.debug(String.format("permissions for '%s' are not correct: have 0x%x, want 0x%x",
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:            LOG.debug("Unexpected shortname in SASL ACL: {}", id);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:          LOG.debug("unexpected ACL id '{}'", id);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:                LOG.debug(String.format(
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:    LOG.debug(prefix("Received ZooKeeper Event, " +
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:        LOG.debug("{} connected", this.identifier);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:        LOG.debug(prefix("Received Disconnected from ZooKeeper, ignoring"));
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:        LOG.debug(prefix("ZooKeeper client closed, ignoring"));
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:      LOG.debug("ZK sync() operation took {}ms", EnvironmentEdgeManager.currentTime() - startTime);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java:    LOG.debug(prefix("Received InterruptedException, will interrupt current thread"
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:        LOG.debug(zkw.prefix("Set watcher on existing znode=" + znode));
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:        LOG.debug(zkw.prefix("Set watcher on znode that does not yet exist, " + znode));
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:      LOG.debug(zkw.prefix("Unable to list children of znode " + znode + " " +
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:      LOG.debug(zkw.prefix("Unable to list children of znode " + znode +
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:      LOG.debug(zkw.prefix("Unable to get data of znode " + znode + " " +
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:      LOG.debug(zkw.prefix("Unable to get data of znode " + znode + " " +
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:        LOG.debug("Could not acquire current User.", e);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:    LOG.debug("Current zk system:");
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:    LOG.debug(prefix + root);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java:      LOG.debug(prefix + child);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java:          LOG.debug("Failed binding ZK Server to client port: " + currentClientPort, e);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java:          LOG.debug("Read {}", result);
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java:        LOG.debug("hbase:meta region location doesn't exist, create it");
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java:        LOG.debug("hbase:meta region location doesn't exist for replicaId=" + replicaId +
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java:            LOG.debug("Claimed the leader znode as '"+
./hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java:            LOG.debug("Interrupted waiting on leader", ie);
./hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestCallQueue.java:    LOG.debug("elementsAdded:" + elementsAdded +
./hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServerCmdLine.java:    LOG.debug(getParametersString());
./hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServerCmdLine.java:        LOG.debug("Stopping " + this.implType.simpleClassName() + " Thrift server");
./hbase-thrift/src/test/java/org/apache/hadoop/hbase/thrift/HBaseThriftTestingUtility.java:    LOG.debug("Stopping Thrift Server");
./hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java:    LOG.debug("scannerClose: id=" + scannerId);
./hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java:      LOG.debug("Using framed transport");
./hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java:      LOG.debug("Using compact protocol");
./hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java:      LOG.debug("Using binary protocol");
./hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java:        LOG.debug("Web UI port set to " + val);
./hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java:      LOG.debug("deleteTable: table={}", tableName);
./hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java:    LOG.debug("scannerClose: id={}", id);
./hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java:    LOG.debug("scannerGetList: id={}", id);
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/ClassLoaderTestHelper.java:    LOG.debug("Setting classpath to: " + classpath);
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/LoadTestKVGenerator.java:      LOG.debug("verify failed, expected value: " + Bytes.toStringBinary(expectedData)
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestThreads.java:        LOG.debug("Sleeper thread: sleeping for " + SLEEP_TIME_MS);
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestThreads.java:        LOG.debug("Sleeper thread: finished sleeping");
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestThreads.java:    LOG.debug("Starting sleeper thread (" + SLEEP_TIME_MS + " ms)");
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestThreads.java:    LOG.debug("Main thread: sleeping for 200 ms");
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestThreads.java:    LOG.debug("Interrupting the sleeper thread and sleeping for 500 ms");
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestThreads.java:    LOG.debug("Interrupting the sleeper thread and sleeping for 800 ms");
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestThreads.java:    LOG.debug("Interrupting the sleeper thread again");
./hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestThreads.java:    LOG.debug("Target sleep time: " + SLEEP_TIME_MS + ", time elapsed: " +
./hbase-common/src/test/java/org/apache/hadoop/hbase/ClassFinder.java:        LOG.debug("Looking in " + resourcePath + "; isJar=" + isJar);
./hbase-common/src/test/java/org/apache/hadoop/hbase/ClassFinder.java:      LOG.debug("Failed to instantiate or check " + className + ": " + exception);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java:      LOG.debug("Set storagePolicy={} for path={}", storagePolicy, path);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java:        LOG.debug("Unable to set storagePolicy=" + storagePolicy + " for path=" + path, e);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java:          LOG.debug("The underlying FileSystem implementation doesn't support " +
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java:        LOG.debug("{} not available, will not set replicate when creating output stream", builderName);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java:          LOG.debug("Using builder API via reflection for DFS file creation.");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java:          LOG.debug("Could not find replicate method on builder; will not set replicate when" +
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/PrettyPrinter.java:      LOG.debug("Given interval value is not a number, parsing for human readable format");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/PrettyPrinter.java:      LOG.debug("Given size value is not a number, parsing for human readable format");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java:        LOG.debug("Class {} not found - using dynamical class loader", name);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java:        LOG.debug("Not checking DynamicClassLoader for missing class because it is disabled."
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java:          LOG.debug("Class {} already loaded", name);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java:            LOG.debug("Finding class: {}", name);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java:            LOG.debug("Loading new jar files, if any");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java:            LOG.debug("Finding class again: {}", name);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java:          LOG.debug("Ignored non-jar file {}", fileName);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java:      LOG.debug("Found classloader "+ cl + " for "+ pathStr);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java:        LOG.debug("Found classloader "+ cl + " for "+ pathStr);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java:        LOG.debug("Skipping exempt class " + name +
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java:          LOG.debug("Class " + name + " already loaded");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java:            LOG.debug("Finding class: " + name);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java:            LOG.debug("Class " + name + " not found - delegating to parent");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java:              LOG.debug("Class " + name + " not found in parent loader");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java:        LOG.debug("Checking parent first for resource " + name);
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java:      LOG.debug("Using Unsafe to estimate memory layout");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java:    LOG.debug("Not using Unsafe to estimate memory layout");
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java:              LOG.debug("" + index + " " + aField.getName() + " " + aField.getType());
./hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java:        LOG.debug("Primitives=" + coeff[0] + ", arrays=" + coeff[1] +
./hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java:          LOG.debug("Unable to decrypt data with current cipher algorithm '"
./hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java:        LOG.debug("Installed " + providerClassName + " into key provider cache");
./hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java:      LOG.debug(e.toString(), e);
./hbase-common/src/main/java/org/apache/hadoop/hbase/ScheduledChore.java:          LOG.debug("{} execution time: {} ms.", getName(),
./hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java:      LOG.debug("ClassNotFound: ConfServlet");
./hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java:        LOG.debug(String.format("Config option \"%s\" was found through" +
./hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java:        LOG.debug(String.format(
./hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java:      LOG.debug(String.format(
./hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RefreshHFilesEndpoint.java:        LOG.debug("Refreshing HFiles for region: " + store.getRegionInfo().getRegionNameAsString() +
./hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/RefreshHFilesClient.java:    LOG.debug("Done refreshing HFiles");
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java:      LOG.debug("execute procedure " + this + " step=" + step);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java:      LOG.debug("execute procedure " + this + " step=" + step);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java:      LOG.debug("rollback procedure " + this + " step=" + step);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java:          LOG.debug("log file " + file.getPath() + " size=" + file.getLen());
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java:        LOG.debug("no files under: " + logDir);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureBypass.java:        LOG.debug("Sleep is interrupted.", t);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureExecutor.java:      LOG.debug("waiting for thread count=" + expectedThreads +
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureNonce.java:      LOG.debug("execute procedure " + this + " step=" + step);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java:        LOG.debug("THROW INTERRUPT");
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java:      LOG.debug(getProcId() + " rollback state " + state + " ts=" + ts);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java:        LOG.debug("THROW INTERRUPT");
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java:          LOG.debug("loading completed procId=" + procId + ": " + proc);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java:          LOG.debug("loading runnable procId=" + procId + ": " + proc);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java:        LOG.debug("corrupted procId=" + proc.getProcId() + ": " + proc);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSchedulerConcurrency.java:              LOG.debug("WAKE BATCH " + ev[i] + " total=" + wakeCount.get());
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSchedulerConcurrency.java:              LOG.debug("WAKE " + ev + " total=" + wakeCount.get());
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSchedulerConcurrency.java:            LOG.debug("WAIT " + proc.getEvent());
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java:      LOG.debug("EXEC LIST: " + execList);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestWALProcedureStore.java:            LOG.debug("Simulate FileNotFound at count=" + count + " for " + path);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestWALProcedureStore.java:          LOG.debug("Simulate recoverFileLease() at count=" + count + " for " + path);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestWALProcedureStore.java:    LOG.debug("corrupt log " + logFile.getPath() +
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestWALProcedureStore.java:    LOG.debug("expected: " + procIds);
./hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestProcedureStoreTracker.java:        LOG.debug("loading " + numProcs + " procs from start=" + start);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/AbstractProcedureScheduler.java:        LOG.debug("the scheduler is not running");
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java:    LOG.debug("Abort requested for {}", this);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java:      LOG.debug("{} didn't hold the lock before restarting, skip acquiring lock.", this);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java:      LOG.debug("{} is already finished, skip acquiring lock.", this);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java:      LOG.debug("{} is already bypassed, skip acquiring lock.", this);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java:      LOG.debug("{} is in WAITING STATE, and holdLock=false, skip acquiring lock.", this);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java:    LOG.debug("{} held the lock before restarting, call acquireLock to restore it.", this);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:          LOG.debug("Procedure {} has already been finished and parent is succeeded," +
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:          LOG.debug("No pending procedure with id = {}, skip force updating.", procId);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:          LOG.debug("Procedure {} has already been finished and expired, skip force updating",
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("Force update procedure {}", proc);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:        LOG.debug("Completed {}", proc);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("Loading {}", proc);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("Procedure pid={} does not exist, skipping bypass", pid);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:    LOG.debug("Begin bypass {} with lockWait={}, override={}, recursive={}",
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("Waited {} ms, but {} is still running, skipping bypass with force={}",
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("Waited {} ms, but {} is still running, begin bypass with force={}",
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:        LOG.debug("{} is already finished, skipping bypass", procedure);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:          LOG.debug("{} has children, skipping bypass", procedure);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:        LOG.debug("Bypassing procedures in RUNNABLE, WAITING and WAITING_TIMEOUT states "
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:        LOG.debug("Bypassing {}", current);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:        LOG.debug("transform procedure {} from WAITING_TIMEOUT to RUNNABLE", procedure);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:          LOG.debug("removed procedure {} from timeoutExecutor", procedure);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:        LOG.debug("Bypassing {} and its ancestors successfully, adding to queue", procedure);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:        LOG.debug("Bypassing {} and its ancestors successfully, but since it is already running, "
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:    LOG.debug("Stored {}", proc);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("Stored " + Arrays.toString(procs));
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("pid={} already removed by the cleaner.", procId);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("{} is already finished, skipping execution", proc);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:          LOG.debug(lockState + " " + proc);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug("Roll back attempt failed for {}", proc, e);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:      LOG.debug(msg);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:    LOG.debug(msg);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java:        LOG.debug("Added new worker thread {}", worker);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:        LOG.debug("Created Procedure Store WAL archive dir {}", this.walArchiveDir);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:      LOG.debug("Starting WAL Procedure Store lease recovery");
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:          LOG.debug("Someone else has already created log {}. Retrying.", flushLogId);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:          LOG.debug("Someone else created new logs. Expected maxLogId < {}", flushLogId);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:        LOG.debug("Lease acquired for flushLogId={}", flushLogId);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:        LOG.debug("No state logs to replay.");
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:      LOG.debug("WALs cleanup on load is not enabled: " + getActiveLogs());
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:      LOG.debug("Removed log={}, activeLogs={}", log, logs);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java:    LOG.debug("Opening Pv2 {}", logFile);
./hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/ProcedureTree.java:      LOG.debug("Procedure {} stack ids={}", entry, entry.proc.getStackIdList());
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java:    LOG.debug("row keyvalues:" + stringifyKvs(table.get(get).listCells()));
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java:    LOG.debug("row keyvalues:" + stringifyKvs(table.get(get).listCells()));
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java:      LOG.debug("We failed " + failureNumber + " times during test");
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java:    LOG.debug("row keyvalues:" +
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java:    LOG.debug("row2 keyvalues:" +
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java:      LOG.debug("We failed " + failureNumber + " times during test");
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java:          LOG.debug("failed comparison: actual: " +
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java:      LOG.debug("retrying after failed comparison: " + i);
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java:            LOG.debug("Default response is " + TestProtos.EchoRequestProto.getDefaultInstance());
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java:            LOG.debug("Batch.Call returning result " + response);
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java:            LOG.debug("Default response is " + TestProtos.EchoRequestProto.getDefaultInstance());
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java:            LOG.debug("Batch.Call returning result " + response);
./hbase-endpoint/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java:              LOG.debug("Batch.Call got result " + response);
./hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java:      LOG.debug("Create the region {} with favored nodes {}", regionInfo.getRegionNameAsString(),
./hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java:          LOG.debug("Place the secondary and tertiary region server for region "
./hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java:          LOG.debug("Place the secondary and tertiary region server for region "
./hbase-balancer/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java:        LOG.debug("Got exception in closing the meta scanner visitor", t);
./hbase-balancer/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java:      LOG.debug("Added region {}", regionInfo.getRegionNameAsString());
./hbase-balancer/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java:    METALOG.debug("{} {}", p.getClass().getSimpleName(), p.toJSON());
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("FS state before snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("Snapshot completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("FS state after snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("FS state before snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("Snapshot completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("FS state after snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("FS state before snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("Snapshot completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("FS state after snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java:    LOG.debug("------- Starting Snapshot test -------------");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotDescriptionUtils.java:      LOG.debug("Correctly failed when snapshot doesn't have a tablename");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreSnapshotHelper.java:      LOG.debug("get reference name for file " + refFile + " = " + refPath);
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreSnapshotHelper.java:    LOG.debug("pre-restore table=" + htdClone.getTableName() + " snapshot=" + snapshotDir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreSnapshotHelper.java:    LOG.debug("post-restore table=" + htdClone.getTableName() + " snapshot=" + snapshotDir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestRegionSnapshotTask.java:          LOG.debug("Introducing delay before adding store file to manifest");
./hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotWhenChoreCleaning.java:            LOG.debug("toDeleteFiles[{}] is: {}", i, deletableFiles.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionMetrics.java:      LOG.debug("serverName=" + serverName + ", getRegionLoads=" +
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionMetrics.java:      LOG.debug("serverName=" + serverName + ", regionLoads=" +
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover2.java:      LOG.debug("Unloading {}", regionServer.getServerName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover2.java:      LOG.debug("Successfully Unloaded, now Loading");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover2.java:      LOG.debug("Unloading {}", regionServer.getServerName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover2.java:      LOG.debug("Successfully Unloaded, now Loading");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover2.java:      LOG.debug("Unloading {}", regionServer.getServerName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover2.java:      LOG.debug("Successfully Unloaded, now Loading");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReaderWithACL.java:      LOG.debug("[" + readerId + "] FAILED read, key = " + (keyToRead + "") + ", "
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/RestartMetaTest.java:    LOG.debug("Loading data....\n\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/RestartMetaTest.java:    LOG.debug("Sleeping for " + SLEEP_SEC_AFTER_DATA_LOAD +
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/RestartMetaTest.java:    LOG.debug("Killing hbase:meta region server running on port " + metaRSPort);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/RestartMetaTest.java:    LOG.debug("Restarting region server running on port metaRSPort");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/RestartMetaTest.java:    LOG.debug("Trying to scan meta");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java:      LOG.debug("Inserting keys [" + startKey + ", " + endKey + ")");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java:              LOG.debug("Preparing put for key = [" + Bytes.toString(rowKey) + "], " + columnCount + " columns");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/BaseTestHBaseFsck.java:      LOG.debug("Table: " + tableName + " already disabled, so just deleting it.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdReadWriteLockWithObjectPool.java:            LOG.debug((readLock ? "Read" : "Write") + "lock of Id " + id + " already taken by "
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdReadWriteLockWithObjectPool.java:            LOG.debug("Release " + (readLock ? "Read" : "Write") + " lock of Id" + id + ", we are "
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestIdReadWriteLockWithObjectPool.java:      LOG.debug("Size of entry pool after gc and purge: " + entryPoolSize);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java:      LOG.debug("Reading keys [" + startKey + ", " + endKey + ")");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java:          LOG.debug("[" + readerId + "] FAILED read, key = " + (keyToRead + "")
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java:            LOG.debug("[" + readerId + "] FAILED read, key = " + (keyToRead + "")
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java:          LOG.debug("Null result obtained for the key ="+rowKey);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java:      LOG.debug("The storage policy of path " + p + " is " + policySet);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java:        LOG.debug("The default hdfs storage policy (indicated by home path: "
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java:      LOG.debug("Test thought StreamCapabilities class was present.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java:      LOG.debug("Test didn't think StreamCapabilities class was present.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/HFileArchiveTestingUtil.java:      LOG.debug("backedup files doesn't match expected.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/HFileArchiveTestingUtil.java:        LOG.debug(msg);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/HFileArchiveTestingUtil.java:      LOG.debug(msg);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSTableDescriptors.java:      LOG.debug("Correctly got error when reading a table descriptor from the archive directory: "
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadSequential.java:    LOG.debug("Test setup: isMultiPut=" + isMultiPut);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadSequential.java:    LOG.debug("Test teardown: isMultiPut=" + isMultiPut);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/ProcessBasedLocalHBaseCluster.java:    LOG.debug("Command : " + command);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/ProcessBasedLocalHBaseCluster.java:    LOG.debug("Creating directory " + dir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/ProcessBasedLocalHBaseCluster.java:      LOG.debug("Tailing " + filePath);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java:      LOG.debug("Native code not loaded");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java:          LOG.debug("No JNI for codec '" + codecName + "' " + e.getMessage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java:        LOG.debug("Native lib not available: " + codecName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java:      LOG.debug("Codec class not available: " + codecName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MockServer.java:    LOG.debug("Stop why=" + why);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitter.java:      LOG.debug("split algo = " + algo);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitter.java:        LOG.debug(sb.toString());
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover1.java:      LOG.debug("Unloading {} regions", rs);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover1.java:      LOG.debug("Before:{} After:{}", regionsInDesignatedServer,
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover1.java:      LOG.debug("Unloading {}", rs);
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover1.java:      LOG.debug("DesignatedServer Before:{} After:{}", regionsInDesignatedServer,
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestRegionMover1.java:      LOG.debug("ExcludeServer Before:{} After:{}", regionsInExcludeServer,
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdater.java:      LOG.debug("Updating keys [" + startKey + ", " + endKey + ")");
./hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedUpdater.java:                LOG.debug("Preparing increment and append for key = ["
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:      LOG.debug("exception details", exception);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:          LOG.debug(Objects.toString(ls));
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:            LOG.debug("ignoring error when closing final writer.", exception);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:          LOG.debug("exception details", t);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:      LOG.debug(archived.toString());
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:        LOG.debug("Ignoring problem closing WALFactory.", exception);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:        LOG.debug("no previous CORRUPTDIR to clean.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:      LOG.debug("split with 'skip errors' set to 'false' correctly threw");
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:              LOG.debug("Deleted recovered.edits file=" + file);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplit.java:      LOG.debug("Creating dir for region " + region);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/WALPerformanceEvaluation.java:          LOG.debug("Read count=" + count + " from " + wal);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALRootDir.java:    LOG.debug("Scanning " + dir.toString() + " for WAL files");
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplitToHFile.java:      LOG.debug("Got exception", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java:      LOG.debug("Exception details for failure to close wal factory.", exception);
./hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestFSHLogProvider.java:    LOG.debug(currentTest.getMethodName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java:      LOG.debug("Authentication token request from " + RpcServer.getRequestUserName().orElse(null));
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java:      LOG.debug("whoAmI() request from " + RpcServer.getRequestUserName().orElse(null));
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java://    LOG.debug("Got token: " + token.toString());
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("Master current key (key1) {}", key1);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("Slave current key (key1) {}", slaveCurrent);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("Master new current key (key2) {}", key2);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("Master new current key (key3) {}", key3);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("Slave current key (key3) {}", slaveCurrent);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("New master, current key: "+current.getKeyId());
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("New master, rolled new current key: "+newCurrent.getKeyId());
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("New master 2, current key: "+current2.getKeyId());
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java:    LOG.debug("New master 2, rolled new current key: "+newCurrent2.getKeyId());
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestHDFSAclHelper.java:        LOG.debug("Scan snapshot error, snapshot {}", snapshotName, e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestNamespaceCommands.java:        LOG.debug(Objects.toString(entry));
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java:      LOG.debug("Table: " + tableName + " already disabled, so just deleting it.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java:    LOG.debug("Test for global authorization for a new registered RegionServer.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java:        LOG.debug("Waiting for region to be opened. Already retried " + retries
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java:      LOG.debug("region is not splittable, because " + e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithDefaultVisLabelService.java:              LOG.debug("Got exception writing labels", t);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/visibility/TestVisibilityLabelsWithDefaultVisLabelService.java:            } else LOG.debug("new labels added: " + resp);
./hbase-server/src/test/java/org/apache/hadoop/hbase/security/TestUser.java:    LOG.debug("User1 is "+user1.getName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/errorhandling/TestForeignExceptionDispatcher.java:      LOG.debug("Got the testing exception!");
./hbase-server/src/test/java/org/apache/hadoop/hbase/errorhandling/TestTimeoutExceptionInjector.java:      LOG.debug("Correctly failed timer: " + e.getMessage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/errorhandling/TestTimeoutExceptionInjector.java:      LOG.debug("Correctly failed timer: " + e.getMessage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:        LOG.debug("Interrupted while sleeping." , e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:        LOG.debug("Interrupted while sleeping." , e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:      LOG.debug("Saw fewer violations than desired (expected 3): " + snapshots
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:        LOG.debug("Interrupted while sleeping.", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:        LOG.debug("Interrupted while sleeping." , e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:      LOG.debug("Saw fewer violations than desired (expected 2): " + snapshots
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:        LOG.debug("Interrupted while sleeping.", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:        LOG.debug("Saw unexpected table violation policy, waiting and re-checking.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:          LOG.debug("Interrupted while sleeping");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreWithMiniCluster.java:      LOG.debug("Interrupted while sleeping");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotasWithSnapshots.java:        LOG.debug("Checking for " + expectedFinalSize + " == " + snapshot.getUsage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotasWithSnapshots.java:        LOG.debug("Master observed table sizes from region size reports: " + sizes);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotasWithSnapshots.java:        LOG.debug("Checking for " + expectedFinalSize + " == " + snapshot.getUsage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotasWithSnapshots.java:        LOG.debug("Last observed size=" + lastValue.get());
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotasWithSnapshots.java:        LOG.debug("Last observed size=" + lastValue.get());
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSnapshotQuotaObserverChore.java:        LOG.debug("Current usage=" + snapshot.getUsage() + " snapshotSize=" + snapshotSize);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSuperUserQuotaPermissions.java:      LOG.debug("message", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSuperUserQuotaPermissions.java:      LOG.debug("message", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/ThrottleQuotaTestUtil.java:    LOG.debug("Call constructor of ThrottleQuotaTestUtil");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/ThrottleQuotaTestUtil.java:      LOG.debug("QuotaCache");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/ThrottleQuotaTestUtil.java:      LOG.debug(Objects.toString(quotaCache.getNamespaceQuotaCache()));
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/ThrottleQuotaTestUtil.java:      LOG.debug(Objects.toString(quotaCache.getTableQuotaCache()));
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/ThrottleQuotaTestUtil.java:      LOG.debug(Objects.toString(quotaCache.getUserQuotaCache()));
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/ThrottleQuotaTestUtil.java:      LOG.debug(Objects.toString(quotaCache.getRegionServerQuotaCache()));
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreRegionReports.java:      LOG.debug("MasterQuotaManager is null, waiting...");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreRegionReports.java:        LOG.debug("Saw " + numReports + " reports for " + tn + " while waiting for 1");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaObserverChoreRegionReports.java:        LOG.debug("Saw " + numReports + " reports for " + tn + " while waiting for none");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotaDropTable.java:    LOG.debug("Successfully deleted table ", tn);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotaDropTable.java:    LOG.debug("Successfully re-created table ", tn);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaStatusRPCs.java:        LOG.debug("Namespace snapshot after initial ingest: " + snapshot);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaStatusRPCs.java:        LOG.debug("Namespace snapshot after second ingest: " + snapshot);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaAdmin.java:        LOG.debug(Objects.toString(settings));
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaAdmin.java:        LOG.debug(Objects.toString(settings));
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestRegionSizeUse.java:    LOG.debug("Data was written to HBase");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestRegionSizeUse.java:    LOG.debug("Data flushed to disk");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestRegionSizeUse.java:      LOG.debug("Expecting more regions. Saw " + observedRegions
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestRegionSizeUse.java:    LOG.debug("Observed region sizes by the HMaster: " + regionSizes);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:    LOG.debug("Quota limit set for table = {}, limit = {}", tn, sizeLimit);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:    LOG.debug("Quota limit set for namespace = {}, limit = {}", ns, sizeLimit);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:    LOG.debug("Space quota settings removed from the table ", tn);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:    LOG.debug("Space quota settings removed from the namespace ", ns);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:            LOG.debug("Deleting quota for namespace: " + namespace);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:            LOG.debug("Deleting quota for table: " + tableName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:            LOG.debug("Deleting quota for user: " + userName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:      LOG.debug("Data was written to HBase");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:      LOG.debug("Data flushed to disk");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:      LOG.debug("Saw quota snapshot for " + (null == tn ? ns : tn) + ": " + snapshot);
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/SpaceQuotaHelperForTests.java:            LOG.debug(region.getRegionInfo().getEncodedName() + " still has compacted files");
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotaOnBulkLoad.java:      LOG.debug("Snapshot does not yet realize quota limit: " + snapshots + ", regionsizes: "
./hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestSpaceQuotaOnBulkLoad.java:      LOG.debug(file.getPath() + " -> " + file.getLen() + "B");
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java:    LOG.debug("Writing test data batch " + batchId);
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java:    LOG.debug("Verifying test data batch " + batchId);
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java:    LOG.debug("Setting CF encoding to " + encoding + " (ordinal="
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java:    LOG.debug("Compacting table " + tableName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java:    LOG.debug("Compaction queue size reached 0, continuing");
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingTTL.java:    LOG.debug("Files accessed during scan: " + accessedFiles);
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java:      LOG.debug("compactStores() returned");
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java:      LOG.debug("Storing file offset=" + dummyFileOffset + " and onDiskSize=" +
./hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java:      LOG.debug("Stored key " + ((i - 1) / 2) +" at offset " + dos.size());
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Initially ringbuffer of Slow Log records is empty");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:        LOG.debug("cleared the ringbuffer of Online Slow Log records");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Initially ringbuffer of Slow Log records is empty");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Added 14 * 11 records, ringbuffer should only provide latest 14 records");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("cleared the ringbuffer of Online Slow Log records");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Initially ringbuffer of Slow Log records is empty");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Initially ringbuffer of Slow Log records is empty");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Initially ringbuffer of Slow Log records is empty");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Added 100 records, ringbuffer should only 1 record with matching filter");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Initially ringbuffer of Slow Log records is empty");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Initially ringbuffer of Slow Log records is empty");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestNamedQueueRecorder.java:    LOG.debug("Added 14 * 11 records, ringbuffer should only provide latest 14 records");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestSlowLogAccessor.java:      LOG.debug("No worries.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestSlowLogAccessor.java:      LOG.debug("RingBuffer records count: {}", count);
./hbase-server/src/test/java/org/apache/hadoop/hbase/namequeues/TestSlowLogAccessor.java:      LOG.debug("SlowLog Table records count: {}", count);
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java:    LOG.debug("HBase cluster shut down -- restarting");
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java:    LOG.debug("Flush complete");
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java:    LOG.debug("Last compaction was at " + compactor.lastCompaction);
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java:      LOG.debug("details of failure to close wal factory.", exception);
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java:        LOG.debug("Encountered exception when starting cluster. Trying port {}", connectorPort, e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java:          LOG.debug("Encountered exception shutting down cluster", ex);
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java:        LOG.debug("All the MBeans we found:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java:          LOG.debug("Class and object name: {} [{}]", instance.getClassName(),
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SampleRegionWALCoprocessor.java:        LOG.debug("Found the KeyValue from WALEdit which should be ignored.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SampleRegionWALCoprocessor.java:        LOG.debug("Found the KeyValue from WALEdit which should be changed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SampleRegionWALCoprocessor.java:      LOG.debug("About to delete a KeyValue from WALEdit.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SampleRegionWALCoprocessor.java:    LOG.debug(SampleRegionWALCoprocessor.class.getName() +
./hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SampleRegionWALCoprocessor.java:    LOG.debug(SampleRegionWALCoprocessor.class.getName() +
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java:      LOG.debug("There are " + servers.size() + " servers and " + regionCount
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java:        LOG.debug(server.getServerName() + " Avg: " + avg + " actual: " + serverLoad);
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java:            // LOG.debug(hri.getRegionNameAsString());
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java:            LOG.debug(server.getServerName() + " Isn't balanced!!! Avg: " + avg +
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java:      LOG.debug("Waiting for there to be "+ totalRegions +" regions, but there are "
./hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java:    LOG.debug("Setting {} to {}", HConstants.HBASE_DIR, dataTestDir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java:    LOG.debug("Setting {} to {}", HConstants.HBASE_DIR, dataTestDir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java:      LOG.debug("Table: " + tableName + " already disabled, so just deleting it.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java:    LOG.debug("Found " + regions.size() + " regions for table " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java:    LOG.debug("firstRegionName=" + Bytes.toString(firstRegionName));
./hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java:        LOG.debug("Waiting until all regions of table " + tableName + " get assigned. Timeout = " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure2/store/region/TestRegionProcedureStore.java:    LOG.debug("expected: " + procIds);
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java:          LOG.debug("Task size:" + subprocs.size() + ", getting:" + index);
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java:            LOG.debug("Sending error to coordinator");
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java:              LOG.debug("Wait for latch interrupted, done:" + (coordinatorReceivedErrorLatch.getCount() == 0));
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java:      LOG.debug("Checking mock:" + (j++));
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java:      LOG.debug("Ignoring exception: ", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java:      LOG.debug("Ignoring exception: ", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedureControllers.java:    LOG.debug("Found prepared, posting commit node:" + commit);
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedureControllers.java:    LOG.debug("Commit node:" + commit + ", exists:" + ZKUtil.checkExists(watcher, commit));
./hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/SimpleRSProcedureManager.java:      LOG.debug("Waiting for procedure to finish.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReadRequestMetrics.java:        LOG.debug("server read request is "
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:    LOG.debug("Storage policy of cf 0: [" + spA + "].");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:    LOG.debug("Storage policy of cf 1: [" + spB + "].");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("Storage policy of cf 0: [" + spA + "].");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("Storage policy of cf 1: [" + spB + "].");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:        LOG.debug("Waiting on table to finish schema altering");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:        LOG.debug("Waiting on table to finish schema altering");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("Storage policy of cf 0: [" + spA + "].");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("Storage policy of cf 1: [" + spB + "].");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("Storage policy of cf 0: [" + spA + "].");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("Storage policy of cf 1: [" + spB + "].");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("Create, " + retryCount);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("mkdirs, " + retryCount);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionFileSystem.java:      LOG.debug("rename, " + retryCount);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java:        LOG.debug("catalog janitor returned " + cleaned);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/throttle/TestFlushWithThroughputController.java:    LOG.debug("Throughput is: " + (result.getFirst() / 1024 / 1024) + " MB/s");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/throttle/TestFlushWithThroughputController.java:    LOG.debug("Flush pressure before flushing: " + pressure);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java:        LOG.debug("nodeCreated(" + path + ")");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java:      LOG.debug("Renamed region directory: " + rsSplitDir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java:      LOG.debug("Processing the old log files.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java:      LOG.debug("Trying to roll the WAL.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestLogRollPeriod.java:        LOG.debug("postLogRoll: oldFile="+oldFile+" newFile="+newFile);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java:          LOG.debug("preLogRoll: oldFile=" + oldFile + " newFile=" + newFile);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java:        LOG.debug("recovering lease for " + p);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java:        LOG.debug("Reading WAL " + CommonFSUtils.getPath(p));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java:            LOG.debug("#" + entry.getKey().getSequenceId() + ": " + entry.getEdit().getCells());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java:          LOG.debug("EOF reading file " + CommonFSUtils.getPath(p));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestWALReplay.java:      LOG.debug(
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestFSWAL.java:      LOG.debug("Log obtained is: " + wal1);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestFSWAL.java:    LOG.debug("testFindMemStoresEligibleForFlush");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/AbstractTestFSWAL.java:              LOG.debug("Sleeping before appending 100ms");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactingMemStore.java:      LOG.debug("added kv: " + kv.getKeyString() + ", timestamp:" + kv.getTimestamp());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactingMemStore.java:      LOG.debug("added kv: " + kv.getKeyString() + ", timestamp:" + kv.getTimestamp());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionInDeadRegionServer.java:      LOG.debug("expected exception: ", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScannerRetriableFailure.java:        LOG.debug(" Injecting fault in table=" + tableName + " scanner");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java:      LOG.debug("Result row: " + Bytes.toString(res.getRow()) + ", value: " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java:                  LOG.debug("flushing");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java:                LOG.debug(Objects.toString(r));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java:                  LOG.debug("flushing");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java:                LOG.debug(Objects.toString(r));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionReplicas.java:        LOG.debug(Boolean.toString(getRS().getFileSystem().exists(sf.getPath())));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHStoreFile.java:    LOG.debug("sfs: " + Joiner.on(",").join(sfs));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHStoreFile.java:    LOG.debug("sorted: " + Joiner.on(",").join(sorted));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHStoreFile.java:    LOG.debug(hsf.getPath().toString());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompoundBloomFilter.java:        LOG.debug(String.format(testIdMsg
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompoundBloomFilter.java:    LOG.debug("Total keys/values to insert: " + kvs.size());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestCompactionScanQueryMatcher.java:      LOG.debug("expected " + expected[i] + ", actual " + actual.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java:      LOG.debug("expected " + expected.get(i) + ", actual " + actual.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java:      LOG.debug("expected " + expected.get(i) + ", actual " + actual.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java:      LOG.debug("expected " + expected[i] + ", actual " + actual.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java:      LOG.debug("expected " + expected[i] + ", actual " + actual.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/querymatcher/TestUserScanQueryMatcher.java:      LOG.debug("expected " + expected.get(i) + ", actual " + actual.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactingToCellFlatMapMemStore.java:      LOG.debug("added kv: " + kv.getKeyString() + ", timestamp" + kv.getTimestamp());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBulkLoadReplication.java:          LOG.debug("Another file bulk loaded. Total for {}: {}", clusterName,
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReportForDuty.java:    LOG.debug("Starting master: " + master.getMaster().getServerName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReportForDuty.java:    LOG.debug("Starting 2nd region server: " + rs2.getRegionServer().getServerName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReportForDuty.java:    LOG.debug("Starting new master: " + backupMaster.getMaster().getServerName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReportForDuty.java:    LOG.debug("Starting master: " + master.getMaster().getServerName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReportForDuty.java:    LOG.debug("Starting master: " + master.getMaster().getServerName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReportForDuty.java:      LOG.debug("Waiting for master to come online ...");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReportForDuty.java:      LOG.debug("Waiting 2nd RS to be started ...");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerReportForDuty.java:        LOG.debug("Waiting for master switch over ... ");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java:        LOG.debug("waiting 2 regions to be available, got " + regionsOfTable.size() +
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java:    for (HRegion h : oldRegions) LOG.debug("OLDREGION " + h.getRegionInfo());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java:        for (HRegion h : newRegions) LOG.debug("NEWREGION " + h.getRegionInfo());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java:        LOG.debug("Waiting for SPLIT state on: " + hri);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java:      LOG.debug("Waiting on region move off the hbase:meta server; current index " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java:    LOG.debug(zkw.getZNodePaths().baseZNode + " created");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java:    LOG.debug(zkw.getZNodePaths().splitLogZNode + " created");
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java:    LOG.debug(Objects.toString(nodes));
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHStore.java:          LOG.debug("runner count: " + RUNNER_COUNT.get());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java:            LOG.debug("exception details", exception);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java:      LOG.debug("Received expected exception", ioe);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java:          LOG.debug("iteration = " + i+ " ts="+System.currentTimeMillis());
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java:              LOG.debug("put iteration = {}", numPutsFinished);
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java:    LOG.debug(
./hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMajorCompaction.java:    LOG.debug("Adding deletes to memstore and flushing");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java:    LOG.debug(zkw.getZNodePaths().baseZNode + " created");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java:    LOG.debug(zkw.getZNodePaths().splitLogZNode + " created");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java:    LOG.debug("waiting for task node creation");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java:    LOG.debug("task created");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java:    LOG.debug("task = " + task);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java:      LOG.debug("Deleting snapshot.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java:      LOG.debug("debug in contains, 3.1: " + status.getPath() + " filePath:" + filePath);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java:      LOG.debug("\n\nStopping a backup master: " + master.getServerName() + "\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java:      LOG.debug("\n\nStopping the active master " + active.getServerName() + "\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java:      LOG.debug("\n\nVerifying backup master is now active\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AlwaysStandByHMaster.java:              LOG.debug("Interrupted waiting for master to die", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockProcedure.java:        LOG.debug(String.format("Proc id %s acquired lock.", procId));
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockProcedure.java:    LOG.debug(String.format("Proc id %s : %s.", procId, response.getLockStatus()));
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:    LOG.debug("Disabling table\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:    LOG.debug("Waiting for no more RIT\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:    LOG.debug("Verifying only catalog region is assigned\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:        LOG.debug("Region still online: " + oregion);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:    LOG.debug("Enabling table\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:    LOG.debug("Waiting for no more RIT\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:    LOG.debug("Verifying there are " + numRegions + " assigned on cluster\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:        LOG.debug(
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:        LOG.debug(
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/AbstractTestDLS.java:    LOG.debug("Master is aborted");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java:      LOG.debug("Slept " + sleeps + " times");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java:        LOG.debug("nodeDeleted(" + path + ")");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestAssignmentManagerBase.java:        LOG.debug("Socket timeout for server=" + server + " retries=" + retries);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestSplitTableRegionProcedure.java:      LOG.debug("Expected Split procedure construction failure: " + e.getMessage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java:    LOG.debug("\n\nTRR: " + msg + "\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitor.java:    LOG.debug("Table dir:" + tabledir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitor.java:    LOG.debug("Store dir:" + storedir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitor.java:    LOG.debug("Store archive dir:" + storeArchive);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitor.java:      LOG.debug("Have store file:" + file.getPath());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitor.java:    LOG.debug("Finished cleanup of parent region");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitor.java:    LOG.debug("Current " + description + ": ");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitor.java:      LOG.debug(Objects.toString(file.getPath()));
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitor.java:    LOG.debug("Adding " + count + " store files to the storedir:" + storedir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/janitor/TestCatalogJanitorInMemoryStates.java:          LOG.debug(region.toString() + " IS a parent!");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureEvents.java:    LOG.debug("checking " + event);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureEvents.java:    LOG.debug("submit " + proc);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureEvents.java:    LOG.debug("wait procedure suspended on " + event);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureEvents.java:    LOG.debug("checking " + event + " size=" + event.getSuspendedProcedures().size());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureEvents.java:    LOG.debug("wake " + event);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureEvents.java:    LOG.debug("waiting " + proc);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureEvents.java:    LOG.debug("completed execution of " + proc +
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRestoreSnapshotProcedure.java:    LOG.debug("Restore snapshot failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRestoreSnapshotProcedure.java:      LOG.debug("Restore snapshot failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestDisableTableProcedure.java:    LOG.debug("Disable failed with exception {}" + e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestDisableTableProcedure.java:      LOG.debug("Disable failed with expected exception {}", tnee);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestDisableTableProcedure.java:      LOG.debug("Disable failed with expected exception {}", tnee);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCloneSnapshotProcedure.java:    LOG.debug("Clone snapshot failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestDeleteTableProcedure.java:    LOG.debug("Delete failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestTruncateTableProcedure.java:    LOG.debug("Truncate failed with exception: " + cause);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestTruncateTableProcedure.java:    LOG.debug("Truncate failed with exception: " + cause);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestEnableTableProcedure.java:    LOG.debug("Enable failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateNamespaceProcedure.java:    LOG.debug("Create namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateNamespaceProcedure.java:    LOG.debug("Create namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateNamespaceProcedure.java:    LOG.debug("Create namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateNamespaceProcedure.java:    LOG.debug("Create namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java:    LOG.debug("Table directory layout is as expected.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestDeleteNamespaceProcedure.java:    LOG.debug("Delete namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestDeleteNamespaceProcedure.java:    LOG.debug("Delete namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestDeleteNamespaceProcedure.java:    LOG.debug("Delete namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestModifyNamespaceProcedure.java:      LOG.debug("The namespace " + namespaceName + " does not exist.  This is expected.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestModifyNamespaceProcedure.java:    LOG.debug("modify namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestModifyNamespaceProcedure.java:    LOG.debug("Modify namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestModifyNamespaceProcedure.java:    LOG.debug("Modify namespace failed with exception: " + result.getException());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureSchedulerConcurrency.java:                LOG.debug("[S] peerId="+ peerId +" procId="+ procId +" concurrent="+ concurrent);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureSchedulerConcurrency.java:                LOG.debug("[E] peerId="+ peerId +" procId="+ procId +" concurrent="+ concurrent);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureSchedulerConcurrency.java:                LOG.debug("[S] tableId="+ tableId +" procId="+ procId +" concurrent="+ concurrent);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureSchedulerConcurrency.java:                LOG.debug("[E] tableId="+ tableId +" procId="+ procId +" concurrent="+ concurrent);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestWALProcedureStoreOnHDFS.java:          LOG.debug("[S] INSERT " + procId);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestWALProcedureStoreOnHDFS.java:          LOG.debug("[E] INSERT " + procId);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestWALProcedureStoreOnHDFS.java:          LOG.debug("[F] INSERT " + procId + ": " + e.getMessage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureScheduler.java:      LOG.debug("fetch children " + childProc);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRoundRobinAssignmentOnRestart.java:    LOG.debug("RegionServer {} has {} regions", testServer, regionInfos.size());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRoundRobinAssignmentOnRestart.java:    LOG.debug("RegionServer {} has {} regions", newTestServer, newRegionInfos.size());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java:    LOG.debug("Server holding meta " + metaServer);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java:    LOG.debug("Stopping RS " + serverToKill);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java:        LOG.debug("Adding " + entry.getKey() + " with sedcondary/tertiary " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java:      LOG.debug("Waiting for " + regionsToVerify.size() + " to come online " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java:      LOG.debug("New destination for region " + entry.getKey().getEncodedName() +
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java:      LOG.debug("There are " + regionMovement + "/" + expected +
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancerHeterogeneousCost.java:          LOG.debug(
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBalancerStatusTagInJMXMetrics.java:        LOG.debug("Encountered exception when starting mini cluster. Trying port " + connectorPort,
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBalancerStatusTagInJMXMetrics.java:          LOG.debug("Encountered exception shutting down cluster", ex);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestFavoredStochasticLoadBalancer.java:      LOG.debug("Region in transition after stopping FN's: " + regionInfo);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestFavoredStochasticBalancerPickers.java:      LOG.debug("Server: " + rst.getRegionServer().getServerName() + " regions: " + regions.size());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java:      LOG.debug("testBulkAssignment with " + mock[0] + " regions and " + mock[1] + " servers");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFileSystem.java:    LOG.debug("from fs uri:" + FileSystem.getDefaultUri(fs.getFileSystem().getConf()));
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFileSystem.java:    LOG.debug("from configuration uri:" + FileSystem.getDefaultUri(fs.getConfiguration()));
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterRestartAfterDisablingTable.java:    LOG.debug("\n\nTRR: " + msg + "\n");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizerOnCluster.java:      LOG.debug("waiting for t3 to settle...");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizerOnCluster.java:    LOG.debug("generating test data into {}, {} regions of sizes (mb) {}", tableName, numRegions,
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizerOnCluster.java:    LOG.debug("writing {}mb to {}", numRows, region);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizerOnCluster.java:      LOG.debug("{} was not found in RegionsLoad", regionInfo.getRegionNameAsString());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizerOnCluster.java:    LOG.debug("waiting for region statistics to settle.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestSimpleRegionNormalizerOnCluster.java:    LOG.debug("waiting for region statistics to settle.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestLogsCleaner.java:      LOG.debug("Kept log file for oldWALs: {}", Arrays.toString(statusOldWALs));
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestLogsCleaner.java:      LOG.debug("Kept log file for masterProcedureWALs: {}",
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestLogsCleaner.java:            LOG.debug("Caught Exception", e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileCleaner.java:    LOG.debug("Now is: " + createTime);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileCleaner.java:      LOG.debug("Creating " + getFileStats(fileName, fs));
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileCleaner.java:    LOG.debug("Creating " + getFileStats(saved, fs));
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileCleaner.java:      LOG.debug(stat.getPath().toString());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileCleaner.java:      LOG.debug("Kept hfiles: " + file.getPath().getName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileCleaner.java:    LOG.debug("File deleted from large queue: " + cleaner.getNumOfDeletedLargeFiles()
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileCleaner.java:    LOG.debug("File deleted from large queue: " + cleaner.getNumOfDeletedLargeFiles()
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java:    LOG.debug("Running hfile cleaners");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java:    LOG.debug("Have snapshot hfiles:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java:      LOG.debug(fileName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java:    LOG.debug("Running hfile cleaners");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestCleanerChore.java:    LOG.debug("Writing test data to: " + testDir);
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClockSkewDetection.java:    LOG.debug("regionServerStartup 1");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClockSkewDetection.java:      LOG.debug("Test: Master Time > Region Server Time");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClockSkewDetection.java:      LOG.debug("regionServerStartup 2");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClockSkewDetection.java:      LOG.debug("Test: Master Time < Region Server Time");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClockSkewDetection.java:      LOG.debug("regionServerStartup 3");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClockSkewDetection.java:    LOG.debug("regionServerStartup 4");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestClockSkewDetection.java:    LOG.debug("regionServerStartup 5");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNotCarryTable.java:      LOG.debug("Wait master to create AssignmentManager");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionsRecoveryChore.java:    LOG.debug("All Region Names with refCount....");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionsRecoveryChore.java:        LOG.debug("name: " + new String(regionMetrics.getRegionName()) + " refCount: " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionsRecoveryChore.java:    LOG.debug("All Region Names with refCount....");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionsRecoveryChore.java:        LOG.debug("name: " + new String(regionMetrics.getRegionName()) + " refCount: " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionsRecoveryChore.java:    LOG.debug("All Region Names with refCount....");
./hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionsRecoveryChore.java:        LOG.debug("name: " + new String(regionMetrics.getRegionName()) + " refCount: " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java:        LOG.debug("Need to fix these: " + npe.toString());
./hbase-server/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsKillRS.java:    LOG.debug("group servers {}", servers);
./hbase-server/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsKillRS.java:        LOG.debug("move region {} from {} to {}", entry.getKey().getRegionNameAsString(),
./hbase-server/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsKillRS.java:    LOG.debug("wait for META assigned...");
./hbase-server/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsAdmin2.java:    LOG.debug("Print group info : " + ADMIN.listRSGroups());
./hbase-server/src/test/java/org/apache/hadoop/hbase/rsgroup/TestRSGroupsAdmin2.java:        LOG.debug("server table region size is:{}", regions.size());
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java:      LOG.debug("Waiting for there to be "+ totalRegionNum
./hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestSimpleRpcScheduler.java:        LOG.debug("Request i=" + i + " value=" + work.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestSimpleRpcScheduler.java:      LOG.debug("Total Time: " + totalTime);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java:      LOG.debug("Waiting for [" + serverName + "] to be listed as dead in master");
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java:    LOG.debug("Server [" + serverName + "] marked as dead, waiting for it to " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java:      LOG.debug("Server [" + serverName + "] still being processed, waiting");
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java:    LOG.debug("Server [" + serverName + "] done with server shutdown processing");
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationTrackerZKImpl.java:      LOG.debug("Received regionServerRemoved event: " + regionServer);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:    LOG.debug("putAndReplicateRows");
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:    LOG.debug("mimicSyncUpAfterDelete");
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:          LOG.debug("t1_syncup should have 51 rows at source, and it is " + rowCount_ht1Source);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:          LOG.debug("t2_syncup should have 101 rows at source, and it is " + rowCount_ht2Source);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:        LOG.debug("SyncUpAfterDelete failed at retry = " + i + ", with rowCount_ht1TargetPeer1 =" +
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:    LOG.debug("mimicSyncUpAfterPut");
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:          LOG.debug("t1_syncup should have 102 rows at source, and it is " + rowCount_ht1Source);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:          LOG.debug("t2_syncup should have 202 rows at source, and it is " + rowCount_ht2Source);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpTool.java:        LOG.debug("SyncUpAfterPut failed at retry = " + i + ", with rowCount_ht1TargetPeer1 =" +
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java:      LOG.debug("Failed to calculate the size of hfile " + hfilePath1);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java:      LOG.debug("Failed to calculate the size of hfile " + hfilePath2);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java:    LOG.debug("mimicSyncUpAfterBulkLoad");
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java:          LOG.debug("t1_syncup should have 206 rows at source, and it is " + rowCount_ht1Source);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java:          LOG.debug("t2_syncup should have 406 rows at source, and it is " + rowCount_ht2Source);
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java:        LOG.debug("SyncUpAfterBulkLoad failed at retry = " + i +
./hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSyncUpToolWithBulkLoadedData.java:    LOG.debug("loadAndReplicateHFiles");
./hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java:          LOG.debug(kv_number + ". kv: " + kv);
./hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java:    LOG.debug("check the fetched kv number");
./hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java:          LOG.debug(kv_number + ". kv: " + kv);
./hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java:    LOG.debug("check the fetched kv number");
./hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java:      LOG.debug("Found row: " + Bytes.toStringBinary(row));
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java:    LOG.debug("----Starting archiving");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java:    LOG.debug("----Starting archiving for table:" + tableName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java:        LOG.debug(counter[0] + "/ " + expected + ") Wrapping call to getDeletableFiles for files: "
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/example/TestZooKeeperTableArchiveClient.java:    LOG.debug("Compacting stores");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:      LOG.debug("Current files:" + files);
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:      LOG.debug("Deleting store for test");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:    LOG.debug("-------Loading table");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:    LOG.debug("Disabled table");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:    LOG.debug("Deleted table");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:    LOG.debug("Store files:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:      LOG.debug(i + " - " + storeFiles.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:    LOG.debug("Archive files:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:      LOG.debug(i + " - " + archivedFiles.get(i));
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:    LOG.debug("-------Loading table");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:    LOG.debug("Disabled table");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:          LOG.debug("hfile=" + fid + " should be in the archive");
./hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java:          LOG.debug("hfile=" + fid + " should be in the source location");
./hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorderMultiBlocks.java:          LOG.debug("Failed to find log file '" + hf.getLocalName() + "'; it probably was " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorderMultiBlocks.java:            LOG.debug("Failed to find log file '" + hf.getLocalName() + "'; it probably was " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/MobStressToolRunner.java:      LOG.debug("MOB Directory content: {} len={}", st.getPath(), st.getLen());
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/MobStressToolRunner.java:    LOG.debug("MOB Directory content total files: {}, total size={}", stat.length, size);
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/FaultyMobStoreCompactor.java:            LOG.debug("Compaction progress: {} {}, rate={} KB/sec, throughputController is {}",
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionWithDefaults.java:    LOG.debug("Taking snapshot and cloning table {}", table);
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionWithDefaults.java:    LOG.debug("Taking snapshot and cloning table {}", table);
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionWithDefaults.java:    LOG.debug("Major compact MOB table " + tableDescriptor.getTableName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionWithDefaults.java:      LOG.debug("Waiting for compaction on {} to complete. current state {}", table, state);
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionWithDefaults.java:    LOG.debug("done waiting for compaction on {}", table);
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionWithDefaults.java:    LOG.debug("checking count of rows");
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionWithDefaults.java:      LOG.debug("MOB Directory content: {}", st.getPath());
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionWithDefaults.java:    LOG.debug("MOB Directory content total files: {}", stat.length);
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobFileCleanerChore.java:      LOG.debug("DDDD MOB Directory content: {} size={}", st.getPath(), st.getLen());
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobFileCleanerChore.java:    LOG.debug("MOB Directory content total files: {}", stat.length);
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionRegularRegionBatchMode.java:    LOG.debug("compacting {} in batch mode.", tableDescriptor.getTableName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/mob/TestMobCompactionOptRegionBatchMode.java:    LOG.debug("compacting {} in batch mode.", tableDescriptor.getTableName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncReplicationAdminApiWithClusters.java:            LOG.debug("Table: " + tableName + " already disabled, so just deleting it.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:      LOG.debug("Result: " + Bytes.toString(result.getValue(family, qualifier)));
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:      LOG.debug("Tables: " + admin.listTableDescriptors());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:      LOG.debug("Finished moving meta");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:      LOG.debug("Finished moving user region");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:      LOG.debug("Result: " + Bytes.toString(result.getValue(family, qualifier)));
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:      LOG.debug("Result: " + Bytes.toString(result.getValue(family, qualifier)));
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:      LOG.debug("Result: " + Bytes.toString(result.getValue(family, qualifier)));
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestEnableTable.java:      LOG.debug("Table: " + tableName + " already disabled, so just deleting it.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncAdminBase.java:              LOG.debug("Table: " + tableName + " already disabled, so just deleting it.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcedureAdminApi.java:      LOG.debug("Snapshot completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java:            LOG.debug("Waiting for region to come online: " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMetaReplicasAddressChange.java:    LOG.debug("CurrentServer={}, moveToServer={}", currentServer, moveToServer);
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("Snapshot1 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("Snapshot2 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug(snapshot3 + " completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("FS state before disable:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("FS state before snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("Snapshot completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("FS state after snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("FS state before disable:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("FS state before snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("Snapshot completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:    LOG.debug("FS state after snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug("Snapshot1 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug("Snapshot2 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug(table2Snapshot1 + " completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug("Snapshot1 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug("Snapshot2 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug(table2Snapshot1 + " completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug("Snapshot1 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug("Snapshot2 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug(table2Snapshot1 + " completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug("Snapshot1 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug("Snapshot2 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java:      LOG.debug(table2Snapshot1 + " completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestUpdateConfiguration.java:    LOG.debug("Starting the test {}", name.getMethodName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestUpdateConfiguration.java:    LOG.debug("Starting the test {}", name.getMethodName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestUpdateConfiguration.java:    LOG.debug("Starting the test {} ", name.getMethodName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestUpdateConfiguration.java:    LOG.debug("Starting the test {}", name.getMethodName());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestShutdownOfMetaReplicaHolder.java:      LOG.debug("Waiting for the replica {} to come up", hrl.getRegion());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestShutdownOfMetaReplicaHolder.java:      LOG.debug("Replica {} is online on {}, old server is {}", hrl.getRegion(),
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/ClientPushbackTestBase.java:    LOG.debug("Writing some data to " + tableName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/ClientPushbackTestBase.java:    LOG.debug("Done writing some data to " + tableName);
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/ClientPushbackTestBase.java:    LOG.debug("Backoff calculated for " + region.getRegionInfo().getRegionNameAsString() + " @ " +
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("FS state before disable:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("FS state before snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("Snapshot completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("FS state after snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("Snapshot1 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("FS state before disable:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("FS state before snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("Snapshot completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:    LOG.debug("FS state after snapshot:");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:      LOG.debug("Snapshot1 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:      LOG.debug("Snapshot2 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotTemporaryDirectory.java:      LOG.debug("Table2Snapshot1 completed.");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java:                LOG.debug("Cycle waiting on split");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java:              LOG.debug("CheckForSplit thread exited, current region count: " + count.get());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java:        LOG.debug("Properly split on " + Bytes.toString(splitPoint));
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin1.java:          LOG.debug("SplitKey=" + splitKey + "&deltaForLargestFamily=" + deltaForLargestFamily +
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/MetaWithReplicasTestBase.java:    LOG.debug("All meta replicas assigned");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java:    LOG.debug("Creating test table");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java:    LOG.debug("Creating test data");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java:    LOG.debug("Loading test data");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java:    LOG.debug("Verifying data load");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestReplicaWithCluster.java:    LOG.debug("Verifying replica queries");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide5.java:        LOG.debug("Flushing cache");
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncQuotaAdminApi.java:      LOG.debug(Objects.toString(settings));
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestResult.java:      LOG.debug("As expected: " + ex.getMessage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestResult.java:      LOG.debug("As expected: " + ex.getMessage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestResult.java:      LOG.debug("As expected: " + ex.getMessage());
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestIOFencing.java:      LOG.debug("allowing compactions");
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestIOFencing.java:        LOG.debug("waiting for compaction to block");
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestIOFencing.java:        LOG.debug("compaction block reached");
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestStochasticBalancerJmxMetrics.java:        LOG.debug("Encountered exception when starting cluster. Trying port " + connectorPort, e);
./hbase-server/src/test/java/org/apache/hadoop/hbase/TestStochasticBalancerJmxMetrics.java:          LOG.debug("Encountered exception shutting down cluster", ex);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java:      LOG.debug("No regions under directory:" + snapshotDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java:    LOG.debug("Storing region-info for snapshot.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java:    LOG.debug("Creating references for hfiles");
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java:          LOG.debug("No files under family: " + familyName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java:          LOG.debug("Adding snapshot references for " + storeFiles  + " hfiles");
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java:          LOG.debug("Adding reference for file ("+ (++i) +"/" + sz + "): " + storeFile.getPath());
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java:    LOG.debug("fs=" + fs.getUri().toString() + " root=" + rootDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java:      LOG.debug("Creation time not specified, setting to:" + time + " (current time:"
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java:        LOG.debug("Snapshot current TTL value: {} resetting it to default value: {}", ttl,
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java:    LOG.debug("Sentinel is done, just moving the snapshot from " + workingDir + " to "
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:    LOG.debug("Storing mob region '" + regionInfo + "' region-info for snapshot=" + snapshotName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:    LOG.debug("Creating references for mob files");
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:          LOG.debug("No mob files under family: " + hcd.getNameAsString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:    LOG.debug("Storing '" + region + "' region-info for snapshot=" + snapshotName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:    LOG.debug("Creating references for hfiles");
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:        LOG.debug("Adding snapshot references for " + storeFiles  + " hfiles");
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:        LOG.debug("Adding reference for file (" + (i+1) + "/" + sz + "): " + storeFile.getPath() +
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:      LOG.debug("Storing region-info for snapshot.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:      LOG.debug("Creating references for hfiles");
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:              LOG.debug("No files under family: " + familyName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:      LOG.debug(String.format("Adding snapshot references for %s %ss", storeFiles, fileType));
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:      LOG.debug(String.format("Adding reference for %s (%d/%d): %s",
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java:      LOG.debug("Convert to Single Snapshot Manifest for {}", this.desc.getName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java:      LOG.debug("No manifest files present: " + snapshotDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java:      LOG.debug("No manifest files present: " + snapshotDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java:      LOG.debug("No manifest files present: " + snapshotDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java:        LOG.debug("Update splits parent " + regionInfo.getEncodedName() + " -> " + daughters);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java:          LOG.debug("Restoring missing HFileLink " + storeFile.getName() +
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java:    LOG.debug("Restore reference " + regionName + " to " + clonedRegionName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java:    LOG.debug("get table regions: " + tableDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java:    LOG.debug("found " + regions.size() + " regions for table=" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java:      LOG.debug("Restored table dir:" + restoreDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:      LOG.debug("Deleted task without in memory state " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:    LOG.debug("Put up splitlog task at znode " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:      LOG.debug("Task not yet acquired " + path + ", ver=" + version);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:        LOG.debug("Unacquired orphan task is done " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:        LOG.debug("Found orphan rescan node " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:        LOG.debug("Failed to resubmit task " + path + " version changed");
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:        LOG.debug("Failed to re-resubmit task " + path + " because of deserialization issue", e1);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:      LOG.debug("Failed to resubmit task " + path + " version changed");
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:          LOG.debug("Found pre-existing znode " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java:        LOG.debug("Deleted " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java:      LOG.debug("Retry listChildren of znode " + watcher.getZNodePaths().splitLogZNode
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/DirectMemoryUtils.java:        LOG.debug("Failed to retrieve nio.BufferPool direct MemoryUsed attribute: " + e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/ReplicationChecker.java:          LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/ReplicationChecker.java:        LOG.debug("Undeleted replication hfile-refs queue for removed peer {} found", peerId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:          LOG.debug("Failed to create lock file " + hbckLockFilePath.getName(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:          LOG.debug("Failed to delete " + HBCK_LOCK_PATH, ioe);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:        LOG.debug("Sideline directory contents:");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:      LOG.debug("Loading region dirs from " +tableDir.getPath());
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:      LOG.debug("Undeploy region "  + rse.getRegionInfo() + " from " + rse.getServerName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:    LOG.debug("There are " + regionInfoMap.size() + " region info entries");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:    LOG.debug("[" + thread + "] Contained region dir after close and pause");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:      LOG.debug("[" + thread + "] Sideline directory contents:");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:      LOG.debug("Exception getting table descriptors", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java:                LOG.debug("Loading region info from hdfs:"+ regionDir.getPath());
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java:        LOG.debug("[" + thread + "] Closing region before moving data around: " +  hi);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java:        LOG.debug("[" + thread + "] Contained region dir before close");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:          LOG.debug("-D configuration override: " + kv[0] + "=" + kv[1]);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:    LOG.debug("Creating table " + tableName + " with " + columnFamilies.length
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:      LOG.debug("Table created!  Waiting for regions to show online in META...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:            LOG.debug(onlineRegions + " of " + splitCount + " regions online...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:      LOG.debug("Finished creating table with " + splitCount + " regions");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:      LOG.debug("Bucketing regions by regionserver...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:        LOG.debug("Done with bucketing.  Split time!");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:              LOG.debug(daughterRegions.size() + " RS have regions to splt.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                LOG.debug("Finding a region on " + rsLoc);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                    LOG.debug("Region with " + splitAlgo.rowToStr(split)
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                      LOG.debug("Region already split on "
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                LOG.debug("Splitting at " + splitAlgo.rowToStr(split));
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                    LOG.debug("Wait for outstanding splits " + outstanding.size());
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                      LOG.debug(local_finished.size() + " outstanding splits finished");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                    LOG.debug("STATUS UPDATE: " + splitCount + " / " + origCount
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                LOG.debug("Finally Wait for outstanding splits " + outstanding.size());
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:                  LOG.debug("Finally " + finished.size() + " outstanding splits finished");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:            LOG.debug("All regions have been successfully split!");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:            LOG.debug("TOTAL TIME = "
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:            LOG.debug("Splits = " + splitCount);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:              LOG.debug("Avg Time / Split = "
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:          LOG.debug("No Server Exception thrown for: " + splitAlgo.rowToStr(start));
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:      LOG.debug("Split Scan: " + finished.size() + " finished / "
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:      LOG.debug("No " + splitFile.getName() + " file. Calculating splits ");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:      LOG.debug("Table " + tableName + " has " + rows.size() + " regions that will be split.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:        LOG.debug("Will Split [" + startStr + " , "
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:      LOG.debug("_balancedSplit file found. Replay log to restore state...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:          LOG.debug("Adding: " + r);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:          LOG.debug("Removing: " + r);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java:      LOG.debug("Done reading. " + daughterRegions.size() + " regions left.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java:    LOG.debug("RegionInfo read: " + hri.toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/MoveWithAck.java:        LOG.debug("Retry {} of maximum {} for region: {}", count, retries,
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactionTTLRequest.java:      LOG.debug("Compaction families for region: " + region + " CF: " + familiesToCompact.keySet());
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java:        LOG.debug("Waiting for servers to complete Compactions");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java:      LOG.debug("Table: " + tableName + " Server: " + sn + " No of regions: " + regions.size());
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java:          LOG.debug("Reached region limit for server: " + sn);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java:          LOG.debug("Adding region " + hri + " to queue " + sn + " for compaction");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java:            LOG.debug("Waiting for compaction to complete for region: " + request.getRegion()
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java:        LOG.debug("Error while sweeping of lossyCounting-{}", name, exception);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java:      LOG.debug("attempted to add backwards edge: " + Bytes.toString(start)
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java:    LOG.debug("Shutting down HBase Cluster");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java:            LOG.debug("Found active master hash={}, stopped={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java:          LOG.debug("DFS Client does not support most favored nodes create; using default create");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java:          LOG.debug("Ignoring (most likely Reflection related exception) " + e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java:          LOG.debug("Unable to create version file at " + rootdir.toString() + ", retrying", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java:    LOG.debug("Rewrote the hbase.id file as pb");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java:    LOG.debug("Create cluster ID file [{}] with ID: {}", idFile, clusterId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java:      LOG.debug("Write the cluster ID file to a temporary location: {}", tempIdFile);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java:          LOG.debug("Move the temporary cluster ID file to its target location [{}]:[{}]",
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java:      LOG.debug("Query Path: {} ; # list of files: {}", queryPath, Arrays.toString(statusList));
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java:          LOG.debug("List of region servers: {}", regionServers);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java:          LOG.debug("Excluded RegionServers from unloading regions to because they " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java:          LOG.debug("Ignore region move failure, it might have been split/merged.", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java:      LOG.debug("Exception during readTableDecriptor. Current table name = " + tableName, ioe);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java:            LOG.debug("Cleaned up old tableinfo file " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java:          LOG.debug("Deleted " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java:        LOG.debug(tempPath + " exists; retrying up to " + retries + " times");
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java:        LOG.debug("Wrote into " + tableInfoDirPath);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java:        LOG.debug("Failed write and/or rename; retrying", ioe);
./hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java:      LOG.debug("Current path=" + status.getPath());
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java:      LOG.debug("Exception details for failure to load WALProvider.", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java:              LOG.debug("exception details", exception);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java:          LOG.debug("failed to close temporary singleton. ignoring.", exception);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/FSHLogProvider.java:        LOG.debug("Error instantiating log writer.", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java:      LOG.debug("Exception details for failure to load region grouping strategy.", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java:          LOG.debug("Details of problem shutting down wal provider '" + provider + "'", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java:          LOG.debug("Details of problem closing wal provider '" + provider + "'", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java:        LOG.debug("Wrote file={}, newMaxSeqId={}, maxSeqId={}", newSeqIdFile, newMaxSeqId,
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java:      LOG.debug("Got EOF when reading first WAL entry from {}, an empty or broken WAL file?", dst,
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java:            LOG.debug("WAL {} roll requested", wal);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java:            LOG.debug("WAL {} roll period {} ms elapsed", wal, this.rollPeriod);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java:            LOG.debug("Ignoring exception from listener.", exception);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java:            LOG.debug("Ignoring exception from listener.", exception);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java:        LOG.debug("Used {} bytes of buffered edits, waiting for IO threads", totalBuffered);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java:                LOG.debug("Last flushed sequenceid for " + encodedRegionNameAsStr + ": "
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java:      LOG.debug(log);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java:          LOG.debug("Completed split of {}, journal: {}", wal, status.prettyPrintJournal());
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java:    LOG.debug("Waiting for split writer threads to finish");
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java:          LOG.debug("Still trying to recover WAL lease: " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AsyncFSWALProvider.java:        LOG.debug("Error instantiating log writer.", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java:        LOG.debug("Obtained token " + token.getKind().toString() + " for user " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java:        LOG.debug("Obtained token " + token.getKind().toString() + " for user " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java:        LOG.debug("Sync token keys from zookeeper");
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java:      LOG.debug("Running as master, ignoring new key {}", key);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java:    LOG.debug("Adding key {}", key.getKeyId());
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java:      LOG.debug("Running as master, ignoring removed keyid={}", keyId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java:      LOG.debug("Removing keyid={}", keyId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java:        LOG.debug("Removing expired key {}", key);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java:            LOG.debug("Interrupted waiting for next update", ie);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java:          LOG.debug("Ignoring empty node "+path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java:          LOG.debug("Ignoring empty node "+path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java:      LOG.debug("Skipping permission cache refresh because writable data is empty");
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java:      LOG.debug("Updating permissions cache from {} with data {}", entry,
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/CoprocessorWhitelistMasterObserver.java:            LOG.debug(String.format("Coprocessor %s found in directory %s",
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java:      LOG.debug("Empty family map passed for permission check");
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java:          LOG.debug("Received request from {} to grant access permission {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java:          LOG.debug("Received request from {} to revoke access permission {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java:      LOG.debug("Writing permission with rowKey " + Bytes.toString(rowKey) + " "
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java:      LOG.debug("Removed permission "+ userPerm.toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java:      LOG.debug("Removing permissions of removed table "+ tableName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java:      LOG.debug("Removing permissions of removed namespace "+ namespace);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java:      LOG.debug("Removing permissions of removed column " + Bytes.toString(column) +
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java:      LOG.debug("Read acl: entry[" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java:        LOG.debug("Details on failure to get active system user.", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java:          LOG.debug("Adding the label " + labelStr);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/GssSaslServerAuthenticationProvider.java:    LOG.debug("Server's Kerberos principal name is {}", fullName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/GssSaslServerAuthenticationProvider.java:          LOG.debug("SASL server GSSAPI callback: setting canonicalized client ID: {}", authzid);
./hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionDispatcher.java:    LOG.debug(name + " accepting received exception" , e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/TimeoutExceptionInjector.java:        LOG.debug("Marking timer as complete - no error notifications will be received for " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/TimeoutExceptionInjector.java:    LOG.debug("Scheduling process timer to run in: " + maxTime + " ms");
./hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/TimeoutExceptionInjector.java:      LOG.debug("Triggering timer immediately!");
./hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java:      LOG.debug("Unable to find favored nodes for parent, " + parent
./hbase-server/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesManager.java:    LOG.debug("Loaded default datanode port for FN: " + datanodeDataTransferPort);
./hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager.java:      LOG.debug("Throttling exception for user=" + ugi.getUserName() +
./hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileSystemUtilizationChore.java:        LOG.debug("Preempting execution of FileSystemUtilizationChore because it exceeds the"
./hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java:          LOG.debug("Unexpectedly did not find a space quota for " + table
./hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java:          LOG.debug("Could not get Namespace space quota for " + namespace
./hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java:          LOG.debug(table + " moved into observance of table space quota.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java:          LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java:      LOG.debug("Stopping QuotaRefresherChore chore.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/ExceedOperationQuota.java:          LOG.debug("Read/Write requests num exceeds quota: writes:{} reads:{} scan:{}, "
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java:              LOG.debug("link open path=" + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java:      LOG.debug("Still running " + runnables);
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java:          LOG.debug("Unable to set drop behind on {}", path.getName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java:          LOG.debug("Opening HFile v2 with v3 reader");
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java:            LOG.debug("File " + filePath + " already exists. Deleting!!");
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java:        LOG.debug("Free started because \"" + why + "\"; " + msgBuffer.toString() +
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java:          LOG.debug("Bucket cache free space completed; " + "freed="
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java:          LOG.debug("Prefetch requested for " + path + ", delay=" + delay + " ms");
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java:      LOG.debug("Prefetch completed for " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java:        LOG.debug("Prefetch cancelled for " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java:      LOG.debug("Trying to use External l2 cache");
./hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/SlowLogQueueService.java:    LOG.debug("Received request to clean up online slowlog buffer.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/BalancerDecisionQueueService.java:    LOG.debug("Received request to clean up balancer decision queue.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:        LOG.debug("Setting Master Port to random.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:        LOG.debug("Setting RegionServer Port to random.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:        LOG.debug("Setting RS InfoServer Port to random.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:        LOG.debug("Setting Master InfoServer Port to random.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:            LOG.debug("Interrupted", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:            LOG.debug("Interrupted", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java:    LOG.debug("Loading coprocessor class " + className + " with path " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java:      LOG.debug("Stop coprocessor " + e.getInstance().getClass().getName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceAuditor.java:      LOG.debug("Namespace auditor checks not performed for table " + name.getNameAsString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java:    LOG.debug("Submitting new Subprocedure:" + procName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java:    LOG.debug("Request received to abort procedure " + procName, ee);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java:          LOG.debug("Procedure " + procName
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java:        LOG.debug("Procedure " + procName
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java:        LOG.debug("Submitting procedure " + procName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java:    LOG.debug("received connection failure: " + message, cause);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java:    LOG.debug("abort procedure " + procName, reason);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java:    LOG.debug("Start region server flush procedure manager " + rss.getServerName().toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java:    LOG.debug("Launching subprocedure to flush regions for " + table);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java:      LOG.debug("Waiting for local region flush to finish.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java:          LOG.debug("Completed " + (i+1) + "/" + sz +  " local region flush tasks.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java:        LOG.debug("Completed " + sz +  " local region flush tasks.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java:      LOG.debug("cancelling " + tasks.size() + " flush region tasks " + name);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.java:      LOG.debug("Starting region operation on " + region);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.java:        LOG.debug("Flush region " + region.toString() + " started...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.java:        LOG.debug("Closing region operation on " + region);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.java:      LOG.debug("About to flush family {} on all regions for table {}", family, table);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.java:    LOG.debug("Flush region tasks submitted for " + regions.size() + " regions");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:      LOG.debug("Procedure '" + procName + "' starting 'acquire'");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:      LOG.debug("Waiting for all members to 'acquire'");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:      LOG.debug("Procedure '" + procName + "' starting 'in-barrier' execution.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:      LOG.debug("Waiting for all members to 'release'");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:      LOG.debug("Running finish phase.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:    LOG.debug("Starting procedure '" + procName + "', kicking off acquire phase on members.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:    LOG.debug("Finished coordinator procedure - removing self from list of running procedures");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:    LOG.debug("member: '" + member + "' joining acquired barrier for procedure '" + procName
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:      LOG.debug("Waiting on: " + acquiredBarrierLatch + " remaining members to acquire global barrier");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:      LOG.debug("Member: '" + member + "' released barrier for procedure'" + procName
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java:      ForeignExceptionDispatcher.LOG.debug("Waiting for '" + latchDescription + "' latch. (sleep:"
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:    LOG.debug("Creating acquire znode:" + acquire);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:        LOG.debug("Watching for acquire node:" + znode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:    LOG.debug("Creating reached barrier zk node:" + reachedNode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:        LOG.debug("Attempting to clean out zk node for op:" + procName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:          LOG.debug("Node created: " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:                  LOG.debug("Finished data from procedure '{}' member '{}': {}", procName, member,
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:            LOG.debug("Ignoring created notification for node:" + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:    LOG.debug("Starting controller for procedure member=" + coordName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:    LOG.debug("Aborting procedure '" + procName + "' in zk");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:      LOG.debug("Creating abort znode:" + procAbortNode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java:      LOG.debug("Finished creating abort node:" + procAbortNode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java:    LOG.debug("Current zk system:");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java:    LOG.debug(prefix + root);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java:      LOG.debug(prefix + child);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java:    LOG.debug("Clearing all znodes {}, {}, {}", acquiredZnode, reachedZnode, abortZnode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/RegionServerProcedureManagerHost.java:      LOG.debug("Procedure {} initializing", proc.getProcedureSignature());
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/RegionServerProcedureManagerHost.java:      LOG.debug("Procedure {} initialized", proc.getProcedureSignature());
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/RegionServerProcedureManagerHost.java:      LOG.debug("Procedure {} starting", proc.getProcedureSignature());
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/RegionServerProcedureManagerHost.java:      LOG.debug("Procedure {} started", proc.getProcedureSignature());
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:          LOG.debug("Ignoring created notification for node:" + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:    LOG.debug("Received reached global barrier:" + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:    LOG.debug("Checking for aborted procedures on node: '" + zkController.getAbortZnode() + "'");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:    LOG.debug("Looking for new procedures under znode:'" + zkController.getAcquiredBarrier() + "'");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:        LOG.debug("No running procedures.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:      LOG.debug("No running procedures.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:    LOG.debug("Found procedure znode: " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:        LOG.debug("Not starting:" + opName + " because we already have an abort notification.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:      LOG.debug("start proc data length is " + data.length);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:      LOG.debug("Found data for znode:" + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:      LOG.debug("Member: '" + memberName + "' joining acquired barrier for procedure (" + procName
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:      LOG.debug("Watch for global barrier reached:" + reachedBarrier);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:    LOG.debug("Marking procedure  '" + procName + "' completed for member '" + memberName
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:    LOG.debug("Aborting procedure (" + procName + ") in zk");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:      LOG.debug("Finished creating abort znode:" + procAbortZNode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:    LOG.debug("Aborting procedure member for znode " + abortZNode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java:    LOG.debug("Starting procedure member '" + memberName + "'");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:          LOG.debug("Was remote foreign exception, not redispatching error", ee);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:          LOG.debug("Was KeeperException, not redispatching error", ee);
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:    LOG.debug("Starting subprocedure '" + barrierName + "' with timeout " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:      LOG.debug("Subprocedure '" + barrierName + "' starting 'acquire' stage");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:      LOG.debug("Subprocedure '" + barrierName + "' locally acquired");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:      LOG.debug("Subprocedure '" + barrierName + "' coordinator notified of 'acquire', waiting on" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:      LOG.debug("Subprocedure '" + barrierName + "' received 'reached' from coordinator.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:      LOG.debug("Subprocedure '" + barrierName + "' locally completed");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:      LOG.debug("Subprocedure '" + barrierName + "' has notified controller of completion");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:      LOG.debug("Subprocedure '" + barrierName + "' running cleanup.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java:      LOG.debug("Subprocedure '" + barrierName + "' completed.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java:    LOG.debug("Done splitting WAL {}", filename);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java:      LOG.debug("Opened " + regionName + " on " + this.server.getServerName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java:        LOG.debug("Interrupting thread " + t);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java:      LOG.debug("Closed {}", region.getRegionInfo().getRegionNameAsString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/UnassignRegionHandler.java:      LOG.debug("Received CLOSE for {} which is not ONLINE and we're not opening/closing.",
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java:      LOG.debug("RPC'ing to primary " + ServerRegionReplicaUtil.
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java:        LOG.debug("Failed to trigger a flush of primary region replica {} of region {}, retry={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java:          LOG.debug("Triggered flush of primary region replica " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java:              LOG.debug("Triggered empty flush marker (memstore empty) on primary region replica " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java:      LOG.debug("Starting snapshot operation on " + region);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java:          LOG.debug("take snapshot without flush memstore first");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java:          LOG.debug("Flush Snapshotting region " + region.toString() + " started...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java:          LOG.debug("... SkipFlush Snapshotting region " + region.toString() + " completed.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java:          LOG.debug("... Flush Snapshotting region " + region.toString() + " completed.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java:        LOG.debug("Closing snapshot operation on " + region);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java:    LOG.debug("Flush Snapshot Tasks submitted for " + regions.size() + " regions");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java:    LOG.debug("Start Snapshot Manager " + rss.getServerName().toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java:    LOG.debug("Launching subprocedure for snapshot " + snapshot.getName() + " from table "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java:      LOG.debug("Waiting for local region snapshots to finish.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java:          LOG.debug("Completed " + (i+1) + "/" + sz +  " local region snapshots.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java:        LOG.debug("Completed " + sz +  " local region snapshots.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java:      LOG.debug("cancelling " + tasks.size() + " tasks for snapshot " + name);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java:      LOG.debug("Committing " + buildPath + " as " + dstPath);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java:        LOG.debug("Skipping creation of .regioninfo file for " + regionInfo);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java:          LOG.debug("Skipping creation of .regioninfo file for " + regionInfo);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java:      LOG.debug("DELETING region " + regionDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java:        LOG.debug(msg + ", retries exhausted");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java:      LOG.debug(msg + ", sleeping " + baseSleepBeforeRetries + " times " + sleepMultiplier);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java:    LOG.debug("Installed shutdown hook thread: " + t.getName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushLargeStoresPolicy.java:      LOG.debug("No {} set in table {} descriptor;"
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushLargeStoresPolicy.java:      LOG.debug("Flush {} of {}; "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java:        LOG.debug("HFile Bloom filter type for "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java:    LOG.debug("update config: {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.java:        LOG.debug("deltaSize: " + deltaSize + " bytes; elapseTime: " + elapsedTime + " ns");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.java:        LOG.debug(opName + " sleep=" + sleepTime + "ms because current throughput is "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareCompactionThroughputController.java:        LOG.debug("CompactionPressure is " + compactionPressure + ", tune throughput to "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareFlushThroughputController.java:      LOG.debug("flushPressure is " + flushPressure + ", tune flush throughput to "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushAllLargeStoresPolicy.java:      LOG.debug("Since none of the CFs were above the size, flushing all.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java:            LOG.debug("Unable to unwrap key with WAL key '" + walKeyName + "'");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java:            LOG.debug("Unable to unwrap key with current master key '" + masterKeyName + "'");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java:        LOG.debug("Requesting log roll because of file size threshold; length=" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java:      LOG.debug("Closing WAL writer in " + CommonFSUtils.getPath(walDir));
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java:          LOG.debug("Requesting log roll because of file size threshold; length=" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java:          LOG.debug("EOF at position {}", originalPosition);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java:          LOG.debug("Create new " + implClassName + " writer with pipeline: " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java:            LOG.debug("checkSlowSync triggered but we decided to ignore it; " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java:        LOG.debug("cannot split {} because midkey is the same as first or last row", file);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java:        LOG.debug("Conflict detected by nonce: " + nk + ", " + oldResult);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java:        LOG.debug("Conflict with running op ended: " + nk + ", " + newResult);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java:          LOG.debug("Fail to read the cell, the mob file " + path + " doesn't exist", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java:        LOG.debug("Fail to read the cell", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java:        LOG.debug("Fail to read the cell", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java:          LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java:    LOG.debug("Client=" + RpcServer.getRequestUserName().orElse(null) + "/"
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java:          LOG.debug("clear " + queueName + " compaction queue");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java:          LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java:    LOG.debug("Executing remote procedure {}, pid={}", callable.getClass(), request.getProcId());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitRequest.java:      LOG.debug("Skipping split because server is stopping=" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java:      LOG.debug("regionsWithCommonTable={}", tableRegionsCount);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java:      LOG.debug("Failed getOnlineRegions " + tablename, e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java:      LOG.debug("FLUSHING TO DISK {}, store={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java:    LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:            LOG.debug("loaded {}", storeFile);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:        LOG.debug("Moving the files {} to archive", filesToRemove);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:        LOG.debug("HFile bounds: first=" + Bytes.toStringBinary(firstKey.get()) +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:        LOG.debug("Region bounds: first=" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:          LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:    LOG.debug("Completing compaction from the WAL marker");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:        LOG.debug("StoreFile {} has null Reader", sf);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:      LOG.debug(this + " is initiating " + (request.isMajor() ? "major" : "minor") + " compaction"
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:      LOG.debug("Skipping expired store file removal due to min version of {} being {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:            LOG.debug("The file {} was closed but still not archived", file);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java:        LOG.debug("Moving the files {} to archive", filesToRemove);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java:      LOG.debug("Tuner step size is too low; we will not perform any tuning this time.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java:      LOG.debug(tunerLog.toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java:          LOG.debug("token added " + tok + " for user " + ugi + " return=" + b);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java:        LOG.debug(p.getName()
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java:        LOG.debug("Bulk-load file " + srcPath + " is on different filesystem than " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java:        LOG.debug("Bulk-load file " + srcPath + " is copied to destination staging dir.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java:        LOG.debug("Moving " + p + " to " + stageP);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java:      LOG.debug("Bulk Load done for: " + srcPath);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java:          LOG.debug(p.getName() + " is already available in source directory. Skipping rename.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java:        LOG.debug("Moving " + stageP + " back to " + p);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RemoteProcedureResultReporter.java:      LOG.debug("Failed to complete execution of pid={}", procId, error);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RemoteProcedureResultReporter.java:      LOG.debug("Successfully complete execution of pid={}", procId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java:        LOG.debug("{} stats (chunk size={}): current pool size={}, created chunk count={}, " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java:        LOG.debug("Write stripe metadata for " + writer.getPath().toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java:        LOG.debug("Skip writing stripe metadata for " + writer.getPath().toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java:          LOG.debug("Stopping to use a writer after [" + Bytes.toString(currentWriterEndKey)
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java:          LOG.debug("Stopping to use a writer after [" + Bytes.toString(lastRowInCurrentWriter)
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java:          LOG.debug("Creating new writer starting at [" + Bytes.toString(boundary) + "]");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java:          LOG.debug("Preparing to start a new writer after ["
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java:        LOG.debug("Stopping with "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java:          LOG.debug("Merge expired stripes into one, create an empty file to preserve metadata.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java:    LOG.debug("Splitting the stripe - ratio w/o split " + ratio + ", ratio with split "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java:    LOG.debug("Attempting to merge compaction results: " + compactedFiles.size()
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java:    LOG.debug("Attempting to delete compaction results: " + compactedFiles.size());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java:    LOG.debug("Attempting to load " + storeFiles.size() + " store files.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java:    LOG.debug(sb.toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java:      LOG.debug("Region " + r.getRegionInfo().getRegionNameAsString() +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java:        LOG.debug("Splitting " + r + ", " + this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java:      LOG.debug(reason);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java:      LOG.debug(type + "Compaction requested: " + (selectNow ? compaction.toString() : "system")
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java:      LOG.debug(reason);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java:        LOG.debug("Status {}", CompactSplit.this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java:        LOG.debug("Compaction Rejected: " + runner);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor.java:      LOG.debug(sb.toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:        LOG.debug("filesToCompact: " + filesToCompact + " mcTime: " + mcTime);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:        LOG.debug("lowTimestamp: " + lowTimestamp + " lowTimestamp: " + lowTimestamp + " now: " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:        LOG.debug("Major compaction triggered on store " + this
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:        LOG.debug("Major compaction triggered on store " + this
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:        LOG.debug("Major compaction triggered on store " + this + "; because file "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:        LOG.debug("Major compaction triggered on store " + this +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:      LOG.debug("Major compaction triggered on store " + this
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:    LOG.debug("Skipping major compaction of " + this +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:      LOG.debug("Generated compaction request: " + result);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java:            LOG.debug("Processing files: " + fileList + " for window: " + window);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java:      LOG.debug("Exploring compaction algorithm has selected " + smallest.size()
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java:    LOG.debug("Exploring compaction algorithm has selected {}  files of size {} starting at " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java:            LOG.debug("Major compaction triggered on only store " + regionInfo
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java:            LOG.debug("Skipping major compaction of " + regionInfo
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java:          LOG.debug("Major compaction triggered on store " + regionInfo
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java:        LOG.debug("Major compaction triggered on store " + regionInfo
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactor.java:      LOG.debug("Executing compaction with " + lowerBoundaries.size()
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java:      LOG.debug("Compacting {}, keycount={}, bloomtype={}, size={}, "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java:            LOG.debug("Compaction progress: {} {}, rate={} KB/sec, throughputController is {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java:      LOG.debug("Not selecting compaction: " + filesCompacting.size() + " files compacting");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java:      LOG.debug("There are references in the store; compacting all files");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java:      LOG.debug("Selecting L0 compaction with " + l0Files.size() + " files");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java:      LOG.debug("No good compaction is possible in any stripe");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java:    LOG.debug("Found compaction in a stripe with end key ["
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java:      LOG.debug("Adding " + l0Files.size() + " files to compaction to be able to drop deletes");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java:    LOG.debug("Creating " + kvsAndCount.getSecond() + " initial stripes with "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java:    LOG.debug("Merging " + bestLength + " stripes to delete expired store files");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java:    LOG.debug("Selecting compaction from " + candidateFiles.size() + " store files, " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java:      LOG.debug("Some files are too large. Excluding " + pos
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java:        LOG.debug("Warning, compacting more than " + comConf.getMaxFilesToCompact()
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java:        LOG.debug("Too many admissible files. Excluding " + excess
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java:        LOG.debug("Not compacting files because we only have " + candidates.size() +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java:        LOG.debug("StoreScanner already has the close lock. There is no need to updateReaders");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java:        LOG.debug("StoreScanner already closing. There is no need to updateReaders");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java:    LOG.debug("Switch to stream read (scanned={} bytes) of {}", bytesRead,
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java:          LOG.debug("Under global heap pressure: " + "Region "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java:        LOG.debug("Above memory mark but there is no flushable region");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java:              LOG.debug("Flush thread woke up because memory above low water="
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Instantiated " + this +"; "+ storeHotnessProtector.toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Region open journal for {}:\n{}", this.getRegionInfo().getEncodedName(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("replaying wal for " + this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("stopping wal replay for " + this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Cleaning up temporary data for " + this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Cleaning up detritus for " + this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("writing seq id for {}", this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:            LOG.debug("Failed to clean up wrong region WAL directory {}", wrongRegionWALDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Running coprocessor post-open hooks for " + this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Region " + this
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Region " + this
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Region close journal for {}:\n{}", this.getRegionInfo().getEncodedName(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Closing {}, disabling compactions & flushes",
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug((useTimedWait ? "Time limited wait" : "Waiting without time limit") +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:            LOG.debug("Interrupting region operations after waiting for close lock for " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Acquired close lock on " + this + " after waiting " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Updates disabled for region " + this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:          LOG.debug("waiting for " + writestate.compacting + " compactions"
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("waiting for cache flush to complete for region " + this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Waited {} ms for region {} flush to complete", duration, this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:   *   LOG.debug("Closing " + this + ": disabling compactions & flushes");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Skipping compaction on " + this + " because closing/closed");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Compaction status journal for {}:\n{}", this.getRegionInfo().getEncodedName(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:          LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Flush status journal for {}:\n{}", this.getRegionInfo().getEncodedName(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Flush column family " + store.getColumnFamilyName() + " of " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Flush column family: " + store.getColumnFamilyName() + " of " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:            LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Deleted recovered.edits file={}", file);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Found {} recovered edits file(s) under {}", files == null ? 0 : files.length,
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Found " + (files == null ? 0 : files.size())
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:          LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug(getRegionInfo().getEncodedName() + " : "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug(getRegionInfo().getEncodedName() + " : "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:              LOG.debug(getRegionInfo().getEncodedName() + " : "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:                LOG.debug(getRegionInfo().getEncodedName() + " : "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:              LOG.debug(getRegionInfo().getEncodedName() + " : "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug(getRegionInfo().getEncodedName() + " : "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug(getRegionInfo().getEncodedName() + " : "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug(getRegionInfo().getEncodedName() + " : "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Thread interrupted waiting for lock on row: {}, in region {}", rowKey,
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:    LOG.debug("Opening region: {}", info);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("checking encryption for " + this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("checking classloading for " + this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Opening region (readOnly filesystem): " + info);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:    LOG.debug("Warmup {}", info);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:      LOG.debug("Registered coprocessor service: region=" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Interrupted while waiting for a lock in region {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java:        LOG.debug("Flush requested on " + this.getRegionInfo().getEncodedName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:        LOG.debug("About to register with Master.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:            LOG.debug("Waiting on " + getOnlineRegionsAsPrintableString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:            LOG.debug("master doesn't support ReportRegionSpaceUse, pause before retrying");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug("Failed to report region sizes to Master. This will be retried.", ioe);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:              LOG.debug("Online Regions=" + this.onlineRegions);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:          LOG.debug("Waiting on {}", this.regionsInTransitionInRS.keySet().stream().
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:        LOG.debug("Shutdown / close exception details:", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:          LOG.debug("Config from master: " + key + "=" + value);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug("logDir={}", logDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug("SplitLogWorker started");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:    LOG.debug("Finished post open deploy task for " + r.getRegionInfo().getRegionNameAsString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:            LOG.debug("No master found and cluster is stopped; bailing out");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:            LOG.debug("No master found; retry");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:        LOG.debug("Master is not running yet");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug("Exception details for failure to fetch wal coprocessor information.", exception);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:        LOG.debug("Exception details for failure to fetch wal coprocessor information.", exception);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug("Received CLOSE for a region which is not online, and we're not opening.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug("NotServingRegionException; " + t.getMessage());
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:      LOG.debug("Failed to report file archival(s) to Master. This will be retried.", ioe);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java:      LOG.debug("Swapping pipeline suffix; before={}, new segment={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java:          LOG.debug("Compaction pipeline segment {} flattened", s);
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java:        LOG.debug("From HeapMemoryTuner new memstoreSize: " + memstoreSize
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java:          LOG.debug("No changes made by HeapMemoryTuner.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java:        LOG.debug("ShouldSplit because region size is big enough "
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java:          LOG.debug("ShouldSplit because {} size={}, sizeToCheck={}{}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BusyRegionSplitPolicy.java:        LOG.debug("Going to split region " + region.getRegionInfo().getRegionNameAsString()
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java:    LOG.debug("Current cache:" + cache);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java:        LOG.debug("No snapshots on-disk, clear cache");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotHFileCleaner.java:      LOG.debug("Corrupted in-progress snapshot file exception, ignored ", cse);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java:      LOG.debug("Marking snapshot" + ClientSnapshotDescriptionUtils.toString(snapshot)
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java:      LOG.debug("Launching cleanup of working dir:" + workingDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java:        LOG.debug("Table snapshot journal : \n" + status.prettyPrintJournal());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java:    LOG.debug("Deleting snapshot: " + snapshotName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java:      LOG.debug("Snapshot '" + ssString + "' has completed, notifying client.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java:      LOG.debug("Snapshoting '" + ssString + "' is still in progress!");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java:    LOG.debug("No existing snapshot, attempting snapshot...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java:        LOG.debug("Table enabled, starting distributed snapshots for {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java:        LOG.debug("Started snapshot: {}", ClientSnapshotDescriptionUtils.toString(snapshot));
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java:        LOG.debug("Table is disabled, running snapshot entirely on master for {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java:        LOG.debug("Started snapshot: {}", ClientSnapshotDescriptionUtils.toString(snapshot));
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java:          LOG.debug("No log files to split, proceeding...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java:          LOG.debug("Renamed region directory: " + splitDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java:                LOG.debug("Archived meta log " + status.getPath() + " to " + newPath);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java:      LOG.debug("Heartbeat " + toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java:      if (LOG.isDebugEnabled()) LOG.debug("Timeout failure " + this.event);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java:        if (LOG.isDebugEnabled()) LOG.debug("Calling wake on " + this.event);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java:      LOG.debug((unlock.get()? "UNLOCKED " : "TIMED OUT ") + toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java:        LOG.debug("LOCKED " + toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java:        LOG.debug(LockState.LOCK_EVENT_WAIT + " " + env.getProcedureScheduler().dumpLocks());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java:          LOG.debug("Splittable=" + splittable + " " + node.toShortString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java:      LOG.debug("pid=" + getProcId() + " split storefiles for region " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java:      LOG.debug("pid=" + getProcId() + " splitting started for store file: " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java:      LOG.debug("pid=" + getProcId() + " splitting complete for store file: " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java:        LOG.debug(LockState.LOCK_EVENT_WAIT + " " + env.getProcedureScheduler().dumpLocks());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java:            LOG.debug("NULL result from meta - ignoring but this is strange.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java:    LOG.debug("Deleted regions: {}", regions);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java:    LOG.debug("Overwritten regions: {} ", regionInfos);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java:    METALOG.debug("{} {}", p.getClass().getSimpleName(), p.toJSON());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionRemoteProcedureBase.java:        LOG.debug("{} for region {}, targetServer {} is dead, SCP will interrupt us, give up", this,
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCRegionProcedure.java:              LOG.debug("Archiving region=" + getRegion().getShortNameToLog());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCRegionProcedure.java:              LOG.debug("Failed to delete {}", regionWALDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCRegionProcedure.java:              LOG.debug("Failed to delete {}", regionWALDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java:    LOG.debug("hbase:meta replica znodes: {}", metaZNodes);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java:      LOG.debug("Loaded hbase:meta {}", regionNode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java:      LOG.debug("Only one region server found and hence going ahead with the assignment");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java:      LOG.debug("Split request from " + serverName +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java:      LOG.debug("Handling merge request from RS=" + merged + ", merged=" + merged);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java:    LOG.debug("Joining cluster...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java:        LOG.debug("Stopped! Dropping assign of " + regions.size() + " queued regions.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java:      LOG.debug("Processing assignQueue; systemServersCount=" + serversForSysTables.size() +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitWALManager.java:      LOG.debug("Acquired split WAL worker={}", worker.get());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitWALManager.java:    LOG.debug("Release split WAL worker={}", worker);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java:          LOG.debug("Previously orphan task " + path + " is now being waited upon");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java:          LOG.debug("wait for status of task " + path + " to change to DELETED");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java:        LOG.debug("resubmitting unassigned task(s) after timeout");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java:    LOG.debug("Constructed {}/{} RegionInfo descriptors corresponding to identified holes.",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java:    LOG.debug("Added {}/{} entries to hbase:meta",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java:          ioe -> LOG.debug("Attempt to fix region hole in hbase:meta failed.", ioe));
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java:      LOG.debug("No overlaps.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/CatalogJanitor.java:        LOG.debug("CatalogJanitor already running");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/CatalogJanitor.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/CatalogJanitor.java:      LOG.debug("Deleting region " + parent.getShortNameToLog() + " because daughters -- " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure.java:      LOG.debug("deleteDirectory throws exception: " + e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java:          LOG.debug("waiting for '" + getTableName() + "' regions in transition");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java:          LOG.debug("truncate '" + getTableName() + "' completed");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java:      LOG.debug(msg);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/InitMetaProcedure.java:    LOG.debug("Execute {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:          LOG.debug("Waiting for RIT for {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:          LOG.debug("Deleting regions from filesystem for {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:          LOG.debug("Deleting regions from META for {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:          LOG.debug("Deleting assignment state for {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:          LOG.debug("Finished {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:        LOG.debug("Archived {} regions", tableName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:    LOG.debug("Removing '" + tableName + "' from region states.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:    LOG.debug("Marking '" + tableName + "' as deleted.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java:    LOG.debug("Removing '" + tableName + "' descriptor.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureEnv.java:          LOG.debug("Recover Procedure Store log lease: " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterDDLOperationHelper.java:      LOG.debug("Removing family=" + Bytes.toString(familyName) + " from table=" + tableName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java:      LOG.debug("Stopped");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java:      LOG.debug("ServerManager is null");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java:      LOG.debug("ProcedureExecutor is null");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java:      LOG.debug("ProcedureEnv is null; stopping={}", master.isStopping());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java:      LOG.debug("Request to {} failed, try={}", serverName, numberOfAttemptsSoFar, e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java:        if (!logged) LOG.debug("waitFor " + purpose);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java:            LOG.debug("No change in number of region replicas (configuredReplicaCount={});"
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java:      LOG.debug("Check if {} WAL splitting is done? wals={}, meta={}", serverName, wals, splitMeta);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java:    LOG.debug("Splitting meta WALs {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java:    LOG.debug("Done splitting meta WALs {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java:    LOG.debug("Splitting WALs {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java:    LOG.debug("Done splitting WALs {}", this);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/MasterRegion.java:    LOG.debug("WALDir={}", walDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java:        LOG.debug("Not running balancer because only " + cs.getNumServers()
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java:      LOG.debug("{} {}; total cost={}, sum multiplier={}; cost/multiplier to need a balance is {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java:      LOG.debug("{} sees a total of {} servers and {} regions.", getClass().getSimpleName(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java:        LOG.debug("Not running balancer because only " + cs.getNumServers()
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionLocationFinder.java:      LOG.debug("tableName={}", tableName, fnfe);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionLocationFinder.java:        LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java:    LOG.debug(strBalanceParam.toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java:          LOG.debug("During balanceOverall, we found " + serverload.getServerName()
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java:    LOG.debug("Generating favored nodes for regions missing them.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java:              LOG.debug("Generating favored nodes for: " + hri + " with primary: " + primary);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java:        LOG.debug("Updating FN in meta for missing regions, count: " + regionFNMap.size());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java:      LOG.debug("Unable to find favored nodes for parent, " + parent
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java:      LOG.debug("Found misplaced regions: " + misplacedRegions + ", not on favored nodes.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/HeterogeneousRegionCountCostFunction.java:    LOG.debug("Rebuilding cache of capacity for each RS");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/HeterogeneousRegionCountCostFunction.java:      LOG.debug(sn.getHostname() + " can hold " + capacity + " regions");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java:    LOG.debug("Attempting to fetch active master sn from zk");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java:          LOG.debug("No master available. Notifying waiting threads");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java:            LOG.debug("Interrupted waiting for master to die", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java:      LOG.debug(this.watcher.prefix("Failed delete of our master address node; " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java:      LOG.debug("minRegionServers set to " + val);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java:      LOG.debug("minServers set to " + val);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java:      LOG.debug("localRegionServers set to " + val);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java:      LOG.debug("masters set to " + val);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionsRecoveryChore.java:          LOG.debug("Reopening regions with very high storeFileRefCount is disabled. " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java:        LOG.debug("Channel bindAddress={}, networkInterface={}, INA={}", bindAddress, ni, ina);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:      LOG.debug("Checking to see if procedure from request:"
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:    LOG.debug("Checking to see if snapshot from request:" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:    LOG.debug("Checking to see if procedure is done pid=" + request.getProcId());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:      LOG.debug(master.getClientIdAuditPrefix() + " unassign " + hri.getRegionNameAsString()
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:        LOG.debug("Received region space usage report but HMaster is not ready to process it, "
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:        LOG.debug("Received space quota region size report but HMaster is not ready to process it,"
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:    LOG.debug(master.getClientIdAuditPrefix() + " clear dead region servers.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:        LOG.debug("Some dead server is still under processing, won't clear the dead server list");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/AbstractPeerProcedure.java:      LOG.debug("Skip settting last pushed sequence id for {}", tableName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:          LOG.debug("Created {}", this.clusterStatusPublisherChore);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:      LOG.debug("Balancer post startup initialization complete, took " + (
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:    LOG.debug("Stopping service threads");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:      LOG.debug("Master has not been initialized, don't run {}.", action);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:            LOG.debug("Coprocessor bypassing balancer request");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:          LOG.debug("No more balancing till next balance run; maxBalanceTime="
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:      LOG.debug("Region normalization is disabled, don't run region normalizer.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:        LOG.debug("Unable to determine a plan to assign " + hri);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:        LOG.debug("Unable to determine a plan to assign " + hri);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:        LOG.debug("Skipping move of region " + hri.getRegionNameAsString() +
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:      LOG.debug("Skipping move of region " + hri.getRegionNameAsString()
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:      LOG.debug("HMaster started in backup mode. Stalling until master znode is written.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:        LOG.debug("Waiting for master address and cluster state znode to be written.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:      LOG.debug("Abort called but aborted={}, stopped={}", isAborted(), isStopped());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:      LOG.debug("Registered master coprocessor service: service="+serviceName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:    LOG.debug("{} list replication peers, regex={}", getClientIdAuditPrefix(), regex);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:    LOG.debug("Remote procedure done, pid={}", procId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:    LOG.debug("Remote procedure failed, pid={}", procId, error);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java:          LOG.debug("{} does not exist, but it exists "
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:    LOG.debug("Updating configuration parameters according to new configuration instance.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:      LOG.debug("Normalization of system table {} isn't allowed", table);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:      LOG.debug("Both split and merge are disabled. Skipping normalization of table: {}", table);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:    LOG.debug("Computing normalization plan for table:  {}, number of regions: {}", table,
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:    LOG.debug("Computed {} normalization plans for table {}", plans.size(), table);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:      LOG.debug("{} was not found in RegionsLoad", hri.getRegionNameAsString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:        LOG.debug("Table {} configured with target region count {}, target region size {}", table,
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:    LOG.debug("Table {}, total aggregated regions size: {} and average region size {}", table,
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:      LOG.debug("Table {} has {} regions, required min number of regions for normalizer to run"
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:    LOG.debug("Computing normalization plan for table {}. average region size: {}, number of"
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java:    LOG.debug("Table {}, average region size: {}", ctx.getTableName(), avgRegionSize);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:        LOG.debug("interrupt detected. terminating.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:        LOG.debug("interrupt detected. terminating.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:        LOG.debug("Skipping table {} because normalization is disabled in its table properties.",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:      LOG.debug("Skipping table {} because unable to access its table descriptor.", tableName, e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:      LOG.debug("No normalization required for table {}.", tableName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:          LOG.debug("Nothing to do for {} with PlanType=NONE. Ignoring.", plan);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:      LOG.debug("Sum of merge request size overflows rate limiter data type. {}", plan);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:    LOG.debug("Rate limiting delayed the worker by {}", Duration.ofSeconds(rateLimitedSecs));
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:      LOG.debug("Split request size overflows rate limiter data type. {}", plan);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java:    LOG.debug("Rate limiting delayed this operation by {}", Duration.ofSeconds(rateLimitedSecs));
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterInitializationMonitor.java:          LOG.debug("Initialization completed within allotted tolerance. Monitor exiting.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java:        LOG.debug("Computed {} threads for CleanerChore, using 1 instead", computedThreads);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java:      LOG.debug("Failed to traverse and delete the path: {}", dir, e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java:      LOG.debug("Couldn't delete '{}' yet because it isn't empty w/exception.", dir, exception);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java:            LOG.debug("Couldn't verify if the referenced file still exists, keep it just in case: "
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java:          LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java:        LOG.debug("Couldn't instantiate the file system, not deleting file, just in case. "
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java:      LOG.debug("Size from configuration is the same as previous which "
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java:    LOG.debug("Old WALs for delete: {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java:          LOG.debug("Deleting {}", oldWalFile);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java:    LOG.debug("Cancelling LogCleaner");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Starting for large file={}", large);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Starting for small files={}", small);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Exit {}", Thread.currentThread());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:        LOG.debug("Deleted more than Long.MAX_VALUE large files, reset counter to 0");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:        LOG.debug("Deleted more than Long.MAX_VALUE small files, reset counter to 0");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:    LOG.debug("Stopping file delete threads");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Update configuration triggered but nothing changed for this cleaner");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Updating throttle point, from {} to {}", this.throttlePoint, throttlePoint);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Updating largeQueueInitSize, from {} to {}", this.largeQueueInitSize,
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Updating smallQueueInitSize, from {} to {}", this.smallQueueInitSize,
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Updating largeFileDeleteThreadNumber, from {} to {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java:      LOG.debug("Updating smallFileDeleteThreadNumber, from {} to {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:    LOG.debug("Starting " + getClass().getSimpleName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:    LOG.debug("ZNodes to watch: {}", paths);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:          LOG.debug("Found no data from " + node);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:        LOG.debug("Set data for remote " + node + ", client zk wather: " + clientZkWatcher);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:        LOG.debug("Failed to set data for {} to client ZK, will retry later", node, e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:      LOG.debug("Delete remote " + node + ", client zk wather: " + clientZkWatcher);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:        LOG.debug("Failed to delete node from client ZK, will retry later", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:    LOG.debug("New ZNodes to watch: {}", newPaths);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:      LOG.debug("Client zk updater for znode {} started", znode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:          LOG.debug("Interrupted while checking whether need to update meta location to client zk");
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:      LOG.debug("Client zk updater for znode {} stopped", znode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java:      LOG.debug(message);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java:      LOG.debug("{} {} came back up, removed it from the dead servers list", what, serverName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java:        LOG.debug("Failed to persist last flushed sequence id of regions"
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetaRegionLocationCache.java:        LOG.debug("Error populating initial meta locations", ke);
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetaRegionLocationCache.java:    LOG.debug("Updating meta znode for path {}: {}", path, opType.name());
./hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetaRegionLocationCache.java:        LOG.debug("Error getting meta location for path {}", path, e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java:        LOG.debug("RSGroup information null for region of table " + tableName, exp);
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:            LOG.debug("Dropping " + el + " during move-to-default rsgroup because not online");
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:        LOG.debug("Read ZK GroupInfo count:" + RSGroupInfoList.size());
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:        LOG.debug("Migrating {} in group {}", tableName, groupInfo.getName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:          LOG.debug("Skip migrating {} since it is already in group {}", tableName,
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:      LOG.debug("Done migrating {}, failed tables {}", groupInfo.getName(), failedTables);
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:      LOG.debug("Refreshing in Online mode.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:      LOG.debug("Refreshing in Offline mode.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:          LOG.debug("Updating znode: " + znode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:      LOG.debug("Writing ZK GroupInfo count: " + zkOps.size());
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:              LOG.debug("Move region {} failed, will retry, current retry time is {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:        LOG.debug("Not running balancer because {} region(s) in transition: {}", groupRIT.size(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java:        LOG.debug("Not running balancer because processing dead regionserver(s): {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcExecutor.java:    LOG.debug("Started handlerCount={} with threadPrefix={}, numCallQueues={}, port={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java:              LOG.debug("Interrupted while sleeping");
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java:          LOG.debug("Caught exception while reading:", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java:          RpcServer.LOG.debug("Created SASL server with mechanism={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java:        RpcServer.LOG.debug("Read input token of size={} for processing by saslServer." +
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java:          RpcServer.LOG.debug("Will send token of size " + replyToken.length
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java:        RpcServer.LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java:            RpcServer.LOG.debug("Received ping message");
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java:        RpcServer.LOG.debug("Connection authorization failed: " + ae.getMessage(), ae);
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/CallRunner.java:          RpcServer.LOG.debug(Thread.currentThread().getName() + ": skipped " + call);
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/CallRunner.java:          RpcServer.LOG.debug(call.toShortString() + ", exception=" + e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/CallRunner.java:          RpcServer.LOG.debug(Thread.currentThread().getName() + ": skipped " + call);
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServerResponder.java:    SimpleRpcServer.LOG.debug(getName() + ": starting");
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServerResponder.java:            SimpleRpcServer.LOG.debug(getName() + ": asyncWrite", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServerResponder.java:            SimpleRpcServer.LOG.debug("Interrupted while sleeping");
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServerResponder.java:        SimpleRpcServer.LOG.debug(conn + ": output error -- closing");
./hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java:          LOG.debug("Caught a ServiceException with null cause", e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java:    LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java:      LOG.debug("Executor service {} already running on {}", this,
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java:        LOG.debug("The remote wal {} has already been deleted?", walFile, e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java:    LOG.debug("Removing {} logs in the list: {}", wals.size(), wals);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java:      LOG.debug("Removing {} logs from remote dir {} in the list: {}", remoteWals.size(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java:            LOG.debug("Start tracking logs for wal group {} for peer {}", logPrefix, peerId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java:        LOG.debug(e.toString(), e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.java:          LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.java:    LOG.debug("EOF, closing {}", currentPath);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.java:        LOG.debug("The provider tells us the valid length for " + currentPath + " is " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.java:          LOG.debug("recover WAL lease: " + path);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java:        LOG.debug("Started replicating mutations.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java:        LOG.debug("Finished replicating mutations.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java:            LOG.debug("Replicating {} bulk loaded data", entry.getKey().toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java:              LOG.debug("Finished replicating {} bulk loaded data", entry.getKey().toString());
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationObserver.java:      LOG.debug("Skipping recording bulk load entries in preCommitStoreFile for bulkloaded "
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java:            LOG.debug("Read {} WAL entries eligible for replication", batch.getNbEntries());
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java:          LOG.debug("Failed to read stream of replication entries: " + e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java:      LOG.debug("Edit null or empty for entry {} ", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java:    LOG.debug("updating TimeStampOfLastAttempted to {}, from entry {}, for source queue: {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java:      LOG.debug("Filtered entry for replication: {}", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java:        LOG.debug("Shipper from source {} got entry batch from reader: {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java:      LOG.debug("Finished recovering queue for group {} of peer {}", walGroupId,
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java:      LOG.debug("Finished queue for group {} of peer {}", walGroupId, source.getQueueId());
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java:          LOG.debug("Interrupted while sleeping for throttling control");
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java:          LOG.debug("Replicated {} entries or {} operations in {} ms",
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.java:            LOG.debug("Failed to copy hfile from " + sourceHFilePath + " to " + localHFilePath
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java:        LOG.debug("{} {} Interrupted while sleeping between retries", msg, logPeerId());
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java:        LOG.debug("HFiles will not be replicated belonging to the table {} family {} to peer id {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java:        LOG.debug("HFiles will not be replicated belonging to the table {} family {} to peer id {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java:        LOG.debug("{} preempted start of shipping worker walGroupId={}", logPeerId(), walGroupId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java:        LOG.debug("{} starting shipping worker for walGroupId={}", logPeerId(), walGroupId);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java:        LOG.debug("{} Interrupted while sleeping between retries", logPeerId());
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java:          LOG.debug("{} Could not connect to Peer ZK. Sleeping for {} millis", logPeerId(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:    LOG.debug("Replication barrier for {}: {}", entry, barrierResult);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:      LOG.debug("{} is before the first barrier, pass", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:          LOG.debug("Parent {} has not been finished yet for entry {}, give up",
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:        LOG.debug("{} is in the last range and the region is opening, give up", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:      LOG.debug("{} is in the first range, pass", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:      LOG.debug("Previous range for {} has not been finished yet, give up", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:      LOG.debug("{} is in the last range and the region is opening, give up", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:    LOG.debug("The previous range for {} has been finished, pass", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:      LOG.debug("{} is beyond the previous end barrier {}, remove from cache", entry,
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java:      LOG.debug("Can not push {}, wait", entry);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DefaultSourceFSConfigurationProvider.java:            LOG.debug(HConstants.REPLICATION_CONF_DIR + " is not configured.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationHFileCleaner.java:            LOG.debug("Found hfile reference in ZK, keeping: " + hfile);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationHFileCleaner.java:            LOG.debug("Did not find hfile reference in ZK, deleting: " + hfile);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java:          LOG.debug("Found up in ZooKeeper, NOT deleting={}", wal);
./hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java:        LOG.debug("Fetch slaves addresses failed", ke);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveTableMonitor.java:      LOG.debug("Already archiving table: " + table + ", ignoring it");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:    LOG.debug("Starting hfile archive tracker...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:    LOG.debug("Finished starting hfile archive tracker!");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:    LOG.debug("Archive node: " + path + " created");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:    LOG.debug("Archive node: " + path + " children changed.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:    LOG.debug("Archive node: " + path + " deleted");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:        LOG.debug(archiveHFileZNode + " znode does exist, checking for tables to archive");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:        LOG.debug("Archiving not currently enabled, waiting");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:    LOG.debug("Updating watches on tables to archive.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:    LOG.debug("Starting archive for tables:" + tables);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java:      LOG.debug("No tables to archive.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveManager.java:    LOG.debug("Disabling backups on all tables.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveManager.java:    LOG.debug("Ensuring archiving znode exists");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveManager.java:    LOG.debug("Creating: " + tableNode + ", data: []");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveManager.java:    LOG.debug("Attempting to delete table node:" + tableNode);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveManager.java:      LOG.debug("Stopping HFileArchiveManager...");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.java:      LOG.debug("Archiver says to [" + (ret ? "delete" : "keep") + "] files for table:" +
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:    LOG.debug("ARCHIVING {}", regionDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:      LOG.debug("Directory {} empty.", regionDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:    LOG.debug("Archiving " + toArchive);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:      LOG.debug("No files to dispose of in {}, family={}", parent.getRegionNameAsString(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:      LOG.debug("No files to dispose of, done!");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:    LOG.debug("Archiving compacted files.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:      LOG.debug("{} already exists in archive, moving to timestamped backup and " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:      LOG.debug("Backed up archive file from " + archiveFile);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:              LOG.debug("Created archive directory {}", archiveDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:    LOG.debug("Archived from {} to {}", currentFile, archiveFile);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:      LOG.debug("Deleted {}", regionDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:    LOG.debug("Failed to delete directory {}", regionDir);
./hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java:    LOG.debug("Deleting files without archiving.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java:      LOG.debug("addLocationsOrderInterceptor configured to false");
./hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java:      LOG.debug("The file system is not a DistributedFileSystem. " +
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:    LOG.debug("MOB compaction table={} cf={} region={} files: {}", tableName, familyName,
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:          LOG.debug("Referenced MOB file={} size={}", mobfile, size);
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:          LOG.debug("Mob file {} was not in location {}. May have other locations to try.", mobfile,
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:                      LOG.debug("Closing output MOB File, length={} file={}, store={}", len,
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:            LOG.debug("Compaction progress: {} {}, rate={} KB/sec, throughputController is {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:        LOG.debug("Aborting writer for {} because of a compaction failure, Store {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:      LOG.debug("New MOB writer created={} store={}", mobFileWriter.getPath().getName(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:      LOG.debug("Commit or abort size={} mobCells={} major={} file={}, store={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:        LOG.debug("Aborting writer for {} because there are no MOB cells, store={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java:      LOG.debug("Mob file writer is null, skipping commit/abort, store=",
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java:      LOG.debug("Flush store file: {}, store: {}", writer.getPath(), getStoreInfo());
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/ManualMobMaintHFileCleaner.java:        LOG.debug("Had to calculate name of mob region for table {} and it is {}", tableName,
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/ManualMobMaintHFileCleaner.java:        LOG.debug("Keeping file '{}' because it is from mob dir", fStat.getPath());
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java:          LOG.debug("Checking file {}", fileName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java:            LOG.debug("{} is an expired file", fileName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java:          LOG.debug("Serialized MOB file refs array was treated as the placeholder 'no entries' but"
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java:                LOG.debug("Table={} cf ={}: batch MOB compaction is disabled, {}=0 -"+
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java:              LOG.debug("Skipping table={} column family={} because it is not MOB-enabled",
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java:      LOG.debug(
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java:        LOG.debug("MobFileCache enabled with cacheSize=" + mobFileMaxCacheSize +
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java:      LOG.debug("Still running " + runnables);
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java:                    LOG.debug("Skipping file without MOB references (bulkloaded file):{}", pp);
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java:                  LOG.debug("Found {} mob references for store={}", mobs.size(), sf);
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java:      LOG.debug("Found: {} active mob refs for table={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java:      LOG.debug("Skipping archiving old MOB files - no files found for table={} cf={}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java:      LOG.debug("MOB Cleaner is archiving: {}", p);
./hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java:          LOG.debug("Loading constraint:" + key);
./hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java:          LOG.debug("Constraint: {} is DISABLED - skipping it", key);
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java:    LOG.debug("Copying coprocessor jar '{}' to '{}'.", path, tempPath);
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java:    LOG.debug("Validating class '{}'.", className);
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java:      LOG.debug("Validating table {}", tableDescriptor.getTableName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java:    LOG.debug("Classpath: {}", urlList);
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:        LOG.debug("rawScan {} for {}", rawScanEnabled, region.getTable());
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:      LOG.debug("Reading from {} {} {} {}", region.getTable(), region.getRegionNameAsString(),
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:        LOG.debug("Reading table descriptor for table {}", region.getTable());
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:        LOG.debug("sniffRegion {} of {} failed", region.getEncodedName(), e);
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:          LOG.debug("Writing to {} {} {} {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:        LOG.debug("Reading from {} {} {} {}",
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:        LOG.debug("The targeted table was disabled.  Assuming success.");
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:          LOG.debug(String.format("reading list of tables"));
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:      LOG.debug("Reading list of tables");
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:    LOG.debug("Checking table is enabled and getting table descriptor for table {}", tableName);
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:    LOG.debug("Reading list of regions for table {}", tableDesc.getTableName());
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:      LOG.debug("Reading list of tables");
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java:        LOG.debug("Reading list of tables and locations");
./hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java:      LOG.debug("encountered", fnfe);
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java:    LOG.debug(writer.toString());
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java:    LOG.debug(writer.toString());
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java:    LOG.debug(writer.toString());
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java:            LOG.debug("tableName=" + tableName +
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java:        LOG.debug(" split " + i + ": " + Bytes.toStringBinary(splits[i]));
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java:          LOG.debug("Interrupted, continuing" + e.toString());
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestTableResource.java:      LOG.debug("looking for region " + region.getName());
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/TestTableResource.java:        LOG.debug("comparing to region " + hriRegionName);
./hbase-rest/src/test/java/org/apache/hadoop/hbase/rest/client/TestXmlParsing.java:      LOG.debug("exception text: '" + exceptionText + "'", e);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java:        LOG.debug("port set to " + val);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java:        LOG.debug("readonly set to true");
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java:        LOG.debug("WEB UI port set to " + val);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java:        LOG.debug("Skipping Kerberos login for REST server");
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("Exception received while closing the table", ioe);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("Exception received while closing the table", ioe);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("Exception received while closing the table", ioe);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("Exception received while closing the table", ioe);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("Exception received while closing the table", ioe);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("APPEND " + append.toString());
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("Exception received while closing the table" + table.getName(), ioe);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("INCREMENT " + increment.toString());
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java:        LOG.debug("Exception received while closing the table " + table.getName(), ioe);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java:    LOG.debug("classpath " + clspath);
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java:      LOG.debug("Performing negotiation with the server.");
./hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Response.java:        LOG.debug("encountered ioe when obtaining body", ioe);
./hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java:        LOG.debug("error clearing the jmx it appears the metrics system hasn't been started",
./hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/metrics/impl/GlobalMetricRegistriesAdapter.java:          LOG.debug("Registering adapter for the MetricRegistry: " + info.getMetricsJmxContext());
./hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/metrics/impl/GlobalMetricRegistriesAdapter.java:          LOG.debug("Removing adapter for the MetricRegistry: " + info.getMetricsJmxContext());
./hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java:    LOG.debug("Creating new MetricsTableSourceImpl for table '{}'", tblName);
./hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java:    LOG.debug("Creating new MetricsRegionSourceImpl for table " +
./hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java:      LOG.debug("Creating new MetricsUserSourceImpl for user " + user);
./hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java:      LOG.debug("Removing user Metrics for user: " + user);
./hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java:    LOG.debug("Listing beans for {}", qry);
./hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java:      LOG.debug("getting attribute " + attName + " of " + oname + " threw an exception", e);
./hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java:      LOG.debug("doAsUser = {}, RemoteUser = {} , RemoteAddress = {} ",
./hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java:          LOG.debug("Proxy user Authentication successful");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDDLMasterFailover.java:    LOG.debug("Initializing/checking cluster has " + SERVER_COUNT + " servers");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDDLMasterFailover.java:    LOG.debug("Done initializing/checking cluster");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDDLMasterFailover.java:      LOG.debug("Setting up connection ...");
./hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java:          LOG.debug("Persisting current.length={}, count={}, id={}, current={}, i=",
./hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestZKAndFSPermissions.java:      LOG.debug("Caught exception for missing znode", ex);
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestMobCompaction.java:    LOG.debug("Initializing/checking cluster has {} servers",regionServerCount);
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestMobCompaction.java:    LOG.debug("Done initializing/checking cluster");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestMobCompaction.java:      LOG.debug("MOB Directory content: {}", st.getPath());
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestMobCompaction.java:    LOG.debug("MOB Directory content total files: {}", stat.length);
./hbase-it/src/test/java/org/apache/hadoop/hbase/CoprocClusterManager.java:        LOG.debug("Executed remote command: {}, exit code:{} , output:{}", command, resp.getExitCode(),
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java:      LOG.debug("Created PerformanceEvaluationCallable with args: " + argv);
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java:    LOG.debug(MoreObjects.toStringHelper("Parsed Options")
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java:        LOG.debug(desc + "{" + YammerHistogramUtils.getHistogramReport(r.hist) + "}");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java:    LOG.debug("Populating table.");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java:      LOG.debug("Launching non-replica job " + (i + 1) + "/" + maxIters);
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java:    LOG.debug("Altering " + tableName + " replica count to " + replicaCount);
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestRegionReplicaPerf.java:      LOG.debug("Launching replica job " + (i + 1) + "/" + maxIters);
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestBase.java:      LOG.debug("noClusterCleanUp is set, skip restoring the cluster");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestBase.java:    LOG.debug("Restoring the cluster");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestBase.java:    LOG.debug("Done restoring the cluster");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java:    LOG.debug("Initializing/checking cluster has " + SERVER_COUNT + " servers");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java:    LOG.debug("Done initializing/checking cluster");
./hbase-it/src/test/java/org/apache/hadoop/hbase/StripeCompactionsPerformanceEvaluation.java:    LOG.debug("Initializing/checking cluster has " + MIN_NUM_SERVERS + " servers");
./hbase-it/src/test/java/org/apache/hadoop/hbase/StripeCompactionsPerformanceEvaluation.java:    LOG.debug("Done initializing/checking cluster");
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestingUtility.java:    LOG.debug("Setting {} to {} since it is a distributed cluster",
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:    LOG.debug("Performing start of {} on {}:{}", service, hostname, port);
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:    LOG.debug("Performing stop of {} on {}:{}", service, hostname, port);
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:    LOG.debug("Performing stop followed by start of {} on {}:{}", service, hostname, port);
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:    LOG.debug("Issuing isRunning request against {} on {}:{}", service, hostname, port);
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:    LOG.debug("Command {} of {} on {} submitted as commandId {}",
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:    LOG.debug("Command {} of {} on {} submitted as commandId {} completed successfully.",
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:          LOG.debug("command {} is still active.", commandId);
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:          LOG.debug("command {} is still active.", commandId);
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:        LOG.debug("command {} completed as {}.", commandId, isSuccess);
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:          LOG.debug("execution failed with exception. Retrying.", e);
./hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:          LOG.debug("execution failed with exception. Retrying.", e);
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestBackupRestore.java:    LOG.debug("Initializing/checking cluster has {} servers",regionServerCount);
./hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestBackupRestore.java:    LOG.debug("Done initializing/checking cluster");
./hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java:    LOG.debug("Ignoring setConf call.");
./hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestTableSnapshotInputFormat.java:      LOG.debug("Running job with mapreduce API.");
./hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestTableSnapshotInputFormat.java:      LOG.debug("Running job with mapred API.");
./hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestBulkLoad.java:      LOG.debug("Region Replicas enabled: " + replicaCount);
./hbase-it/src/test/java/org/apache/hadoop/hbase/MockHttpApiRule.java:      LOG.debug("Registering responder to '{}'", pathRegex);
./hbase-it/src/test/java/org/apache/hadoop/hbase/MockHttpApiRule.java:      LOG.debug("Clearing registrations");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java:    LOG.debug("List files in {} in root {} at {}", fs, root, dir);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java:        LOG.debug(Objects.toString(fstat.getPath()));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java:    LOG.debug("Added authentication properties to config successfully.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java:      LOG.debug(MoreObjects.toStringHelper("needsDelete")
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java:            LOG.debug(" split " + i + ": " + Bytes.toStringBinary(splits[i]));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java:      LOG.debug("get location for region: " + hRegionLocation);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java:        LOG.debug("deleting region from meta: " + regionInfo);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapred/TestTableInputFormat.java:    LOG.debug("submitting job.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatTestBase.java:      LOG.debug("map: key -> " + Bytes.toStringBinary(key.get()) +
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatTestBase.java:        LOG.debug("reduce: key[" + count + "] -> " +
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALRecordReader.java:    LOG.debug("log="+logDir+" file="+ split.getLogFileName());
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java:    LOG.debug(String.format("Wrote test data to file: %s", inputPath));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java:      LOG.debug("Forcing combiner.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java:    LOG.debug("Running ImportTsv with arguments: " + argv);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithTTLs.java:        LOG.debug("Deleting test subdirectory");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultithreadedTableMapper.java:        LOG.debug("Verification attempt failed: " + e.getMessage());
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultithreadedTableMapper.java:            LOG.debug("second key is not the reverse of first. row=" +
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java:    LOG.debug("row count duration (ms): " + duration);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestRowCounter.java:    LOG.debug("row count duration (ms): " + duration);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java:      LOG.debug("Storage policy of cf 0: [" + spA + "].");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java:      LOG.debug("Storage policy of cf 1: [" + spB + "].");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java:      LOG.debug("Storage policy of cf 0: [" + spA + "].");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat2.java:      LOG.debug("Storage policy of cf 1: [" + spB + "].");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHashTable.java:      LOG.debug("partition: " + Bytes.toInt(bytes.get()));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHashTable.java:        LOG.debug("Key: " + (keyString.isEmpty() ? "-1" : Integer.parseInt(keyString, 16))
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHashTable.java:      LOG.debug("Output file: " + file.getPath());
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHashTable.java:      LOG.debug("Data file: " + file.getPath());
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java:    LOG.debug(String.format("Wrote test data to file: %s", inputPath));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java:      LOG.debug("Forcing combiner.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java:    LOG.debug("Running ImportTsv with arguments: " + argv);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java:      LOG.debug("Deleting test subdirectory");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java:    LOG.debug("Validating table.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java:            LOG.debug("Getting results " + res.size());
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithOperationAttributes.java:          LOG.debug("allow any put to happen " + region.getRegionInfo().getRegionNameAsString());
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:      LOG.debug("SOURCE row: " + (sourceRow == null ? "null" : Bytes.toInt(sourceRow.getRow()))
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:      LOG.debug("TARGET row: " + (targetRow == null ? "null" : Bytes.toInt(targetRow.getRow()))
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:        LOG.debug("Source cells: " + Arrays.toString(sourceCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:        LOG.debug("Target cells: " + Arrays.toString(targetCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:          LOG.debug("Source cell: " + sourceCell + " target cell: " + targetCell);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:      LOG.debug("SOURCE row: " + (sourceRow == null ? "null"
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:      LOG.debug("TARGET row: " + (targetRow == null ? "null"
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:          LOG.debug("Source cells: " + Arrays.toString(sourceCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:          LOG.debug("Target cells: " + Arrays.toString(targetCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:          LOG.debug("Source cells: " + Arrays.toString(sourceCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:          LOG.debug("Target cells: " + Arrays.toString(targetCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:          LOG.debug("Source cell: " + sourceCell + " target cell: "
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:      LOG.debug("SOURCE row: " + (sourceRow == null ?
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:      LOG.debug("TARGET row: " + (targetRow == null ?
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:      LOG.debug("rowsCount: " + rowsCount);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:        LOG.debug("Source cells: " + Arrays.toString(sourceCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:        LOG.debug("Target cells: " + Arrays.toString(targetCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:          LOG.debug("Source cells: " + Arrays.toString(sourceCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:          LOG.debug("Target cells: " + Arrays.toString(targetCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:        LOG.debug("Source cells: " + Arrays.toString(sourceCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:        LOG.debug("Target cells: " + Arrays.toString(targetCells));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSyncTable.java:            LOG.debug(
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatTestBase.java:      LOG.debug("Ensuring table doesn't exist.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:    LOG.debug("Validating table after delete.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:    LOG.debug(String.format("Wrote test data to file: %s", inputPath));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:      LOG.debug("Forcing combiner.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:    LOG.debug("Running ImportTsv with arguments: " + argv);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:    LOG.debug("validating the table " + createdHFiles);
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:      LOG.debug("Deleting test subdirectory");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:    LOG.debug("Validating HFiles.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:      LOG.debug("The output path has files");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:    LOG.debug("Validating table.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java:          LOG.debug("Getting results " + res.size());
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java:    LOG.debug(String.format("Wrote test data to file: %s", inputPath));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java:      LOG.debug("Forcing combiner.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java:    LOG.debug("Running ImportTsv with arguments: " + Arrays.toString(argsArray));
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java:      LOG.debug("Deleting test subdirectory");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java:    LOG.debug("Validating table.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java:    LOG.debug("Validating HFiles.");
./hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableInputFormat.java:    LOG.debug("submitting job.");
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java:      LOG.debug("Injecting failure. Count: " + testing.injectedFailureCount);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java:        LOG.debug("export split=" + i + " size=" + StringUtils.humanReadableInt(sizeGroups[i]));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java:    LOG.debug("inputFs={}, inputRoot={}", inputFs.getUri().toString(), inputRoot);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java:    LOG.debug("outputFs={}, outputRoot={}, skipTmp={}, initialOutputSnapshotDir={}",
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java:        LOG.debug("TIFB.restart, firstRow: " +
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java:      LOG.debug("TIFB.restart, firstRow: " +
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java:        LOG.debug("recovered from " + StringUtils.stringifyException(e));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java:          LOG.debug("cell is not a mob ref, even though we asked for only refs. cell={}", c);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java:        LOG.debug("Found file '{}' in mob area", file);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java:              LOG.debug("Found file '{}' in archive area. has proper hlink back references to "
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java:                LOG.debug("Target file does not exist for ref {}", tmp);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java:              LOG.debug("Found file '{}' as a ref in the mob area: {}", file, found);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java:            LOG.debug("Note that we don't have the server-side tag from the mob cells that says "
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java:          LOG.debug("getSplits: split -> " + i + " -> " + split);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java:          LOG.debug("Hash mismatch.  Key range: " + toHex(targetHasher.getBatchStartKey())
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java:            LOG.debug("Target missing row: " + Bytes.toString(nextSourceRow));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java:            LOG.debug("Source missing row: " + Bytes.toString(nextTargetRow));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java:            LOG.debug("Target missing cell: " + sourceCell);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java:            LOG.debug("Source missing cell: " + targetCell);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java:              LOG.debug("Different values: ");
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java:              LOG.debug("  source cell: " + sourceCell
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java:              LOG.debug("  target cell: " + targetCell
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java:              LOG.debug("First rowkey: [{}]", Bytes.toString(rowKey));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java:                LOG.debug("Use favored nodes writer: {}", initialIsa.getHostString());
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java:          LOG.debug("SplitPoint startkey for " + tableName + ": " + Bytes.toStringBinary(fullKey));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java:                  LOG.debug("getSplits: split -> " + (count++) + " -> " + split);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java:        LOG.debug("Dry mode: Table: " + tableName + " already disabled, so just deleting it.");
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java:      LOG.debug("Configuring multithread runner to use " + numberOfThreads +
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java:    LOG.debug("Scanning " + dir.toString() + " for WAL files");
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java:      LOG.debug("No configured filter class, accepting all keyvalues.");
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java:    LOG.debug("Attempting to create filter:" + filterClass);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java:    LOG.debug(String.format("For class %s, using jar %s", my_class.getName(), jar));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java:      LOG.debug("add incremental job :" + hfileOutPath + " from " + inputDirs);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java:      LOG.debug("Created new MultiTableRecordReader with WAL "
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java:        LOG.debug("Opening HTable \"" + Bytes.toString(tableName.get())+ "\" for writing");
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RegionSizeCalculator.java:          LOG.debug("Region " + regionLoad.getNameAsString() + " has size " + regionSizeBytes);
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RegionSizeCalculator.java:    LOG.debug("Region sizes calculated");
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RegionSizeCalculator.java:      LOG.debug("Unknown region:" + Arrays.toString(regionId));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/PutCombiner.java:            LOG.debug(String.format("Combined %d Put(s) into %d.", cnt, 1));
./hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/PutCombiner.java:        LOG.debug(String.format("Combined %d Put(s) into %d.", cnt, 1));
./hbase-shell/src/test/java/org/apache/hadoop/hbase/client/AbstractTestShell.java:    LOG.debug("Configure jruby runtime, cluster set to {}", TEST_UTIL);
./hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java:        LOG.debug("MemcachedBlockCache can not cache Cacheable's of type "
./hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java:        LOG.debug("Exception pulling from memcached [ "
./hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java:        LOG.debug("Error deleting " + cacheKey.toString(), e);
