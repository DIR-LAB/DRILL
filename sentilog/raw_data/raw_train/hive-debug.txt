./druid-handler/src/java/org/apache/hadoop/hive/druid/security/DruidKerberosUtil.java:      LOG.debug("Got valid challenge for host {}", serverName);
./druid-handler/src/java/org/apache/hadoop/hive/druid/security/KerberosHttpClient.java:        LOG.debug("No Auth Cookie found for URI{}. Existing Cookies{} Authenticating... ",
./druid-handler/src/java/org/apache/hadoop/hive/druid/security/KerberosHttpClient.java:        LOG.debug("The user credential is {}", currentUser);
./druid-handler/src/java/org/apache/hadoop/hive/druid/security/KerberosHttpClient.java:        LOG.debug("Found Auth Cookie found for URI {} cookie {}",
./druid-handler/src/java/org/apache/hadoop/hive/druid/security/KerberosHttpClient.java:        LOG.debug("Preparing for Retry boolean {} and result {}, object{} ", true,
./druid-handler/src/java/org/apache/hadoop/hive/druid/security/RetryIfUnauthorizedResponseHandler.java:    LOG.debug("UnauthorizedResponseHandler - Got response status {}", httpResponse.getStatus());
./druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidQueryBasedInputFormat.java:    LOG.debug("sending request {} to query for segments", request);
./druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidOutputFormat.java:    LOG.debug(String.format("running with Data schema [%s] ", dataSchema));
./druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java:    LOG.debug("pre-create data source with name {}", dataSourceName);
./druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java:        LOG.debug("No Kafka Supervisor found for datasource[%s]", dataSourceName);
./druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java:    LOG.debug("checking load status from coordinator {}", coordinatorAddress);
./druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java:          LOG.debug("Checking segment [{}] response is [{}]", input, result);
./druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java:    LOG.debug("commit insert into table {} overwrite {}", table.getTableName(), overwrite);
./druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java:      LOG.debug("Setting {} to {} to enable split generation on HS2",
./druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java:    LOG.debug("Supplying SQL Connector with DB type {}, URI {}, User {}", dbType, uri, username);
./druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandlerUtils.java:      LOG.debug("Request[%s] received redirect response to location [%s].", request.getUrl(), redirectUrlStr);
./druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidQueryRecordReader.java:      LOG.debug("Retrieving data from druid location[{}] using query:[{}] ", address, query);
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/AccumuloIndexedOutputFormat.java:        LOG.debug("Adding table: {}", tableName);
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/AccumuloIndexedOutputFormat.java:      LOG.debug("mutations written: {}, values written: {}", this.mutCount, this.valCount);
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java:          LOG.debug("Adding Hadoop Token for Accumulo to Job's Credentials: " + accumuloToken);
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java:        LOG.debug("Adding Hadoop Token for Accumulo to Job's Credentials");
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java:        LOG.debug("All job tokens: " + jobConf.getCredentials().getAllTokens());
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloDefaultIndexScanner.java:        LOG.debug("Searching tab=" + indexTable + " column=" + column + " range=" + indexRange);
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloDefaultIndexScanner.java:        LOG.debug("Using Column Family=" + toString());
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloDefaultIndexScanner.java:          LOG.debug("Found 0 index matches");
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloDefaultIndexScanner.java:          LOG.debug("Found " + rowIds.size() + " index matches");
./accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloDefaultIndexScanner.java:    LOG.debug("Index lookup failed for table " + indexTable);
./kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaRecordReader.java:      LOG.debug("Consumer poll timeout [{}] ms", pollTimeout);
./kafka-handler/src/java/org/apache/hadoop/hive/kafka/RetryUtils.java:      LOG.debug(fullMessage, e);
./kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaScanTrimmer.java:        LOG.debug("Optimized scan:");
./kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaScanTrimmer.java:        optimizedScan.forEach((tp, input) -> LOG.debug(
./kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaScanTrimmer.java:        LOG.debug("No optimization thus using full scan ");
./kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaScanTrimmer.java:        fullHouse.forEach((tp, input) -> LOG.debug(
./kafka-handler/src/java/org/apache/hadoop/hive/kafka/VectorizedKafkaRecordReader.java:    LOG.debug("Consumer poll timeout [{}] ms", pollTimeout);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java:    LOG.debug("List of stats columns before analyze Part1: " + txnHandler.findColumnsWithStats(ci));
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java:    LOG.debug("List of stats columns after analyze Part1: " + txnHandler.findColumnsWithStats(ci));
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java:    LOG.debug("List of stats columns before analyze Part2: " + txnHandler.findColumnsWithStats(ci));
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java:    LOG.debug("List of stats columns after analyze Part2: " + txnHandler.findColumnsWithStats(ci));
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java:    LOG.debug(msg);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java:      LOG.debug(" rowIdx=" + rowIdx++ + ":" + row);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java:    LOG.debug("Executing: " + cmd);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorTestUtil.java:    LOG.debug("Executing: " + cmd);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerCheckInvocation.java:    LOG.debug("Got privilege object " + tableObj);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerCheckInvocation.java:    LOG.debug("Got privilege object " + tableObj);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java:                  LOG.debug("Verifier - Function: " + String.valueOf(args.funcName));
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java:          LOG.debug("Verifier - Function: " + String.valueOf(args.funcName));
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + parts.size());
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetastoreTransformer.java:      LOG.debug("Return list size=" + extTables.size() + ",bitValue=" + requestedFields);
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestTenantBasedStorageHierarchy.java:      LOG.debug("Catalog does not exist, creating a new one");
./itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestTenantBasedStorageHierarchy.java:          LOG.debug("Catalog " + catalog + " created");
./itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeelinePasswordOption.java:      LOG.debug(output);
./itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliAdapter.java:            LOG.debug("will initialize metastore database in class rule");
./itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliAdapter.java:                LOG.debug("will destroy metastore database in class rule (if not derby)");
./itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliAdapter.java:              LOG.debug("will initialize metastore database in test rule");
./itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliAdapter.java:                LOG.debug("will destroy metastore database in test rule (if not derby)");
./itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java:    LOG.debug(s);
./itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java:    LOG.debug("Executing " + q1);
./llap-common/src/java/org/apache/hadoop/hive/llap/security/LlapTokenSelector.java:      LOG.debug("Looking for a token with service " + service);
./llap-common/src/java/org/apache/hadoop/hive/llap/security/LlapTokenSelector.java:        LOG.debug("Token = " + token.getKind() + "; service = " + token.getService());
./llap-common/src/java/org/apache/hadoop/hive/llap/io/ChunkedInputStream.java:    LOG.debug("Creating chunked input for {}", id);
./llap-common/src/java/org/apache/hadoop/hive/llap/io/ChunkedInputStream.java:    LOG.debug("{}: Closing chunked input.", id);
./llap-common/src/java/org/apache/hadoop/hive/llap/io/ChunkedInputStream.java:          LOG.debug("{}: Chunk size {}", id, unreadBytes);
./llap-common/src/java/org/apache/hadoop/hive/llap/io/ChunkedInputStream.java:          LOG.debug("{}: Hit end of data", id);
./llap-common/src/java/org/apache/hadoop/hive/llap/io/ChunkedOutputStream.java:    LOG.debug("Creating chunked input stream: {}", id);
./llap-common/src/java/org/apache/hadoop/hive/llap/io/ChunkedOutputStream.java:    LOG.debug("{}: Closing underlying output stream.", id);
./llap-common/src/java/org/apache/hadoop/hive/llap/io/ChunkedOutputStream.java:      LOG.debug("{}: Writing chunk of size {}", id, bufPos);
./llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java:      LOG.debug("Getting host proxies for {}", hostId);
./llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java:        LOG.debug("Creating a client without a token for " + nodeId);
./llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java:      LOG.debug("Creating a client for " + nodeId + "; the token is " + nodeToken);
./spark-client/src/test/java/org/apache/hive/spark/client/rpc/TestRpc.java:    LOG.debug("Transferring messages...");
./spark-client/src/test/java/org/apache/hive/spark/client/rpc/TestRpc.java:        LOG.debug("Error while connecting to port " + expectedPort + " retrying: " + e.getMessage());
./spark-client/src/test/java/org/apache/hive/spark/client/rpc/TestRpc.java:        LOG.debug("Error while connecting to port " + expectedPort + " retrying");
./spark-client/src/test/java/org/apache/hive/spark/client/rpc/TestRpc.java:    LOG.debug("Transferred {} outbound client messages.", count);
./spark-client/src/test/java/org/apache/hive/spark/client/rpc/TestRpc.java:    LOG.debug("Transferred {} outbound server messages.", count);
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:      LOG.debug("Remote Spark Driver configured with: " + e._1() + "=" + e._2());
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:      LOG.debug("Send error to Client: {}", Throwables.getStackTraceAsString(error));
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:      LOG.debug("Send error to Client: {}", cause);
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:      LOG.debug("Send job({}) result to Client.", jobId);
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:      LOG.debug("Send job({}/{}) submitted to Client.", jobId, sparkJobId);
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:      LOG.debug("Send task({}/{}/{}/{}) metric to Client.", jobId, sparkJobId, stageId, taskId);
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:      LOG.debug("Shutting down due to EndSession request.");
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:      LOG.debug("Received client job request {}", msg.id);
./spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java:          LOG.debug("Client job {}: {} of {} Spark jobs finished.",
./spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcDispatcher.java:    LOG.debug("[{}] Discarding failed RPC {}.", name(), id);
./spark-client/src/main/java/org/apache/hive/spark/client/rpc/SaslHandler.java:    LOG.debug("Handling SASL challenge message...");
./spark-client/src/main/java/org/apache/hive/spark/client/rpc/SaslHandler.java:      LOG.debug("Sending SASL challenge response...");
./spark-client/src/main/java/org/apache/hive/spark/client/rpc/SaslHandler.java:    LOG.debug("SASL negotiation finished with QOP {}.", qop);
./spark-client/src/main/java/org/apache/hive/spark/client/AbstractSparkClient.java:      LOG.debug("Interrupted before driver thread was finished.");
./spark-client/src/main/java/org/apache/hive/spark/client/AbstractSparkClient.java:      LOG.debug("Send JobRequest[{}].", jobId);
./spark-client/src/main/java/org/apache/hive/spark/client/AbstractSparkClient.java:        LOG.debug("Received result for client job {}", msg.id);
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java:      LOG.debug(Bytes.toStringBinary(startRow) + " ~ " + Bytes.toStringBinary(stopRow));
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDeHelper.java:      LOG.debug("Generated columns: [" + sb.toString() + "]");
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDeHelper.java:      LOG.debug("Generated column types: [" + sb.toString() + "]");
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java:    LOG.debug("HBaseSerDe initialized with : {}", serdeParams);
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseTableSnapshotInputFormatUtil.java:      LOG.debug("Probably don't support table snapshots. Returning null instance.", e);
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java:      LOG.debug("Ignoring residual predicate " + residualPredicate.getExprString());
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java:      LOG.debug("Using TableSnapshotInputFormat");
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java:    LOG.debug("Using HiveHBaseTableInputFormat");
./hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java:    LOG.debug("Configuring JobConf for table {}.{}", tableDesc.getDbName(), tableDesc.getTableName());
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/metrics/BlacklistingLlapMetricsListener.java:      LOG.debug("Checking node {} with data: " +
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/metrics/BlacklistingLlapMetricsListener.java:    LOG.debug("Found slowest node {} with data: " +
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/metrics/BlacklistingLlapMetricsListener.java:          LOG.debug("Trying to blacklist node: " + maxAverageTimeIdentity);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/metrics/BlacklistingLlapMetricsListener.java:          LOG.debug("Can not blacklist node: " + maxAverageTimeIdentity, t);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/metrics/BlacklistingLlapMetricsListener.java:      LOG.debug("Got result for lock check: {}", lockResult);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/metrics/BlacklistingLlapMetricsListener.java:      LOG.debug("Skipping check. Current time {} and we are waiting for {}.", currentTime, nextCheckTime);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapContainerLauncher.java:      LOG.debug("No-op launch for container: " +
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapContainerLauncher.java:      LOG.debug("No-op stopContainer invocation for containerId={}",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/endpoint/LlapPluginSecurityInfo.java:      LOG.debug("Trying to get KerberosInfo for " + protocol);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/endpoint/LlapPluginSecurityInfo.java:      LOG.debug("Trying to get TokenInfo for " + protocol);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:        LOG.debug("GetTotalResources: numInstancesFound={}, totalMem={}, totalVcores={}",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:        LOG.debug("GetAvailableResources: numInstancesFound={}, totalMem={}, totalVcores={}",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:      LOG.debug("Processing deallocateTask for task={}, taskSucceeded={}, endReason={}", task,
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:      LOG.debug("Ignoring deallocateContainer for containerId: {}",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:      LOG.debug("selectingHost for task={} on hosts={}", request.task,
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:      LOG.debug("ShouldDelayForLocality={} for task={} on hosts={}", shouldDelayForLocality,
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:                        LOG.debug("Host={} will not become available within requested timeout", nodeInfo);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:              LOG.debug("Delaying local allocation for [" + request.task +
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:              LOG.debug("Skipping local allocation for [" + request.task +
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:          LOG.debug("No-locality requested. Selecting a random host for task={}", request.task);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:          LOG.debug("Requested node [{}] in consistent order does not exist. Falling back to random selection for " +
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:            LOG.debug("Assigning {} in consistent order when looking for first requested host, from #hosts={},"
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:            LOG.debug("Node: " + nodeInfo.toShortString() +
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:            LOG.debug("Ignoring disableNode invocation for null NodeInfo");
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:        LOG.debug("ScheduleRun: {}", constructPendingTaskCountsLogMessage());
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:            LOG.debug("ScheduleResult for Task: {} = {}", taskInfo, scheduleResult);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:                LOG.debug("Attempting to preempt on requested host for task={}, potentialHosts={}",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:                  LOG.debug(
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:                  LOG.debug("Attempting to preempt for {} on potential hosts={}. TotalPendingPreemptions={}",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:                  LOG.debug("Not preempting for {} on potential hosts={}. An existing preemption request exists",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:              LOG.debug("Attempting to preempt on any host for task={}, pendingPreemptions={}",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:                  LOG.debug(
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:                  LOG.debug(
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:          LOG.debug(
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java:        LOG.debug("No tasks qualify as killable to schedule tasks at priority {}. Current priority={}",
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java:        LOG.debug("Received heartbeat from [" + hostname + ":" + port +" (" + uniqueId +")]");
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java:        LOG.debug("Registering " + containerId + ", " + taskAttemptId + " for node: " + host + ":" + port);
./llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java:        LOG.debug("Registering " + containerId + " for node: " + hostname + ":" + port);
./llap-client/src/java/org/apache/hadoop/hive/registry/impl/ZkRegistryBase.java:    LOG.debug("Added path={}, host={} instance={} to cache."
./llap-client/src/java/org/apache/hadoop/hive/registry/impl/ZkRegistryBase.java:    LOG.debug("Removed path={}, host={} from cache."
./llap-client/src/java/org/apache/hadoop/hive/registry/impl/ZkRegistryBase.java:      LOG.debug("Returning " + byHost.size() + " hosts for locality allocation on " + host);
./llap-client/src/java/org/apache/hadoop/hive/llap/LlapRowRecordReader.java:          LOG.debug("Error deserializing row from data: " + data);
./llap-client/src/java/org/apache/hadoop/hive/llap/LlapBaseRecordReader.java:          LOG.debug("Interrupting reader thread due to reader event with error " + event.getMessage());
./llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java:        LOG.debug("Received terminate response for " + taskAttemptId);
./llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java:        LOG.debug("Received heartbeat from container, request=" + request);
./llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java:        LOG.debug("Heartbeat from " + taskAttemptIdString +
./llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java:            LOG.debug("Task completed event for " + taskAttemptIdString);
./llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java:            LOG.debug("Task failed event for " + taskAttemptIdString);
./llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java:            LOG.debug("Task update event for " + taskAttemptIdString);
./llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java:        LOG.debug("Node heartbeat from " + hostname + ":" + port + ", " + uniqueId);
./llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java:        LOG.debug("Can not get the current configuration lock time");
./llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java:        LOG.debug("Can not lock window {}-{}. Current value is {}.", windowStart, windowEnd, current.postValue());
./llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java:        LOG.debug("Can not lock window {}-{}. Current value is changed to {}.", windowStart, windowEnd,
./llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:        LOG.debug("Setting attempt number to: {}, task number to: {} from given taskAttemptId: {} in conf",
./llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:              LOG.debug("Executing session query: {}", q);
./llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:      LOG.debug("Closing {} connections for handle ID {}", handleConnections.size(), handleId);
./llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:      LOG.debug("No connection found for handle ID {}", handleId);
./llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:    LOG.debug("Closing all handles");
./llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:        LOG.debug("Sending queued event to record reader: " + readerEvent.getEventType());
./llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java:          LOG.debug("No registered record reader, queueing event " + readerEvent.getEventType()
./hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/HCatDataCheckUtil.java:    LOG.debug("Hive conf : {}", hiveConf.getAllProperties());
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java:      LOG.debug("Error closing metastore client. Ignored the error.", e);
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java:      LOG.debug("Using a version of guava <12.0. Stats collection is enabled by default.");
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java:              LOG.debug("Evicting client: " + Integer.toHexString(System.identityHashCode(hiveMetaStoreClient)));
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java:        LOG.debug("Cleaning up hive client cache in ShutDown hook");
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordObjectInspectorFactory.java:      LOG.debug("Got asked for OI for {} [{} ]", typeInfo.getCategory(), typeInfo.getTypeName());
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordObjectInspectorFactory.java:      LOG.debug("Got asked for OI for {}, [{}]", typeInfo.getCategory(), typeInfo.getTypeName());
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:      LOG.debug("moveTaskOutputs "
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:            LOG.debug("Testing if moving file: [" + file + "] to ["
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:          LOG.debug("Moving file: [ " + file + "] to [" + finalOutputPath + "]");
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:              LOG.debug("Moving directory: " + file + " to " + parentDir);
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:        LOG.debug("FinalPath(file:"+file+":"+src+"->"+dest+"="+itemDest);
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/PartInfo.java:          LOG.debug("Can't suppress data-schema. Partition-schema and table-schema seem to differ! "
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/PartInfo.java:          LOG.debug("Partition's storageHandler (" + storageHandlerClassName + ") " +
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/PartInfo.java:          LOG.debug("Partition's InputFormat (" + inputFormatClassName + ") " +
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/PartInfo.java:          LOG.debug("Partition's OutputFormat (" + outputFormatClassName + ") " +
./hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/PartInfo.java:          LOG.debug("Partition's SerDe (" + serdeClassName + ") " +
./hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java:    LOG.debug("+getStatus()");
./hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java:    LOG.debug("-getStatus()");
./hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java:    LOG.debug("+listDataBases()");
./hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java:    LOG.debug("-listDataBases()");
./hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java:      LOG.debug(type + ": " + method.getURI().getEscapedURI());
./hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java:      LOG.debug("Http Status Code=" + httpStatus);
./hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java:      LOG.debug("response: " + resp);
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java:          LOG.debug("Getting tokens for " + uri);
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JobRequestExecutor.java:    LOG.debug("Starting new " + type + " job request with time out " + this.requestExecutionTimeoutInSec
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JobRequestExecutor.java:    LOG.debug("Completed " + type + " job request.");
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ProxyUserSupport.java:            LOG.debug("User [" + proxyUser + "] is authorized to do doAs any user.");
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ProxyUserSupport.java:            LOG.debug("User [" + proxyUser + 
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ProxyUserSupport.java:            LOG.debug("User [" + proxyUser + 
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ProxyUserSupport.java:            LOG.debug("User [" + proxyUser + "] is authorized to do doAs from any host.");
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ProxyUserSupport.java:            LOG.debug("User [" + proxyUser + 
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ProxyUserSupport.java:            LOG.debug("User [" + proxyUser
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ProxyUserSupport.java:    LOG.debug(MessageFormat.format("Authorization check proxyuser [{0}] host [{1}] doAs [{2}]",
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SqoopDelegator.java:      LOG.debug("libdir=" + libdir);
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Main.java:    LOG.debug("Loaded conf " + conf);
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Main.java:      LOG.debug("XSRF filter enabled");
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java:    LOG.debug("Received callback " + theUriInfo.getRequestUri());
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java:        LOG.debug(MessageFormat.format("Resolved remote hostname: [{0}]", hostName));
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/LauncherDelegator.java:      LOG.debug("queued job " + id + " in " + elapsed + " ms");
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/LauncherDelegator.java:      LOG.debug("Launching job: " + args);
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java:      LOG.debug(Sqoop.LIB_JARS + "=" + conf.get(Sqoop.LIB_JARS));
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java:    LOG.debug(HADOOP_CLASSPATH_EXTRAS + "=" + conf.get(HADOOP_CLASSPATH_EXTRAS));
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java:      LOG.debug("Preparing to submit job: " + Arrays.toString(args));
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java:      LOG.debug("Added delegation token for jobId=" + submittedJobId.toString() +
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java:    LOG.debug("Creating hive metastore delegation token for user " + user);
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob.java:    LOG.debug("Creating hiveserver2 delegation token for user " + user);
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/HDFSStorage.java:        LOG.debug(p + " does not exist.");
./hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/CompleteDelegator.java:            LOG.debug("Cancelled token for jobId=" + id + " status from JT=" + jobStatus);
./hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/MetadataJSONSerializer.java:        LOG.debug("Could not de-serialize from: " + hcatTableStringRep);
./hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/MetadataJSONSerializer.java:        LOG.debug("Could not de-serialize partition from: " + hcatPartitionStringRep);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:    LOG.debug("File=" + INPUT_FILE_NAME);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:      LOG.debug("cpr.respCode=" + e.getResponseCode() + " cpr.errMsg=" + e.getMessage() + " for table " + tblName);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:    LOG.debug("Dumping rows via SQL from " + tblName);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:      LOG.debug(t == null ? null : t.toString() + " t.class=" + t.getClass());
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:    LOG.debug("File=" + INPUT_FILE_NAME);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:      LOG.debug("cpr.respCode=" + e.getResponseCode() + " cpr.errMsg=" + e.getMessage());
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:    LOG.debug("Dumping rows via SQL from " + tblName);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:      LOG.debug(t == null ? null : t.toString());
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:    LOG.debug("Dumping raw file: " + fileName);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatStorerTest.java:      LOG.debug(line);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/AbstractHCatLoaderTest.java:    LOG.debug("Executing: " + cmd);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderEncryption.java:    LOG.debug("Executing: " + cmd);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderStorer.java:    LOG.debug("File=" + INPUT_FILE_NAME);
./hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderStorer.java:        LOG.debug("t=" + t);
./hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatBaseStorer.java:      LOG.debug("convertPigSchemaToHCatSchema(pigSchema,tblSchema)=(" + pigSchema + "," + tableSchema + ")");
./hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatBaseStorer.java:    LOG.debug("convertPigSchemaToHCatSchema(computed)=(" + s + ")");
./hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatBaseStorer.java:        LOG.debug("hcatFieldSchema is null for fSchema '" + fSchema.alias + "'");
./hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java:      LOG.debug("outputSchema=" + outputSchema);
./hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java:      LOG.debug("setting " + configuredOptions.getOptionValue(ON_OOR_VALUE_OPT));
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:    LOG.debug("Going to execute query [{}][1={}]", sfuSql, sequence);
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:    LOG.debug("Going to execute query [{}][1={}][2={}]", NL_UPD_SQL, updatedNLId, sequence);
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:      LOG.debug("Updated {} rows for sequnce {}", rowCount, sequence);
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:      LOG.debug("Locking Derby table [{}]", lockingQuery);
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:    LOG.debug("Going to execute query [{}]", sfuSql);
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:    LOG.debug("Going to execute query [{}][1={}]", EV_UPD_SQL, updatedEventId);
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:      LOG.debug("Updated {} rows for NOTIFICATION_SEQUENCE table", rowCount);
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:    LOG.debug("DbNotificationListener: adding write notification log for : {}", event.getMessage());
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:      LOG.debug("Going to execute query <" + s.replaceAll("\\?", "{}") + ">",
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:    LOG.debug("DbNotificationListener: adding notification log for : {}", event.getMessage());
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:      LOG.debug("Going to execute insert <" + s + "> with parameters (" +
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:    LOG.debug("DbNotificationListener: Processing : {}:{}", event.getEventId(),
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:        LOG.debug("Cleaner thread running");
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:        LOG.debug("Cleaner thread done");
./hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java:          LOG.debug("Sleeping {}ms", sleepTime);
./cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:      LOG.debug("CliDriver inited with classpath {}", System.getProperty("java.class.path"));
./parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java:      LOG.debug("Parsing command: " + command);
./parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java:      LOG.debug("Parse Completed");
./parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java:    LOG.debug("Parsing hint: {}", command);
./parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java:      LOG.debug("Parse Completed");
./parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java:    LOG.debug("Parsing command: {}", command);
./parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java:      LOG.debug("Parse Completed");
./parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java:    LOG.debug("Parsing expression: {}", command);
./parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java:      LOG.debug("Parse Completed");
./streaming/src/test/org/apache/hive/streaming/TestStreamingDynamicPartitioning.java:    LOG.debug(sql);
./streaming/src/test/org/apache/hive/streaming/TestStreaming.java:    LOG.debug(sql);
./streaming/src/test/org/apache/hive/streaming/TestStreaming.java:    //LOG.debug("Running Hive Query: "+ sql);
./streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java:        LOG.debug("Created partition {} for table {}", partName,
./streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java:      LOG.debug("Write notification log is ignored as dml event logging is disabled.");
./streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java:        LOG.debug("TxnId: " + currentTxnId + ", WriteId: " + currentWriteId
./streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java:          LOG.debug("TxnId: " + currentTxnId + ", WriteId: " + currentWriteId
./streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java:      LOG.debug("Overriding HiveConf setting : " + var + " = " + value);
./streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java:      LOG.debug("Overriding HiveConf setting : " + var + " = " + value);
./streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java:      LOG.debug("Overriding HiveConf setting : " + var + " = " + true);
./streaming/src/java/org/apache/hive/streaming/AbstractRecordWriter.java:        LOG.debug("Orc memory pressure notified! usedMemory: {} maxMemory: {}.",
./streaming/src/java/org/apache/hive/streaming/AbstractRecordWriter.java:        LOG.debug("Created new filesystem instance: {}", System.identityHashCode(this.fs));
./streaming/src/java/org/apache/hive/streaming/AbstractRecordWriter.java:          LOG.debug("Flushing record updater for partitions: {}", entry.getKey());
./streaming/src/java/org/apache/hive/streaming/AbstractRecordWriter.java:        LOG.debug("Closing updater for partitions: {}", partition);
./streaming/src/java/org/apache/hive/streaming/AbstractRecordWriter.java:          LOG.debug("Low memory canary is set and ingestion size (buffered) threshold '{}' exceeded. " +
./streaming/src/java/org/apache/hive/streaming/AbstractRecordWriter.java:    LOG.debug("{} [record-updaters: {}, partitions: {}, buffered-records: {} total-records: {} " +
./jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java:      LOG.debug("Fetching delegation token from session.");
./jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java:      LOG.debug("Servers in ZooKeeper after removing rejected: {}", serverHosts);
./jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java:              LOG.debug("Configurations applied to JDBC connection params. {}", hiveServer2Instance.getProperties());
./jdbc/src/java/org/apache/hive/jdbc/Utils.java:        LOG.debug("Resolved authority: " + authorityStr);
./jdbc/src/java/org/apache/hive/jdbc/Utils.java:    LOG.debug("Resolved authority: " + authorityStr);
./jdbc/src/java/org/apache/hive/jdbc/logs/InPlaceUpdateStream.java:      LOG.debug("progress bar is complete");
./jdbc/src/java/org/apache/hive/jdbc/logs/InPlaceUpdateStream.java:      LOG.debug("operations log is shown to the user");
./jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java:        LOG.debug("HiveQueryResultsFetchReq: {}", fetchReq);
./storage-api/src/java/org/apache/hadoop/hive/common/io/DiskRangeList.java:        LOG.debug("Creating new range; last range (which can include some previous adds) was "
./storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java:          LOG.debug("Error while retrieving literalList, returning null", err);
./common/src/java/org/apache/hadoop/hive/common/auth/HiveAuthUtils.java:          LOG.debug("Disabling SSL Protocol: " + protocol);
./common/src/java/org/apache/hadoop/hive/common/FileUtils.java:      LOG.debug("Skipping child access check since the directory is already removed");
./common/src/java/org/apache/hadoop/hive/common/FileUtils.java:    LOG.debug("copying srcPaths : {}, to DestPath :{} ,with doAs: {}",
./common/src/java/org/apache/hadoop/hive/common/FileUtils.java:    LOG.debug("deleting  " + f);
./common/src/java/org/apache/hadoop/hive/common/FileUtils.java:        LOG.debug("purge is set to true. Not moving to Trash " + f);
./common/src/java/org/apache/hadoop/hive/common/type/TimestampTZUtil.java:        LOG.debug("Invalid string " + s + " for TIMESTAMP WITH TIME ZONE", e);
./common/src/java/org/apache/hadoop/hive/common/CompressionUtils.java:          LOG.debug(String.format("Attempting to write output directory %s.",
./common/src/java/org/apache/hadoop/hive/common/CompressionUtils.java:            LOG.debug(String.format("Attempting to create output directory %s.",
./common/src/java/org/apache/hadoop/hive/common/CompressionUtils.java:            LOG.debug(String.format("Creating flat output file %s.", flatOutputFile.getAbsolutePath()));
./common/src/java/org/apache/hadoop/hive/common/CompressionUtils.java:            LOG.debug(String.format("Attempting to create output directory %s.",
./common/src/java/org/apache/hadoop/hive/common/CompressionUtils.java:            LOG.debug(String.format("Creating output file %s.", outputFile.getAbsolutePath()));
./common/src/java/org/apache/hadoop/hive/conf/HiveConfUtil.java:      LOG.debug("Setting job conf credstore location to " + jobKeyStoreLocation
./common/src/java/org/apache/hadoop/hive/conf/HiveConf.java:        LOG.debug("Using the default value passed in for log id: {}", defaultValue);
./common/src/java/org/apache/hadoop/hive/conf/HiveConf.java:    LOG.debug("Found metastore URI of " + msUri);
./common/src/java/org/apache/hadoop/hive/conf/HiveConf.java:    LOG.debug("Valid non-MapReduce execution engines: {}", validNonMrEngines);
./common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java:      LOG.debug("<PERFLOG method=" + method + " from=" + callerName + ">");
./common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java:      LOG.debug(sb.toString());
./common/src/java/org/apache/hive/common/util/TimestampParser.java:        LOG.debug("Could not format millis: {}", text);
./common/src/java/org/apache/hive/common/util/TimestampParser.java:        LOG.debug("Could not parse timestamp text: {}", text);
./common/src/java/org/apache/hive/common/util/FixedSizedObjectPool.java:  // TODO: Temporary for debugging. Doesn't interfere with MTT failures (unlike LOG.debug).
./common/src/java/org/apache/hive/common/util/HiveVersionInfo.java:    LOG.debug("version: "+ version);
./common/src/java/org/apache/hive/http/security/PamLoginService.java:    LOG.debug("logout {}", user);
./common/src/java/org/apache/hive/http/JMXJsonServlet.java:    LOG.debug("Listing beans for "+qry);
./common/src/java/org/apache/hive/http/JMXJsonServlet.java:        LOG.debug("getting attribute "+attName+" of "+oname+" is unsupported");
./common/src/java/org/apache/hive/http/JMXJsonServlet.java:      LOG.debug("getting attribute "+attName+" of "+oname+" threw an exception", e);
./common/src/java/org/apache/hive/http/Log4j2ConfiguratorServlet.java:            LOG.debug("Requested logger ({}) not found. Adding as new logger with {} level", loggerName, logLevel);
./common/src/java/org/apache/hive/http/Log4j2ConfiguratorServlet.java:            LOG.debug("Updating logger ({}) to {} level", loggerName, logLevel);
./shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java:              LOG.debug("The details are: " + e, e);
./shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java:      LOG.debug("Exception while inheriting permissions", e);
./shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java:    LOG.debug(ArrayUtils.toString(command));
./shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java:    LOG.debug("Return value is :" + retval);
./shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java:        LOG.debug("The details are: " + e, e);
./shims/common/src/main/java/org/apache/hadoop/hive/shims/Utils.java:      LOG.debug("Filter {} found, using as-is.", filterClass);
./shims/common/src/main/java/org/apache/hadoop/hive/shims/Utils.java:      LOG.debug("Unable to use {}, got exception {}. Using internal shims impl of filter.",
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:    LOG.debug("Looking for databases");
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:      LOG.debug("Found " + databases.size() + " databases to process");
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:        LOG.debug("Will wait for " + compactTablesState.getMetaInfo().getCompactionIds().size() +
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:              LOG.debug("Required compaction succeeded: " + e.toString());
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:            //LOG.debug("Still waiting  on: " + e.toString());
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:            LOG.debug("Still working on: " + e.toString());
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:        LOG.debug("found {} tables in {}", tables.size(), dbName);
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:        LOG.debug("found {} {} in {}", tables.size(), runOptions.getTableType().name(), dbName);
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:      LOG.debug("processing table " + Warehouse.getQualifiedName(t));
./upgrade-acid/pre-upgrade/src/main/java/org/apache/hadoop/hive/upgrade/acid/PreUpgradeTool.java:    LOG.debug("Writing compaction commands to " + fileName);
./kudu-handler/src/java/org/apache/hadoop/hive/kudu/KuduStorageHandler.java:      LOG.debug("Setting {} to {} to enable split generation on HS2",
./kudu-handler/src/java/org/apache/hadoop/hive/kudu/KuduHiveUtils.java:          LOG.debug("Not importing credentials for service " + service +
./kudu-handler/src/java/org/apache/hadoop/hive/kudu/KuduHiveUtils.java:        LOG.debug("Importing credentials for service " + service);
./beeline/src/test/org/apache/hive/beeline/cli/TestHiveCli.java:    LOG.debug(output);
./beeline/src/java/org/apache/hive/beeline/SQLCompleter.java:      LOG.debug("fail to get SQL key words from database metadata due to the exception: " + e, e);
./beeline/src/java/org/apache/hive/beeline/SQLCompleter.java:      LOG.debug(
./beeline/src/java/org/apache/hive/beeline/SQLCompleter.java:      LOG.debug(
./beeline/src/java/org/apache/hive/beeline/SQLCompleter.java:      LOG.debug(
./beeline/src/java/org/apache/hive/beeline/SQLCompleter.java:      LOG.debug(
./beeline/src/java/org/apache/hive/beeline/schematool/HiveSchemaTool.java:      LOG.debug("Going to run command <" + builder.buildToLog() + ">");
./udf/src/java/org/apache/hadoop/hive/ql/exec/MethodUtils.java:        LOG.debug("Method " + (match ? "did" : "didn't") + " match: passed = "
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientPreCatalog.java:      LOG.debug("Unable to shutdown metastore client. Will try closing transport directly.", e);
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientPreCatalog.java:    LOG.debug("Got back " + rsp.getEventsSize() + " events");
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientPreCatalog.java:      LOG.debug("Columns is empty or partNames is empty : Short-circuiting stats eval on client side.");
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientPreCatalog.java:      LOG.debug("Columns is empty or partNames is empty : Short-circuiting stats eval on client side.");
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/utils/TestTxnDbUtil.java:            LOG.debug("Ignoring sql error {}", e.getMessage());
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/utils/TestTxnDbUtil.java:        LOG.debug("Successfully truncated table " + name);
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/utils/TestTxnDbUtil.java:          LOG.debug("Not truncating " + name + " because it doesn't exist");
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java:    LOG.debug("Testing filter: " + filter);
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestObjectStoreInitRetry.java:      LOG.debug("MISBEHAVE:" + TestObjectStoreInitRetry.getInjectConnectFailure(), re);
./standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestObjectStoreInitRetry.java:      LOG.debug("." + e.getStackTrace()[1].getLineNumber() + ":" + TestObjectStoreInitRetry.getInjectConnectFailure());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/common/ndv/fm/FMSketch.java:    LOG.debug("NumDistinctValueEstimator");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/common/ndv/fm/FMSketch.java:    LOG.debug("Number of Vectors: {}", numBitVectors);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/common/ndv/fm/FMSketch.java:    LOG.debug("Vector Size: {}", BIT_VECTOR_SIZE);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/common/ndv/fm/FMSketch.java:    LOG.debug("Serialized Vectors: ");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/common/ndv/fm/FMSketch.java:    LOG.debug(t);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/metrics/PerfLogger.java:      LOG.debug("<PERFLOG method=" + method + " from=" + callerName + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/metrics/PerfLogger.java:      LOG.debug(sb.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/datasource/HikariCPDataSourceProvider.java:    LOG.debug("Creating Hikari connection pool for the MetaStore");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/datasource/DbCPDataSourceProvider.java:    LOG.debug("Creating dbcp connection pool for the MetaStore");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java:      LOG.debug("Failed to get object from Metastore ", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java:        LOG.debug("Accessing Metastore failed due to invalid input ", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java:      LOG.debug("Object not found in metastore ", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java:          LOG.debug("Didn't find object in metastore ", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java:      LOG.debug("Could not find db entry." + nsoe);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/MetastoreSchemaTool.java:      LOG.debug("Received following output from Sqlline:");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/MetastoreSchemaTool.java:      LOG.debug(outputForLog.toString("UTF-8"));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/MetastoreSchemaTool.java:        LOG.debug("Going to invoke file that contains:");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/MetastoreSchemaTool.java:            LOG.debug("script: " + line);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskMoveDatabase.java:    LOG.debug("Going to run " + update);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskMergeCatalog.java:        LOG.debug("[external table property] Executing SQL:" + insert);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskMergeCatalog.java:        LOG.debug("[external.table.purge] Executing SQL:" + insert);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskMergeCatalog.java:        LOG.debug("[tableType=EXTERNAL_TABLE] Executing SQL:" + update);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskMergeCatalog.java:        LOG.debug("[catalog name] Executing SQL:" + merge);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskAlterCatalog.java:        LOG.debug("Going to run " + update);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskCreateUser.java:    LOG.debug("Found oracle, hacking our way through it rather than using SqlLine");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:      LOG.debug("Failed to determine schema version from Hive Metastore DB," + he.getMessage(), he);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:    LOG.debug("Validating tables in the schema for version " + version);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:        LOG.debug("schema is not supported");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:        LOG.debug("Found table " + table + " in HMS dbstore");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:      LOG.debug("Parsing schema script " + schemaFile);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:        LOG.debug("Parsing subscript " + schemaFile);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:    LOG.debug("Schema tables:[ " + Arrays.toString(schemaTables.toArray()) + " ]");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:    LOG.debug("DB tables:[ " + Arrays.toString(dbTables.toArray()) + " ]");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:          LOG.debug("Schema subscript " + subScript + " found");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskValidate.java:          LOG.debug("Found table " + table + " in the schema");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskMoveTable.java:    LOG.debug("Going to run " + update);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskMoveTable.java:    LOG.debug("Going to run " + query);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskMoveTable.java:    LOG.debug("Going to run " + update);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskCreateCatalog.java:    LOG.debug("Going to run " + query);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskCreateCatalog.java:    LOG.debug("Going to run " + query);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/tools/schematool/SchemaToolTaskCreateCatalog.java:    LOG.debug("Going to run " + update);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute query <" + sCheckAborted + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:              LOG.debug("Found potential compaction: " + info.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("No compactions found ready to compact");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:            LOG.debug("Another Worker picked up " + info);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:            LOG.debug("Found ready to clean: " + info.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:      LOG.debug("Running markCleaned with CompactionInfo: " + info.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute update <" + s + "> for CQ_ID=" + info.id);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Removed " + updCount + " records from completed_txn_components");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Going to execute update <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Removed " + rc + " records from txn_components");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute delete <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Going to execute update <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Removed " + rc + "  empty Aborted and Committed transactions from TXNS");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Set " + updated + " compaction queue entries to " + INITIATED_RESPONSE + " state for host "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:      LOG.debug("Finding columns with statistics info for CompactionInfo: " + ci.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Found columns to update stats: " + columns + " on " + ci.tableName +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("About to execute: " + sqlText);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("About to execute: " + sqlText);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Going to execute update <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Removed " + count + " records from COMPLETED_COMPACTIONS");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:      LOG.debug("Marking as failed: CompactionInfo: " + ci.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:          LOG.debug("The failure occurred before we even made an entry in COMPACTION_QUEUE. Generated ID so that we "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Inserted " + updCount + " entries into COMPLETED_COMPACTIONS");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to execute <" + s + ">  with jobId: " + hadoopJobId + " and CQ id: " + id);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/AcidHouseKeeperService.java:      LOG.debug("Total time AcidHouseKeeperService took: {} seconds.", elapsedSince(start));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/AcidHouseKeeperService.java:    LOG.debug("{} took {} seconds.", description, elapsedSince(start));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java:        LOG.debug("Adding query to batch: <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java:          LOG.debug("Going to execute queries in oracle anonymous statement. " + batch);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java:        LOG.debug("Going to execute queries in oracle anonymous statement. " + batch);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java:      LOG.debug("Adding query to batch: <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java:        LOG.debug("Going to execute queries in batch. Batch size: " + batchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java:      LOG.debug("Going to execute queries in batch. Batch size: " + queryCounter % batchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/AcidTxnCleanerService.java:      LOG.debug("Txn cleaner service took: {} seconds.", elapsedSince(start));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Catching sql exception in min history level check", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query<" + txnsQuery + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:              LOG.debug("Open transaction added for missing value in TXNS {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Got OpenTxnList with hwm: {} and openTxnList size {}.", hwm, txnInfos.size());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute insert <" + insertQuery + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute select <" + query.replaceAll("\\?", "{}") + ">", quoteString(replPolicy));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("targetTxnid for srcTxnId " + sourceTxnIdList.toString() + " is " + targetTxnIdList.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute query<" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + query.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Updating repl id for db <" + query.replaceAll("\\?", "{}") + ">", lastReplId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + query.replaceAll("\\?", "{}") + ">", quoteString(table));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Updating repl id for table <" + query.replaceAll("\\?", "{}") + ">", lastReplId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query " + query + " with partitions " +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Updating repl id for part <" + query.replaceAll("\\?", "{}") + ">", lastReplId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:                  LOG.debug("Executing a batch of <" + sql + "> queries. Batch size: " + maxBatchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:                LOG.debug("Executing a batch of <" + sql + "> queries. Batch size: " + insertCounter % maxBatchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query: <" + writeConflictQuery + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute query: <" + writeConflictQuery + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute insert <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + sql.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute insert <" + sql.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + s.replaceAll("\\?", "{}") + ">", quoteString(names[0]),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query<" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query<" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query<" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute query <" + query.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute insert <" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute update <" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:            LOG.debug("Going to execute delete <" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:              LOG.debug("Executing a batch of <" + TXN_TO_WRITE_ID_INSERT_QUERY + "> queries. " +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:            LOG.debug("Executing a batch of <" + TXN_TO_WRITE_ID_INSERT_QUERY + "> queries. " +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute insert <" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("MinOpenTxnIdWaterMark calculated with minOpenTxn {}, lowWaterMark {}", minOpenTxn, lowWaterMark);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Minimum open write id do not match for table {}", fullyQualifiedName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Acquiring lock for materialization rebuild with {} for {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + selectQ.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute update <" + insertQ.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute update <" + s.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + selectQ + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute update <" + deleteQ + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to retry enqueueLock for request: {}, after catching RetryException with message: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute updates in batch: <" + incrCmd + ">, and <" + updateLocksCmd + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:            LOG.debug("Executing a batch of <" + TXN_COMPONENTS_INSERT_QUERY + "> queries. Batch size: " + maxBatchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Executing a batch of <" + TXN_COMPONENTS_INSERT_QUERY + "> queries. Batch size: " + insertCounter % maxBatchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + SELECT_WRITE_ID_QUERY + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Executing a batch of <" + insertLocksQuery + "> queries. Batch size: " + maxBatchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Executing a batch of <" + insertLocksQuery + "> queries. Batch size: " + intLockId % maxBatchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to retry checkLock for extLockId={}/txnId={} after catching RetryException with message: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to retry checkLock for request={} after catching RetryException with message: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Successfully unlocked at least 1 lock with extLockId={}", extLockId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute update <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + query.replaceAll("\\?", "{}") + ">",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + sb.toString() + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:              LOG.debug("Executing a batch of <" + TXN_COMPONENTS_INSERT_QUERY + "> queries. Batch size: " + maxBatchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:            LOG.debug("Executing a batch of <" + TXN_COMPONENTS_INSERT_QUERY + "> queries. Batch size: " + insertCounter % maxBatchSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:              LOG.debug("Skipping cleanup because db: " + dbName + " belongs to catalog "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:              LOG.debug("Skipping cleanup because table: " + tblName + " belongs to catalog "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:              LOG.debug("Skipping cleanup because partitions belong to catalog "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute update <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute update <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to commit: " + callSig);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback: " + callSig);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Failure to acquire lock({} intLockId:{} {}), blocked by ({})", JavaUtils.lockIdToString(extLockId),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:            LOG.debug("Going to execute query: <" + cleanupQuery + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query: <" + updateBlockedByQuery + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Successfully acquired locks: " + locksBeingChecked);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query: <" + getIntIdsQuery + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute update <" + updateHeartbeatQuery + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Successfully heartbeated for extLockId={}", extLockId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute update <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Successfully heartbeated for txnId={}", txnid);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + SELECT_LOCKS_FOR_LOCK_ID_QUERY + "> for extLockId={}", extLockId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("getTxnIdFromLockId(" + extLockId + ") Return " + JavaUtils.txnIdToString(info.txnId));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query <" + SELECT_LOCKS_FOR_LOCK_ID_QUERY + "> for extLockId={}", extLockId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Found {} locks for extLockId={}. Locks: {}", locks.size(), extLockId, locks);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Going to execute query: <" + SELECT_TIMED_OUT_LOCKS_QUERY + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Did not find any timed-out locks, therefore retuning.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("Going to execute update: <" + query + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute query <" + s + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to rollback");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug("Going to execute insert <" + insert + ">");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:      LOG.debug("Removed committed transaction txnId: (" + txnid + ") from MIN_HISTORY_LEVEL");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:          LOG.debug("About to execute SQL: " + sqlStmt);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug(quoteString(key) + " locked by " + quoteString(TxnHandler.hostname));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:    LOG.debug("TXN lock locked by {} in mode {}", quoteString(TxnHandler.hostname), shared);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java:        LOG.debug(quoteString(key) + " unlocked by " + quoteString(TxnHandler.hostname));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:    LOG.debug("updating cache using notification events starting from event id " + lastEventId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("no events to process");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:    LOG.debug("num events to process" + eventList.size());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("Event to process " + event);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:              LOG.debug(ExceptionUtils.getStackTrace(e));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:              LOG.debug(ExceptionUtils.getStackTrace(e));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:            LOG.debug("Processed database: {}'s table: {}. Cached {} / {}  tables so far.", dbName, tblName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("Processed database: {}. Cached {} / {} databases so far.", dbName, ++numberOfDatabasesCachedSoFar,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached objects. Shared cache has been update {} times so far.",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:              LOG.debug(ExceptionUtils.getStackTrace(e));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached objects. Shared cache update count is: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached database objects for catalog: {}", catName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached database objects for catalog: {}", catName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached table objects for catalog: {}, database: {}", catName, dbName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:          LOG.debug("CachedStore: updated cached table objects for catalog: {}, database: {}", catName, dbName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:          LOG.debug("Unable to refresh cached tables for database: " + dbName, e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached table col stats objects for catalog: {}, database: {}", catName, dbName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached table col stats objects for catalog: {}, database: {}", catName, dbName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached foreign keys objects for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached foreign keys objects for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached not null constraints for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached not null constraints for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached unique constraints for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached unique constraints for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached primary keys objects for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached primary keys objects for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached default Constraint objects for catalog: {}, database: {}, table: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached default constraint objects for catalog: {}, database: {}, table: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached check constraint objects for catalog: {}, database: {}, table: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached check constraint objects for catalog: {}, database: {}, table: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached partition objects for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached partition objects for catalog: {}, database: {}, table: {}", catName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("CachedStore: updating cached partition col stats objects for catalog: {}, database: {}, table: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("CachedStore: updated cached partition col stats objects for catalog: {}, database: {}, table: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:          LOG.debug("CachedStore: updated cached aggregate partition col stats objects for catalog:"
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:    LOG.debug("Didn't find aggr stats in cache. Merging them. tblName= {}, parts= {}, cols= {}", tblName, partNames,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:          LOG.debug("Stats not found in CachedStore for: dbName={} tblName={} partName={} colName={}", dbName, tblName,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("No stats data found for: dbName={} tblName= {} partNames= {} colNames= ", dbName, tblName, partNames,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("Trying to match: {} against blacklist pattern: {}", str, pattern);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("Found matcher group: {} at start index: {} and end index: {}", matcher.group(), matcher.start(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("Trying to match: {} against whitelist pattern: {}", str, pattern);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:        LOG.debug("Found matcher group: {} at start index: {} and end index: {}", matcher.group(), matcher.start(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("{}.{} is in blacklist, skipping", dbName, tblName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java:      LOG.debug("{}.{} is not in whitelist, skipping", dbName, tblName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:              LOG.debug("Eviction happened for table " + notification.getKey());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:              LOG.debug("current table cache contains " + tableCache.size() + "entries");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:          LOG.debug("Partition: " + partVals + " is not present in the cache.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:          LOG.debug("Constraint: " + name + " does not exist in cache.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping primary key cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:        LOG.debug("Primary keys refresh in cache was successful for {}.{}.{}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping foreign key cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:        LOG.debug("Foreign keys refresh in cache was successful for {}.{}.{}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping not null constraints cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:        LOG.debug("Not null constraints refresh in cache was successful for {}.{}.{}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping unique constraints cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:        LOG.debug("Unique constraints refresh in cache was successful for {}.{}.{}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping default constraint cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:        LOG.debug("Default constraints refresh in cache was successful for {}.{}.{}", this.getTable().getCatName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping check constraint cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:        LOG.debug("check constraints refresh in cache was successful for {}.{}.{}", this.getTable().getCatName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping partition cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping table col stats cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:          LOG.debug("Write id list " + writeIdList + " is not compatible with write id " + writeId);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:                  LOG.debug("The current cached store transactional partition column statistics for {}.{}.{} "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Skipping partition column stats cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:                LOG.debug("Skipping partition column stats cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Unable to cache partition column stats for table: " + tableName, e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:              LOG.debug("Skipping aggregate stats cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:              LOG.debug("Skipping aggregate stats cache update for table: " + getTable().getTableName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:      LOG.debug("Skipping database cache update; the database list we have is dirty.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:            LOG.debug("Unable to cache partition column stats for table: " + tableName, e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java:      LOG.debug("Skipping table cache update; the table list we have is dirty.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:                       LOG.debug("Found {} to be different. Old val : {} : New Val : {}", key,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:                      LOG.debug("Found masked property {} to be different", key);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:                LOG.debug("Closing PersistenceManagerFactory");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:                LOG.debug("PersistenceManagerFactory closed");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:        LOG.debug("Removed cached classloaders from DataNucleus NucleusContext");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:      LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:        LOG.debug("Overriding {} value {} from jpox.properties with {}", varName, prevVal, confVal);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:          LOG.debug("Overriding " + e.getKey() + " value " + prevVal
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:          LOG.debug("{} = {}", e.getKey(), e.getValue());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:            LOG.debug("Interrupted while sleeping before retrying.", ie);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PersistenceManagerProvider.java:            LOG.debug("Non-retriable exception.", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:        LOG.debug(ADMIN +" role already exists",e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:        LOG.debug(PUBLIC + " role already exists",e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:        LOG.debug("Failed while granting global privs to admin", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:          LOG.debug(userName + " already in admin role", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:      LOG.debug("{}: {}", threadLocalId.get(), m);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:        LOG.debug((getThreadLocalIpAddress() == null ? "" : "source:" + getThreadLocalIpAddress() + " ") +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:            LOG.debug("Creating database path " + dbExtPath);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:        LOG.debug("Id shouldn't be set but table {}.{} has the Id set to {}. Id is ignored.", tbl.getDbName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:        LOG.debug("get_tables_ext:getTables() returned " + tables.size());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:          LOG.debug("get_tables_ext:getTableObjectsByName() returned " + tObjects.size());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:                LOG.debug("Table " + entry.getKey().getTableName() + " requires " + Arrays.toString((entry.getValue()).toArray()));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:          LOG.debug(part);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:          LOG.debug("Some of the partitions miss stats.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:            LOG.debug("All the column stats " + csNew.getStatsDesc().getPartName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java:          LOG.debug("All the column stats are not accurate to merge.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ThreadPool.java:      LOG.debug("ThreadPool initialized");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:        LOG.debug("Using direct SQL, underlying DB is " + dbType);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:        LOG.debug("getDatabase: directsql returning db " + db.getName()
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:      LOG.debug("Columns is empty or partNames is empty : Short-circuiting stats eval");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:    LOG.debug("useDensityFunctionForNDVEstimation = " + useDensityFunctionForNDVEstimation
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:      LOG.debug("getDefaultConstraints: directsql : " + queryText);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:      LOG.debug("getCheckConstraints: directsql : " + queryText);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:    LOG.debug("Running {}", queryText);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java:    LOG.debug("Running {}", queryText);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/AuthFactory.java:          LOG.debug("Adding server definition for PLAIN SaSL with authentication "+ authTypeStr +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfoFactory.java:      LOG.debug("HIVE_HOME is not set. Using current directory instead");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MsckPartitionExpressionProxy.java:      LOG.debug("Partition expr: {}", expr);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MsckPartitionExpressionProxy.java:        LOG.debug("Matched partition: {}", s);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Initialized ObjectStore");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("ObjectStore, initialize called");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Creating catalog {}", cat);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Fetching catalog {}", catalogName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Fetching all catalog names");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Dropping catalog {}", catalogName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Failed to get database {}.{}, returning NoSuchObjectException",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("type not found {}", typeName, e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("getTableMeta with filter " + filterBuilder + " params: " +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("Executing getMTable for {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing getPartitions");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing getPartitionLocations");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for getPartitionLocations");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing getPartitionNames");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing getPartitionNamesByFilter");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Filter specified is {}, JDOQL filter is {}", filter,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Parms is {}", params);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for getPartitionNamesByFilter");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for getPartitionNamesByFilter, size: {}", partNames.size());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("executing listPartitionNamesPsWithAuth");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPartitionNamesPs");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing listMPartitions");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listMPartitions");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listMPartitions {}", mparts);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listMPartitionsWithProjection");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving {} objects for listMPartitionsWithProjection", mparts.size());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done executing query for getPartitionsViaOrmFilter");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done retrieving all objects for getPartitionsViaOrmFilter");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Deleted {} partition from store", deleted);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug(" JDOQL filter is {}", filterStr);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:            LOG.debug("Using direct SQL optimization.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:          LOG.debug("Not using direct SQL optimization.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Full DirectSQL callstack for debugging (not an error)", ex);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("{} retrieved using {} in {}ms", result, retrieveType, time);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("JDO filter pushdown cannot be used: {}", queryBuilder.getErrorMessage());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("jdoFilter = {}", jdoFilter);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("JDO filter pushdown cannot be used: {}", queryBuilder.getErrorMessage());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("jdoFilter = {}", jdoFilter);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listTableNamesByFilter");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("filter specified is {}, JDOQL filter is {}", filter, queryFilterString);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:          LOG.debug("key: {} value: {} class: {}", entry.getKey(), entry.getValue(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listTableNamesByFilter");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listTableNamesByFilter");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("execute removeUnusedColumnDescriptor");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("successfully deleted a CD in removeUnusedColumnDescriptor");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Attempting to add a guid {} for the metastore db", guid);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("Found guid {}", uuid);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("Returning guid of metastore db : {}", uuids.get(0));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listRoles");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listRoles");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing listMSecurityPrincipalMembershipRole");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Retrieving all objects for listMSecurityPrincipalMembershipRole");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listMSecurityPrincipalMembershipRole: {}", mRoleMemebership);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listAllRoleNames");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("Found " + revokePrivilegeSet.size() + " new revoke privileges to be synced.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("No new revoke privileges are required to be synced.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("Found " + grantPrivilegeSet.size() + " new grant privileges to be synced.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("No new grant privileges are required to be synced.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listRoleMembers");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listRoleMembers");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalDBGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalDBGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing listPrincipalAllDBGrant");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("Done retrieving all objects for listPrincipalAllDBGrant: {}", mSecurityDBList);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("Done retrieving all objects for listPrincipalAllDBGrant: {}", mSecurityDBList);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listTableAllPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listTableAllPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listTableAllColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Query to obtain objects for listTableAllColumnGrants finished");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("RetrieveAll on all the objects for listTableAllColumnGrants finished");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Transaction running query to obtain objects for listTableAllColumnGrants " +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving " + mPrivs.size() + " objects for listTableAllColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listTableAllPartitionColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listTableAllPartitionColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPartitionAllColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPartitionAllColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPartitionAllColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing listDatabaseGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listDatabaseGrants: {}", mSecurityDBList);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalTableColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalTableColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalPartitionColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalPartitionColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalPartitionColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPrincipalPartitionColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalPartitionColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPartitionColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPartitionColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPartitionColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing listPrincipalAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPrincipalAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listTableGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listTableGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalAllTableGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing listPrincipalAllPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalAllPartitionGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalPartitionGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPrincipalPartitionGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalPartitionGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalPartitionGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPrincipalPartitionGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalPartitionGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing listPrincipalAllTableColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalAllTableColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalTableColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPrincipalTableColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalTableColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Executing listPrincipalTableColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing query for listPrincipalTableColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalTableColumnGrantsAll");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Executing listPrincipalAllTableColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done retrieving all objects for listPrincipalAllTableColumnGrants");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Begin Executing isPartitionMarkedForEvent");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing isPartitionMarkedForEvent");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing markPartitionForEvent");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done executing markPartitionForEvent");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing getTableColumnStatistics with status : {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing getTableColumnStatistics with status : {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("PartNames and/or ColNames are empty");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("PartNames and/or ColNames are empty");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:              LOG.debug("The current metastore transactional partition column statistics for {}.{}.{} "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:          LOG.debug("The current metastore transactional partition column "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing cleanupEvents");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing cleanupEvents");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing addToken");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done executing addToken with status : {}", committed);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing removeToken");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done executing removeToken with status : {}", committed);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing getToken");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done executing getToken with status : {}", committed);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing getAllTokenIdentifiers");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing getAllTokenIdentifers with status : {}", committed);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing addMasterKey");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done executing addMasterKey with status : {}", committed);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing updateMasterKey");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done executing updateMasterKey with status : {}", committed);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing removeMasterKey");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Done executing removeMasterKey with status : {}", success);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("Begin executing getMasterKeys");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Done executing getMasterKeys with status : {}", committed);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("Found expected HMS version of {}", dbSchemaVer);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("{}", message, new Exception("Debug Dump Stack Trace (Not an Exception)"));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("{}", message);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:            LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("getSchemaVersionsByColumns going to execute query {}", sql);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:        LOG.debug("With parameters");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:          LOG.debug(p.getKey() + " : " + p.getValue());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("runtimeStat: {}", stat);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:    LOG.debug("isCurrentStatsValidForTheQuery with stats write ID {}; query {}; writer: {} params {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("Caught jdo exception; exclusive", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("replication metrics deletion is disabled");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("scheduled executions retention is disabled");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java:      LOG.debug("scheduled executions - time_out mark is disabled");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PartitionProjectionEvaluator.java:            LOG.debug("Found " + partitionField + " included within given projection field "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetastoreVersionInfo.java:    LOG.debug("version: "+ version);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:      LOG.debug("No stats data found for: tblName= {}, partNames= {}, colNames= {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:    LOG.debug("Aggregating column stats. Threads used: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:            LOG.debug(e.getMessage());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:          LOG.debug(e.getMessage());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:    LOG.debug("Time for aggr col stats in seconds: {} Threads used: {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:      LOG.debug("New ColumnStats size is {}, but old ColumnStats size is {}",
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:    LOG.debug("tablePath:" + tablePath + ", partCols: " + partCols);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:      LOG.debug("currPath=" + currPath);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java:    LOG.debug("Converting '" + partitionValue + "' to type: '" + type + "'.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/StringColumnStatsMerger.java:    LOG.debug("Merging statistics: [aggregateColStats:{}, newColStats: {}]", aggregateColStats, newColStats);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/StringColumnStatsMerger.java:      LOG.debug("Use bitvector to merge column {}'s ndvs of {} and {} to be {}", aggregateColStats.getColName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/BooleanColumnStatsMerger.java:    LOG.debug("Merging statistics: [aggregateColStats:{}, newColStats: {}]", aggregateColStats, newColStats);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java:    LOG.debug("Merging statistics: [aggregateColStats:{}, newColStats: {}]", aggregateColStats, newColStats);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java:      LOG.debug("Use bitvector to merge column {}'s ndvs of {} and {} to be {}", aggregateColStats.getColName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/TimestampColumnStatsMerger.java:    LOG.debug("Merging statistics: [aggregateColStats:{}, newColStats: {}]", aggregateColStats, newColStats);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/TimestampColumnStatsMerger.java:      LOG.debug("Use bitvector to merge column {}'s ndvs of {} and {} to be {}", aggregateColStats.getColName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/LongColumnStatsMerger.java:    LOG.debug("Merging statistics: [aggregateColStats:{}, newColStats: {}]", aggregateColStats, newColStats);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/LongColumnStatsMerger.java:      LOG.debug("Use bitvector to merge column {}'s ndvs of {} and {} to be {}", aggregateColStats.getColName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DateColumnStatsMerger.java:    LOG.debug("Merging statistics: [aggregateColStats:{}, newColStats: {}]", aggregateColStats, newColStats);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DateColumnStatsMerger.java:      LOG.debug("Use bitvector to merge column {}'s ndvs of {} and {} to be {}", aggregateColStats.getColName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/BinaryColumnStatsMerger.java:    LOG.debug("Merging statistics: [aggregateColStats:{}, newColStats: {}]", aggregateColStats, newColStats);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DoubleColumnStatsMerger.java:    LOG.debug("Merging statistics: [aggregateColStats:{}, newColStats: {}]", aggregateColStats, newColStats);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DoubleColumnStatsMerger.java:      LOG.debug("Use bitvector to merge column " + aggregateColStats.getColName() + "'s ndvs of "
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DateColumnStatsAggregator.java:    LOG.debug("all of the bit vectors can merge for " + colName + " is " + (ndvEstimator != null));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DateColumnStatsAggregator.java:      LOG.debug("start extrapolation for " + colName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DateColumnStatsAggregator.java:    LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/LongColumnStatsAggregator.java:    LOG.debug("all of the bit vectors can merge for " + colName + " is " + (ndvEstimator != null));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/LongColumnStatsAggregator.java:      LOG.debug("start extrapolation for " + colName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/LongColumnStatsAggregator.java:    LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DecimalColumnStatsAggregator.java:    LOG.debug("all of the bit vectors can merge for " + colName + " is " + (ndvEstimator != null));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DecimalColumnStatsAggregator.java:      LOG.debug("start extrapolation for " + colName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DecimalColumnStatsAggregator.java:    LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/TimestampColumnStatsAggregator.java:    LOG.debug("all of the bit vectors can merge for " + colName + " is " + (ndvEstimator != null));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/TimestampColumnStatsAggregator.java:      LOG.debug("start extrapolation for " + colName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/TimestampColumnStatsAggregator.java:    LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java:    LOG.debug("all of the bit vectors can merge for " + colName + " is " + (ndvEstimator != null));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java:      LOG.debug("start extrapolation for " + colName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java:    LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java:    LOG.debug("all of the bit vectors can merge for " + colName + " is " + (ndvEstimator != null));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java:      LOG.debug("start extrapolation for " + colName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java:    LOG.debug(
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/AggregateStatsCache.java:      LOG.debug("No aggregate stats cached for " + key.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/AggregateStatsCache.java:      LOG.debug("Interrupted Exception ignored ",e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/AggregateStatsCache.java:      LOG.debug("Interrupted Exception ignored ", e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/AggregateStatsCache.java:              LOG.debug("Interrupted Exception ignored ",e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/AggregateStatsCache.java:      LOG.debug("Interrupted Exception ignored ",e);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDirectSqlUtils.java:    LOG.debug("Direct SQL query in " + (queryTime - start) / 1000000.0 + "ms + " +
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDirectSqlUtils.java:    LOG.debug("Value is of type {}", value.getClass());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDirectSqlUtils.java:      LOG.debug("Expected blob type but got " + value.getClass().getName());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java:      LOG.debug("Seeding table {} with writeId {}", tableName, maxWriteIdOnFilesystem);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java:      LOG.debug("Seeding global txns sequence with  {}", maxVisibilityTxnId + 1);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java:              LOG.debug(msg);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java:            LOG.debug("Generated partExpr: {} for partName: {}", partExpr, partName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsRebuildLockCleanerTask.java:      LOG.debug("Cleaning up materialization rebuild locks");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsRebuildLockCleanerTask.java:          LOG.debug("Number of materialization locks deleted: " + removedCnt);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java:            LOG.debug("{}.{}.{}.{} expired. createdAt: {} current: {} age: {}s expiry: {}s", partition.getCatName(),
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java:      LOG.debug("PartitionName: " + partitionName);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java:    LOG.debug("Number of partitions not in metastore : " + result.getPartitionsNotInMs().size());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java:    LOG.debug("Max writeId {}, max txnId {} found in partition {}", maxWriteId, maxVisibilityId,
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java:      LOG.debug("Using single-threaded version of MSCK-GetPaths");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java:      LOG.debug("Using multi-threaded version of MSCK-GetPaths with number of threads " + poolSize);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PartFilterExprUtil.java:    LOG.debug("Filter specified is " + filter);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:        LOG.debug("Table has no specific required capabilities");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("External bucketed table with HB2 capability:RW");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("External bucketed table without HB2 capability:RO");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                  LOG.debug("Bucketed table without HIVEBUCKET2 capability, removed bucketing info from table");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("External unbucketed table with EXTREAD/WRITE capability:RW");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("External unbucketed table with EXTREAD capability:RO");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("External unbucketed table without EXTREAD/WRITE capability:NONE");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:              LOG.debug("Managed non-acid table:RW");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                  LOG.debug("Managed acid table with INSERTWRITE or CONNECTORWRITE capability:RW");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                  LOG.debug("Managed acid table with INSERTREAD or CONNECTORREAD capability:RO");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                  LOG.debug("Full acid table with ACIDWRITE or CONNECTORWRITE capability:RW");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                  LOG.debug("Full acid table with ACIDREAD or CONNECTORREAD capability:RO");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                  LOG.debug("Full acid table without ACIDREAD/WRITE or CONNECTORREAD/WRITE capability:NONE");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:              LOG.debug("Bucketed table without HIVEBUCKET2 capability, removed bucketing info from table");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:            LOG.debug("Adding HIVEBUCKET2 to requiredWrites");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:            LOG.debug("No matches, accessType=" + ACCESSTYPE_NONE);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("Managed acid table with CONNECTORWRITE capability:RW");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("Managed acid table with MANAGEDREAD capability:RO");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("Managed acid table with CONNECTORREAD capability:RO");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("Managed acid table without any READ capability:NONE");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("Full acid table with CONNECTORWRITE capability:RW");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("Full acid table with CONNECTORREAD/ACIDREAD capability:RO");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:                LOG.debug("Full acid table without READ capability:RO");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("Table belongs to non-default catalog, skipping translation");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:        LOG.debug("Number of original part buckets=" + partBuckets);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:        LOG.debug("Table " + table.getTableName() + " has no specific required capabilities");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("Table belongs to non-default catalog, skipping");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("Table is a MANAGED_TABLE");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:            LOG.debug("Processor has required capabilities to be able to create INSERT-only tables");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:            LOG.debug("Processor has required capabilities to be able to create FULLACID tables.");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("Table to be created is of type " + tableType);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("Table belongs to non-default catalog, skipping translation");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:    LOG.debug("Transformer returning table:" + table.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("Database belongs to non-default catalog, skipping translation");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:          LOG.debug("Processor has atleast one of ACID write capabilities, setting current locationUri " + db.getLocationUri() + " as managedLocationUri");
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:          LOG.debug("Processor has atleast one of ACID write capabilities, setting default managed path to " + mgdWhLocation.toString());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:    LOG.debug("diffList=" + Arrays.toString(diffList.toArray()) + ",master list=" + Arrays.toString(list1.toArray()));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("diffList=" + Arrays.toString(diffList.toArray()));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:        LOG.debug("list1.size():" + list1.size());
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("diff returning " + Arrays.toString(diffList.toArray()) + ",full list=" + Arrays.toString(list1.toArray()));
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java:      LOG.debug("ValidateTablePaths: whRoot={} dbLocation={} tableLocation={} ", whRootPath.toString(), dbLocation.toString(), tableLocation);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/messaging/json/gzip/DeSerializer.java:      LOG.debug("base64 encoded String", messageBody);
./standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/messaging/json/gzip/Serializer.java:      LOG.debug("message " + messageAsString);
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:        LOG.debug("Creating client");
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:        LOG.debug("Closed a connection to metastore.");
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:        LOG.debug("aborted all opened txns");
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:      LOG.debug("opened txns, count=", howMany);
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:        LOG.debug("opened txn, id={}", txnId);
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:        LOG.debug("aborted all opened txns");
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:      LOG.debug("sending lock request");
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:      LOG.debug("dropping {} tables", howMany);
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java:      LOG.debug("executing getValidWriteIds");
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkTool.java:      LOG.debug("creating directory {}", location);
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSBenchmarks.java:      LOG.debug("Created {} partitions", howMany);
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSBenchmarks.java:      LOG.debug("started benchmark... ");
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSBenchmarks.java:      LOG.debug("Created {} partitions", howMany);
./standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSBenchmarks.java:      LOG.debug("started benchmark... ");
./standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java:      LOG.debug("Adding configuration resource {}", r);
./standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java:      LOG.debug("Configuration {} does not exist", r);
./standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java:    LOG.debug("Opening kerberos connection to HMS");
./standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java:    LOG.debug("Connecting to {}, framedTransport = {}", uri, useFramedTransport);
./standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java:    LOG.debug("Connected to metastore, using compact protocol = {}", useCompactProtocol);
./standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java:      LOG.debug("Closing thrift transport");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:      LOG.debug("Not setting UGI conf as passed-in authMethod of {} = current",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:      LOG.debug("Setting UGI conf as passed-in authMethod of {} != current",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:      LOG.debug("Not setting UGI conf as passed-in authMethod of {} = current",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:      LOG.debug("Setting UGI conf as passed-in authMethod of {} != current",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:    LOG.debug("Current authMethod = {}", ugi.getAuthenticationMethod());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:          LOG.debug("SASL client callback: setting username: {}", userName);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:          LOG.debug("SASL client callback: setting userPassword");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:          LOG.debug("SASL client callback: setting realm: {}",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:          LOG.debug("SASL server DIGEST-MD5 callback: setting password "
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:              LOG.debug("SASL server DIGEST-MD5 callback: setting "
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:       LOG.debug("Sasl Server AUTH ID: {}", authId);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:           LOG.debug("Set remoteUser: {}", remoteUser.get());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge.java:           LOG.debug("Set remoteUser: {}, from endUser: {}", remoteUser.get(),
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/LdapSearchFactory.java:      LOG.debug("Could not connect to the LDAP Server:Authentication failed for {}", principal);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/LdapSearchFactory.java:    LOG.debug("Connecting using principal {} to ldap url {}", principal, ldapUrl);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/LdapSearch.java:      LOG.debug("Matched users: {}", allLdapNames);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/LdapSearch.java:    LOG.debug("Executing a query: '{}' with base DNs {}.", query.getFilter(), baseDns);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/LdapSearch.java:        LOG.debug("Exception happened for query '" + query.getFilter() +
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/GroupFilterFactory.java:        LOG.debug("User {} member of : {}", userDn, memberOf);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/GroupFilterFactory.java:          LOG.debug("GroupMembershipKeyFilter passes: user '{}' is a member of '{}' group",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/GroupFilterFactory.java:          LOG.debug("Cannot find DN for group " + groupId, e);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/GroupFilterFactory.java:        LOG.debug(msg);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/GroupFilterFactory.java:            LOG.debug("UserMembershipKeyFilter passes: user '{}' is a member of '{}' group",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ldap/GroupFilterFactory.java:            LOG.debug(msg, e);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:      LOG.debug("Cache entry weight - key: {}, value: {}, total: {}", keySize, valSize, keySize + valSize);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:            LOG.debug("Caffeine - ({}, {}) was removed ({})", key, val, cause);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(cacheObjName + ": " + mscLocalCache.stats().toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(cacheObjName + ": " + mscLocalCache.stats().toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(cacheObjName + ": " + mscLocalCache.stats().toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(cacheObjName + ": " + mscLocalCache.stats().toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(cacheObjName + ": " + mscLocalCache.stats().toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(cacheObjName + ": " + mscLocalCache.stats().toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(cacheObjName + ": " + mscLocalCache.stats().toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:          LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientWithLocalCache.java:        LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/FileUtils.java:    LOG.debug("deleting  " + f);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/FileUtils.java:        LOG.debug("purge is set to true. Not moving to Trash " + f);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/SecurityUtils.java:          LOG.debug("Disabling SSL Protocol: " + protocol);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java:        LOG.debug("User \"" + user + "\" belongs to super-group \"" + superGroupName + "\". " +
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java:      LOG.debug("Failed to get EZ for non-existent path: "+ fullPath, fnfe);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java:          LOG.debug("The details are: " + e, e);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java:              LOG.debug("The details are: " + e, e);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java:      LOG.debug("Exception while inheriting permissions", e);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java:    LOG.debug(ArrayUtils.toString(command));
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java:    LOG.debug("Return value is :" + retval);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java:      LOG.debug("Reconnection status for Method: " + method.getName() + " is " + shouldReconnect);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:              LOG.debug(
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:            LOG.debug("HMSC::open(): Creating plain authentication thrift connection.");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:                LOG.debug("HMSC::open(): Found delegation token. Creating DIGEST-based thrift connection.");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:                LOG.debug("HMSC::open(): Could not find delegation token. Creating KERBEROS-based thrift connection.");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:            LOG.debug("Failed to connect to the MetaStore Server URI ({})",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:      LOG.debug("Unable to shutdown metastore client. Will try closing transport directly.", e);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("Columns is empty or partNames is empty : Short-circuiting stats eval on client side.");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getAggrColStatsFor",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:      LOG.debug("Selecting dropDatabase method for " + dbName + " (" + tableCount + " tables), " +
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("Dropping database in a per table batch manner.");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("Dropping database in a per DB manner.");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "listPartitionsWithAuthInfo",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "listPartitionsWithAuthInfo",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "listPartitionsByExpr",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "listPartitionsSpecByExpr",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getDatabase",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getTable",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getTable",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getPrimaryKeys",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getForeignKeys",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getUniqueConstraints",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getNotNullConstraints",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getAllTableConstraints",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getTableColumnStatistics",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getTableColumnStatistics",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getConfigValue",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:    LOG.debug("Got back {} events", rsp!= null ? rsp.getEventsSize() : 0);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("Got event with id : {}", e.getEventId());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("Columns is empty or partNames is empty : Short-circuiting stats eval on client side.");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:        LOG.debug("class={}, method={}, duration={}, comments={}", CLASS_NAME, "getAggrColStatsFor",
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java:        LOG.debug("A file with the same content of {} already exists, ignore", path.toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java:    LOG.debug("Encoded URI: " + encodedUri);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java:    LOG.debug("Reading Encoded URI: " + result[0] + ":: " + result[1] + ":: " + result[2] + ":: " + result[3]);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java:                    LOG.debug("Move " + file.toString() + " to trash");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java:                    LOG.debug("Remove " + file.toString());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java:        LOG.debug("repl policy for database {} is {}", db.getName(), replPolicyId);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java:      LOG.debug("Repl policy is not set for database: {}", db.getName());
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreConfigAuthenticationProviderImpl.java:      LOG.debug("Invalid user " + authUser);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreConfigAuthenticationProviderImpl.java:      LOG.debug("Invalid password for user " + authUser);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreConfigAuthenticationProviderImpl.java:    LOG.debug("User " + authUser + " successfully authenticated.");
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java:        LOG.debug("Setting conf value " + var.varname + " using value " +
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java:          LOG.debug("Picking up system property " + s + " with value " + v);
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java:      LOG.debug(dumpConfig(conf));
./standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/Warehouse.java:        LOG.debug("Exception when checking if path (" + path + ")", e);
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:          LOG.debug("Registering watches for AppDirs: appId={}, dagId={}", applicationIdString, dagIdentifier);
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:                LOG.debug("PathCacheEviction: " + notification.getKey() + ", Reason=" +
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:            LOG.debug("Loaded : " + key + " via loader");
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:      LOG.debug("Registering " + identifier + " via watcher");
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:          LOG.debug("KeepAliveParam : " + keepAliveList
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:        LOG.debug("RECV: " + request.getUri() +
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:          LOG.debug("Retrieved pathInfo for " + identifier + " check for corresponding "
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:        LOG.debug("jobId=" + jobId + ", mapId=" + mapId + ",dataFile=" + pathInfo.dataPath +
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:        LOG.debug("Content Length in shuffle : " + contentLength);
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:        LOG.debug("verifying request. enc_str=" + enc_str + "; hash=..." +
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:        LOG.debug("Fetcher request verified. enc_str=" + enc_str + ";reply=" +
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:          LOG.debug("Ignoring closed channel error", cause);
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java:          LOG.debug("Ignoring client socket close", cause);
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java:          LOG.debug("shuffleBufferSize: {}, path: {}", shuffleBufferSize, identifier);
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/IndexCache.java:      LOG.debug("IndexCache HIT: MapId " + mapId + " found");
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/IndexCache.java:      LOG.debug("IndexCache HIT: MapId " + mapId + " found");
./llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/IndexCache.java:    LOG.debug("IndexCache MISS: MapId " + mapId + " not found") ;
./llap-server/src/java/org/apache/hadoop/hive/llap/cache/LlapAllocatorBuffer.java:        LlapIoImpl.LOG.debug("Buffer incRef is deffering an interrupt");
./llap-server/src/java/org/apache/hadoop/hive/llap/security/LlapServerSecurityInfo.java:      LOG.debug("Trying to get KerberosInfo for " + protocol);
./llap-server/src/java/org/apache/hadoop/hive/llap/security/LlapServerSecurityInfo.java:      LOG.debug("Trying to get TokenInfo for " + protocol);
./llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java:        LlapIoImpl.LOG.debug("Created a reader at " + i + ": " + columnReaders[i]
./llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java:      LlapIoImpl.LOG.debug("setDone called; closed {}, interrupted {}, err {}, pending {}",
./llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java:    LlapIoImpl.LOG.debug("setError called; closed {}, interrupted {},  err {}, pending {}",
./llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java:      LOG.debug("Logical table includes: {}", readerLogicalColumnIds);
./llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java:    LOG.debug("Starting proactive eviction.");
./llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java:      LOG.debug(sb.toString());
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VectorDeserializeOrcWriter.java:        LlapIoImpl.LOG.debug("Includes for deserializer are " + DebugUtils.toString(includes));
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VectorDeserializeOrcWriter.java:      // LlapIoImpl.LOG.debug("Adding batch " + batch);
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VectorDeserializeOrcWriter.java:      // LlapIoImpl.LOG.debug("Writing batch " + batch);
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java:      LOG.debug("From {}, the file includes are {}", includes, DebugUtils.toString(fileIncludes));
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java:    LOG.debug("Encoded reader is being stopped");
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java:    LlapIoImpl.LOG.debug("Encoded reader is being stopped");
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java:      LlapIoImpl.LOG.debug("Discarding disk data (if any wasn't cached)");
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java:            LlapIoImpl.LOG.debug("Buffers are null for " + receiver.name);
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java:    // LlapIoImpl.LOG.debug("diskData " + diskData);
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java:        LlapIoImpl.LOG.debug("Creating slice to cache in addition to an existing slice "
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java:      LlapIoImpl.LOG.debug("Processing slice #" + stripeIx + " " + sliceStr + "; has"
./llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java:      LlapIoImpl.LOG.debug("Using {} to read data with skip.header.line.count {} and skip.footer.line.count {}",
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/status/LlapStatusServiceDriver.java:        LOG.debug("Using appName: {}", appName);
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/status/LlapStatusServiceDriver.java:      LOG.debug("Final AppState: " + appStatusBuilder.toString());
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/status/LlapStatusServiceDriver.java:      LOG.debug("No information found in the LLAP registry");
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/status/LlapStatusServiceDriver.java:        LOG.debug("Potential instances starting up: {}", appStatusBuilder.allRunningInstances());
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/status/LlapStatusServiceDriver.java:        LOG.debug("Instances likely to shutdown soon: {}", llapExtraInstances);
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/status/LlapStatusServiceDriver.java:              LOG.debug(
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/status/LlapStatusServiceDriver.java:                LOG.debug(
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/status/LlapStatusServiceDriver.java:    LOG.debug("Completed processing - exiting with " + ret);
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/AsyncTaskCopyLocalJars.java:      LOG.debug("Copying " + jarPath + " to " + libDir);
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/AsyncTaskDownloadTezJars.java:        LOG.debug("Copying tez libs from " + tezLibs);
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/LlapServiceDriver.java:      LOG.debug("Config Json generation took " + (System.nanoTime() - t0) + " ns");
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/LlapServiceDriver.java:      LOG.debug("Exiting successfully");
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/LlapServiceDriver.java:    LOG.debug("Calling package.py via: " + scriptArgs);
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/LlapServiceDriver.java:      LOG.debug("Started the cluster via service API");
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/LlapServiceDriver.java:    LOG.debug("Completed processing - exiting with " + ret);
./llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/LlapTarComponentGatherer.java:      LOG.debug(task.getKey() + " waited for " + (t2 - t1) + " ns");
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java:            LOG.debug("Removing " + notification.getValue()  + " from pool.Pool size: " + ugiPool.size());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java:              LOG.debug("Added new ugi pool for " + appTokenIdentifier + ", Pool Size: ");
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryFragmentInfo.java:              LOG.debug("Cannot finish due to source: " + inputSpec.getConnectedVertexName());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java:        LOG.debug("Query complete received for {}", queryIdentifier);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java:          LOG.debug("Removed following AMs due to query complete:");
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java:            LOG.debug(amNodeInfo.toString());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java:                LOG.debug(
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java:        LOG.debug("canFinish: " + taskSpec.getTaskAttemptID() + ": " + canFinish());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java:        LOG.debug("taskOwner hashCode:" + taskOwner.hashCode());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java:            LOG.debug(
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java:            LOG.debug("Reporting taskKilled for non-started fragment {}", getRequestId());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java:          LOG.debug("Successfully finished {}", requestId);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/FunctionLocalizer.java:        LOG.debug("Localizer thread interrupted");
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/FunctionLocalizer.java:        LOG.debug("Localizer thread interrupted");
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTokenChecker.java:        LOG.debug("Token {}", id);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java:        LOG.debug("Registering request for {} with the ShuffleHandler", queryIdentifier);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java:          LOG.debug("Queueing future cleanup for external queryId: {}", queryInfo.getHiveQueryIdString());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java:        LOG.debug("Deleting path: " + pathToDelete);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java:        LOG.debug("Sending heartbeat to AM, request=" + request);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java:        LOG.debug("Received heartbeat response from AM, response=" + response);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java:          LOG.debug("Counters: " + task.getCounters().toShortString());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java:          LOG.debug("Invoking OOB heartbeat for successful attempt: {}, isTaskDone={}", taskAttemptID, task.isTaskDone());
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java:          LOG.debug(
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/StatsRecordingThreadPool.java:            LOG.debug("Received dagId: {} queryId: {} instanceType: {}",
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/StatsRecordingThreadPool.java:              LOG.debug("Updated stats: instance: {} thread name: {} thread id: {} scheme: {} " +
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapLoadGeneratorService.java:    LOG.debug("Local hostname is: {}", localHostName);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapLoadGeneratorService.java:        LOG.debug("Starting load generator process on: {}", localHostName);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java:      LOG.debug("LLAP daemon logging initialized from {} in {} ms. Async: {}",
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:                LOG.debug("Attempting to schedule task {}, canFinish={}. Current state: "
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:                LOG.debug("Grace period ended for the previous kill; preemtping more tasks");
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:        LOG.debug(
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:          LOG.debug("{} added to wait queue. Current wait queue size={}", task.getRequestId(),
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:          LOG.debug("Eviction: {} {} {}", taskWrapper, result, evictedTask);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:          LOG.debug("{} is {} as wait queue is full", taskWrapper.getRequestId(), result);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:          LOG.debug("Finishable state of {} updated to {} during registration for state updates",
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:      LOG.debug("Wait Queue: {}", waitQueue);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:        LOG.debug(
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:        LOG.debug("Fragment not found {}", fragmentId);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:      LOG.debug("Fragment {} guaranteed state changed to {}; finishable {}, in wait queue {}, "
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:            LOG.debug("Removing {} from waitQueue", fragmentId);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:            LOG.debug("Removing {} from preemptionQueue", fragmentId);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:      LOG.debug("{} scheduled for execution. canFinish={}, isGuaranteed={}",
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:    LOG.debug("Preemption Queue: {}", preemptionQueue);
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:      LOG.debug("Fragment {} guaranteed state changed to {}; finishable {}, in wait queue {}, "
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:        LOG.debug("Received successful completion for: {}",
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:        LOG.debug("Received failed completion for: {}",
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:        LOG.debug("Task {} complete. WaitQueueSize={}, numSlotsAvailable={}, preemptionQueueSize={}",
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:          LOG.debug("awaitTermination: " + awaitTermination + " shutting down task executor" +
./llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java:          LOG.debug("awaitTermination: " + awaitTermination + " shutting down task executor" +
./service/src/java/org/apache/hive/service/servlet/QueryProfileServlet.java:      LOG.debug("No display object found for operation {} ", opId);
./service/src/java/org/apache/hive/service/auth/ldap/LdapSearchFactory.java:      LOG.debug("Could not connect to the LDAP Server:Authentication failed for {}", principal);
./service/src/java/org/apache/hive/service/auth/ldap/LdapSearchFactory.java:    LOG.debug("Connecting using principal {} to ldap url {}", principal, ldapUrl);
./service/src/java/org/apache/hive/service/auth/ldap/LdapSearch.java:      LOG.debug("Matched users: {}", allLdapNames);
./service/src/java/org/apache/hive/service/auth/ldap/LdapSearch.java:    LOG.debug("Executing a query: '{}' with base DNs {}.", query.getFilter(), baseDns);
./service/src/java/org/apache/hive/service/auth/ldap/LdapSearch.java:        LOG.debug("Exception happened for query '" + query.getFilter() +
./service/src/java/org/apache/hive/service/auth/ldap/GroupFilterFactory.java:        LOG.debug("User {} member of : {}", userDn, memberOf);
./service/src/java/org/apache/hive/service/auth/ldap/GroupFilterFactory.java:          LOG.debug("GroupMembershipKeyFilter passes: user '{}' is a member of '{}' group",
./service/src/java/org/apache/hive/service/auth/ldap/GroupFilterFactory.java:          LOG.debug("Cannot find DN for group " + groupId, e);
./service/src/java/org/apache/hive/service/auth/ldap/GroupFilterFactory.java:        LOG.debug(msg);
./service/src/java/org/apache/hive/service/auth/ldap/GroupFilterFactory.java:            LOG.debug("UserMembershipKeyFilter passes: user '{}' is a member of '{}' group",
./service/src/java/org/apache/hive/service/auth/ldap/GroupFilterFactory.java:            LOG.debug(msg, e);
./service/src/java/org/apache/hive/service/server/KillQueryZookeeperManager.java:        LOG.debug("Kill query request with id {}", killQuery.getQueryId());
./service/src/java/org/apache/hive/service/server/KillQueryZookeeperManager.java:          LOG.debug("Confirm unknown kill query request with id {}", killQuery.getQueryId());
./service/src/java/org/apache/hive/service/server/HiveServer2.java:      LOG.debug(initLog4jMessage);
./service/src/java/org/apache/hive/service/server/HiveServer2.java:      LOG.debug(oproc.getDebugMessage().toString());
./service/src/java/org/apache/hive/service/server/KillQueryImpl.java:      LOG.debug("Query found with id: {}", queryIdOrTag);
./service/src/java/org/apache/hive/service/server/KillQueryImpl.java:        LOG.debug("Query found with tag: {}", queryIdOrTag);
./service/src/java/org/apache/hive/service/server/KillQueryImpl.java:      LOG.debug("Query not found with tag/id: {}", queryIdOrTag);
./service/src/java/org/apache/hive/service/server/KillQueryImpl.java:          LOG.debug("Killing query with zookeeper coordination: " + queryIdOrTag);
./service/src/java/org/apache/hive/service/server/ThreadWithGarbageCleanup.java:      LOG.debug("RawStore: " + threadLocalRawStore + ", for the thread: " +
./service/src/java/org/apache/hive/service/server/ThreadWithGarbageCleanup.java:      LOG.debug("Thread Local RawStore is null, for the thread: " +
./service/src/java/org/apache/hive/service/server/ThreadWithGarbageCleanup.java:      LOG.debug("Adding RawStore: " + threadLocalRawStore + ", for the thread: " +
./service/src/java/org/apache/hive/service/cli/operation/GetFunctionsOperation.java:            LOG.debug(debugMessage, rowData);
./service/src/java/org/apache/hive/service/cli/operation/GetFunctionsOperation.java:        LOG.debug("No function metadata has been returned");
./service/src/java/org/apache/hive/service/cli/operation/OperationManager.java:        LOG.debug("Unexpected display object value of null for operation {}",
./service/src/java/org/apache/hive/service/cli/operation/OperationManager.java:      LOG.debug(opHandle + ": Operation is already aborted in state - " + opState);
./service/src/java/org/apache/hive/service/cli/operation/OperationManager.java:      LOG.debug(opHandle + ": Attempting to cancel from state - " + opState);
./service/src/java/org/apache/hive/service/cli/operation/GetSchemasOperation.java:          LOG.debug(debugMessage, dbName, DEFAULT_HIVE_CATALOG);
./service/src/java/org/apache/hive/service/cli/operation/GetSchemasOperation.java:        LOG.debug("No schema metadata has been returned.");
./service/src/java/org/apache/hive/service/cli/operation/GetPrimaryKeysOperation.java:          LOG.debug(debugMessage, rowData);
./service/src/java/org/apache/hive/service/cli/operation/GetPrimaryKeysOperation.java:        LOG.debug("No primary key metadata has been returned.");
./service/src/java/org/apache/hive/service/cli/operation/GetTypeInfoOperation.java:          LOG.debug(debugMessage, rowData);
./service/src/java/org/apache/hive/service/cli/operation/GetTypeInfoOperation.java:        LOG.debug("No type info metadata has been returned.");
./service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java:              LOG.debug(debugMessage, rowData);
./service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java:        LOG.debug("No column metadata has been returned.");
./service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java:            LOG.debug(debugMessage, DEFAULT_HIVE_CATALOG, tableMeta.getDbName(),
./service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java:          LOG.debug("No table metadata has been returned.");
./service/src/java/org/apache/hive/service/cli/operation/GetTableTypesOperation.java:          LOG.debug(debugMessage, tableType);
./service/src/java/org/apache/hive/service/cli/operation/GetTableTypesOperation.java:        LOG.debug("No table type metadata has been returned.");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": openSession()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": openSessionWithImpersonation()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": openSession()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": openSession()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": openSession()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": createSessionWithSessionHandle()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": openSession()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": closeSession()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getInfo()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": executeStatement()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": executeStatement()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": executeStatementAsync()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": executeStatementAsync()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getTypeInfo()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getCatalogs()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getSchemas()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getTables()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getTableTypes()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getColumns()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getFunctions()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getPrimaryKeys()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(sessionHandle + ": getCrossReference()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(opHandle + ": getOperationStatus()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:          LOG.debug("timed out and hence returning progress log as NULL");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(opHandle + ": cancelOperation()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(opHandle + ": closeOperation");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(opHandle + ": getResultSetMetadata()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(opHandle + ": fetchResults()");
./service/src/java/org/apache/hive/service/cli/CLIService.java:    LOG.debug(opHandle + ": getQueryId() " + queryId);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:      LOG.debug("Using the random number as the secret for cookie generation " + secret);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:      LOG.debug("Client IP Address: " + clientIpAddress);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:        LOG.debug("{}:{}", X_FORWARDED_FOR, forwarded_for);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:      LOG.debug("Client username: " + clientUserName);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:          LOG.debug("Invalid cookie", e);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:          LOG.debug("Validated the cookie for user " + userName);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:        LOG.debug("No valid cookies associated with the request " + request);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:      LOG.debug("Received cookies: " + toCookieStr(cookies));
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:      LOG.debug("Cookie name = " + AUTH_COOKIE + " value = " + str);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:    LOG.debug("HTTP Auth Header [{}]", authHeader);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java:      LOG.debug("URL query string:" + queryString);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java:    LOG.debug("Client's IP Address: " + clientIpAddress);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java:    LOG.debug("Client's username: " + effectiveClientUser);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java:      LOG.debug("Proxy user from query string: " + proxyUser);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java:      LOG.debug("Proxy user from thrift body: " + proxyUserFromThriftBody);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java:    LOG.debug("Verified proxy user: " + proxyUser);
./service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpCLIService.java:        LOG.debug("XSRF filter enabled");
./service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java:          LOG.debug("Global init file " + hivercFile + " does not exist");
./service/src/java/org/apache/hive/service/cli/session/SessionManager.java:    LOG.debug("setting proxy user name based on query param to: " + userName);
./service/src/java/org/apache/hive/service/CookieSigner.java:      LOG.debug("Signature generated for " + str + " is " + signature);
./service/src/java/org/apache/hive/service/CookieSigner.java:      LOG.debug("Signature generated for " + rawValue + " inside verify is " + currentSignature);
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:    LOG.debug("Setting up output service");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:    LOG.debug("Output service up");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:    LOG.debug("Tearing down service");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:    LOG.debug("Tearing down complete");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:      LOG.debug("Socket connected");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:      LOG.debug("Data written");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:      LOG.debug("Have record writer");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:      LOG.debug("Have record reader");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:        LOG.debug(text.toString());
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:    LOG.debug("Socket connected");
./ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java:    LOG.debug("Data written");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/CompactorTestUtilities.java:      LOG.debug(formatFile + " not found, returning default: " + ORC_ACID_VERSION_DEFAULT);
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/CompactorTest.java:        LOG.debug("Reading records from " + p.toString());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting minorTableWithBase");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the delta file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting minorWithOpenInMiddle");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting minorWithAborted");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the delta file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting minorTableWithBase");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the delta file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting majorTableWithBase");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting majorPartitionWithBase");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting majorTableNoBase");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting majorTableLegacy");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting minorTableLegacy");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting majorPartitionWithBaseMissingBuckets");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:        LOG.debug("This is not the file you are looking for " + stat[i].getPath().getName());
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting majorWithOpenInMiddle");
./ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java:    LOG.debug("Starting majorWithAborted");
./ql/src/test/org/apache/hadoop/hive/ql/io/TestRCFile.java:    LOG.debug("reading " + count + " records");
./ql/src/test/org/apache/hadoop/hive/ql/io/TestRCFile.java:    LOG.debug("reading fully costs:" + cost + " milliseconds");
./ql/src/test/org/apache/hadoop/hive/ql/io/TestRCFile.java:    LOG.debug("reading " + count + " records");
./ql/src/test/org/apache/hadoop/hive/ql/io/TestRCFile.java:    LOG.debug("reading fully costs:" + cost + " milliseconds");
./ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestWorkloadManager.java:    LOG.debug("Joining " + t.getName());
./ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestWorkloadManager.java:    LOG.debug("Joined " + t.getName());
./ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java:    LOG.debug("Starting testUnlockWithTxn");
./ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java:    LOG.debug("Starting deadlock test");
./ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java:                  LOG.debug("no exception, no deadlock");
./ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java:                    LOG.debug("Got an exception, but not a deadlock, SQLState is " +
./ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java:                    LOG.debug("Forced a deadlock, SQLState is " + e.getSQLState() + " class of " +
./ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java:                  LOG.debug("no exception, no deadlock");
./ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java:                    LOG.debug("Got an exception, but not a deadlock, SQLState is " +
./ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java:                    LOG.debug("Forced a deadlock, SQLState is " + e.getSQLState() + " class of " +
./ql/src/test/org/apache/hive/testutils/MiniZooKeeperCluster.java:          LOG.debug("Failed binding ZK Server to client port: " + currentClientPort, e);
./ql/src/java/org/apache/hadoop/hive/llap/LlapCacheAwareFs.java:      LOG.debug("Unregistering " + cachePath);
./ql/src/java/org/apache/hadoop/hive/llap/LlapCacheAwareFs.java:        LOG.debug("Buffers after cache " + RecordReaderUtils.stringifyDiskRanges(drl));
./ql/src/java/org/apache/hadoop/hive/llap/LlapOutputFormatService.java:      LOG.debug("registering socket for: " + id);
./ql/src/java/org/apache/hadoop/hive/llap/ProactiveEviction.java:        LOG.debug(request.toString());
./ql/src/java/org/apache/hadoop/hive/llap/ProactiveEviction.java:          LOG.debug("Requesting proactive eviction for entities in database {}", protoRequest.getDbName());
./ql/src/java/org/apache/hadoop/hive/llap/ProactiveEviction.java:          LOG.debug("Proactively evicted {} bytes", response.getEvictedBytes());
./ql/src/java/org/apache/hadoop/hive/llap/ProactiveEviction.java:          LOG.debug("Proactive eviction freed {} bytes on LLAP daemon {} in total", evictedBytes, instance.toString());
./ql/src/java/org/apache/hadoop/hive/llap/LlapHiveUtils.java:      LOG.debug("Initializing for input " + inputName);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:    LOG.debug("User jar set to " + job.getJar());
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:            LOG.debug("Adding original file " + path + " to dirs to search");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:        LOG.debug("Adding base directory " + baseDir + " to dirs to search");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Adding delta " + delta.getPath() + " to directories to search");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Read length of " + length);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Read numElements of " + numElements);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:        LOG.debug("Read file length of " + len);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Read bucket number of " + bucketNum);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Read base path length of " + len);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Returning " + splits.size() + " splits");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Adding " + file.toString() + " to list of files for splits");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Moving contents of " + tmpLocation.toString() + " to " +
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java:      LOG.debug("Removing " + tmpLocation.toString());
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java:        LOG.debug("Cleaner thread finished one loop.");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java:        LOG.debug("Cleaning based on writeIdList: " + validWriteIdList);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java:      LOG.debug("Going to delete path " + dead.toString());
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java:        LOG.debug("Going to delete path " + dead.toString());
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java:    LOG.debug("Determining who to run the job as.");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java:      LOG.debug("Running job as " + stat.getOwner());
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java:      LOG.debug("Unable to stat file as current user, trying as table owner");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java:        LOG.debug("Running job as " + wrapper.get(0));
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java:          LOG.debug(ci + ": No existing stats found.  Will not run analyze.");
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java:        LOG.debug("Heartbeating compaction transaction id {} for table: {}", compactionTxn, tableName);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java:        LOG.debug("Successfully stop the heartbeating the transaction {}", this.compactionTxn);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java:      LOG.debug("Processing compaction request " + ci);
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java:      LOG.debug("ValidCompactWriteIdList: " + tblValidWriteIds.writeToString());
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java:          LOG.debug("Found " + potentials.size() + " potential compactions, " +
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java:      LOG.debug("Found too many aborted transactions for " + ci.getFullPartitionName() + ", " +
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java:      LOG.debug("Found an aborted transaction for " + ci.getFullPartitionName()
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java:        LOG.debug(msg.toString());
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java:      LOG.debug("Not enough deltas to initiate compaction for table=" + ci.tableName + "partition=" + ci.partName
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java:      LOG.debug("Requesting a major compaction for a MM table; found " + deltas.size() + " deltas, threshold is "
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java:    LOG.debug("Found " + deltas.size() + " delta files, and " + (noBase ? "no" : "has") + " base," +
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/MmMajorQueryCompactor.java:    LOG.debug("Going to delete directories for aborted transactions for MM table " + table.getDbName() + "." + table
./ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/MmMinorQueryCompactor.java:    LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:      LOG.debug("releaseReader: entry: {}, readerCount: {}", this, readerCount);
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:      LOG.debug("addReader: entry: {}, readerCount: {}, added: {}", this, readerCount, added);
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:    LOG.debug("QueryResultsCache lookup for query: {}", request.queryText);
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:    LOG.debug("QueryResultsCache lookup result: {}", result);
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:    LOG.debug("Table changed: {}.{}, at {}", dbName, tableName, updateTime);
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:          LOG.debug("Checking writeIds for table {}: currentWriteIdForTable {}, cachedWriteIdForTable {}",
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:            LOG.debug("Cached query no longer valid due to table {}", tableUsed.getFullyQualifiedName());
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:      LOG.debug("Cache entry size {} larger than max entry size ({})", size, maxEntrySize);
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:      LOG.debug("Handling event {} on table {}.{}", event.getEventType(), dbName, tableName);
./ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java:        LOG.debug("Cache not instantiated, skipping event on {}.{}", dbName, tableName);
./ql/src/java/org/apache/hadoop/hive/ql/util/ResourceDownloader.java:    LOG.debug("Converting to local {}", srcUri);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:        LOG.debug("found {} tables in {}", tableNames.size(), dbName);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:        LOG.debug("found {} {}s in {}", tableNames.size(), runOptions.tableType.name(), dbName);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:      LOG.debug("Processing table {}", getQualifiedName(dbName, tableName));
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:            LOG.debug("Converting {} to external table. {}", getQualifiedName(tableObj), reason);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:          LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:          LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:          LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:          LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:          LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:      LOG.debug("Setting owner/group of {} to {}/{}", path, userName, groupName);
./ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java:      LOG.debug("Setting perms of {} to {}", path, perms);
./ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java:    LOG.debug("Looking for databases");
./ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java:    LOG.debug("Found " + databases.size() + " databases to process");
./ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java:      LOG.debug("found " + tables.size() + " tables in " + dbName);
./ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java:        LOG.debug("processing table " + Warehouse.getQualifiedName(t));
./ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java:          LOG.debug("need to rename: " + file + " to " + newFile);
./ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java:      LOG.debug("need to rename: " + renamePair.getOldPath() + " to " + renamePair.getNewPath());
./ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java:      LOG.debug("Writing CRUD conversion commands to " + fileName);
./ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java:      LOG.debug("Writing managed table conversion commands to " + fileName);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/JoinVisitor.java:      LOG.debug("Translating operator rel#" + joinRel.getId() + ":" + joinRel.getRelTypeName()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/JoinVisitor.java:      LOG.debug("Generated " + joinOp + " with row schema: [" + joinOp.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveSortExchangeVisitor.java:      LOG.debug("Translating operator rel#" + exchangeRel.getId() + ":"
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveOpConverterUtils.java:      LOG.debug("Generated " + selectOp + " with row schema: [" + selectOp.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveOpConverterUtils.java:      LOG.debug("Generated " + rsOp + " with row schema: [" + rsOp.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveProjectVisitor.java:      LOG.debug("Translating operator rel#" + projectRel.getId() + ":"
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveProjectVisitor.java:      LOG.debug("Generated " + selOp + " with row schema: [" + selOp.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveProjectVisitor.java:        LOG.debug("Generated " + ptfOp + " with row schema: [" + ptfOp.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveFilterVisitor.java:      LOG.debug("Translating operator rel#" + filterRel.getId() + ":" + filterRel.getRelTypeName()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveFilterVisitor.java:      LOG.debug("Generated " + filOp + " with row schema: [" + filOp.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveUnionVisitor.java:      LOG.debug("Translating operator rel#" + unionRel.getId() + ":" + unionRel.getRelTypeName()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveUnionVisitor.java:      LOG.debug("Generated " + unionOp + " with row schema: [" + unionOp.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveTableScanVisitor.java:      LOG.debug("Translating operator rel#" + scanRel.getId() + ":" + scanRel.getRelTypeName()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveTableScanVisitor.java:      LOG.debug("Generated " + ts + " with row schema: [" + ts.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveTableFunctionScanVisitor.java:      LOG.debug("Translating operator rel#" + scanRel.getId() + ":"
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveTableFunctionScanVisitor.java:    LOG.debug("genUDTFPlan, Col aliases: {}", colAliases);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveSortLimitVisitor.java:      LOG.debug("Translating operator rel#" + sortRel.getId() + ":" + sortRel.getRelTypeName()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveSortLimitVisitor.java:        LOG.debug("Operator rel#" + sortRel.getId() + ":" + sortRel.getRelTypeName()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveSortLimitVisitor.java:        LOG.debug("Operator rel#" + sortRel.getId() + ":" + sortRel.getRelTypeName()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveSortLimitVisitor.java:        LOG.debug("Operator rel#" + sortRel.getId() + ":" + sortRel.getRelTypeName()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveSortLimitVisitor.java:        LOG.debug("Generated " + resultOp + " with row schema: [" + resultOp.getSchema() + "]");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierUtil.java:          LOG.debug("Old RexCall : " + obyExpr);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierUtil.java:          LOG.debug("New RexCall : " + obyExpr);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java:      LOG.debug("Original plan for PlanModifier\n " + RelOptUtil.toString(newTopNode));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java:        LOG.debug("Plan after top-level introduceDerivedTable\n "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java:      LOG.debug("Plan after nested convertOpTree\n " + RelOptUtil.toString(newTopNode));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java:        LOG.debug("Plan after propagating order\n " + RelOptUtil.toString(newTopNode));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java:      LOG.debug("Plan after fixTopOBSchema\n " + RelOptUtil.toString(newTopNode));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java:      LOG.debug("Final plan after modifier\n " + RelOptUtil.toString(newTopNode));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java:            LOG.debug("Stats for column " + hiveColStats.get(i).getColumnName() +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java:            LOG.debug(hiveColStats.get(i).toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java:          LOG.debug("Stats for column " + cStats.getColumnName() +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java:          LOG.debug(cStats.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinRule.java:      LOG.debug("Original plan cost {} vs Optimized plan cost {}", originalCost, optimizedCost);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinRule.java:          LOG.debug("Plan after:\n" + RelOptUtil.toString(optimized));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinOptimization.java:        LOG.debug("Only plans where root has one input are supported. Root: {}", root);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinOptimization.java:        LOG.debug("Some projected field lineage can not be determined");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinOptimization.java:        LOG.debug("None of the tables has keys projected, unable to join back");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinOptimization.java:        LOG.debug("Nothing was trimmed out.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinOptimization.java:        LOG.debug("Joining back table {}", tableToJoinBack.joinedBackFields.relOptHiveTable.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinOptimization.java:        LOG.debug("Lineage of expression in node {} can not be determined: {}", projectInput, expr);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveCardinalityPreservingJoinOptimization.java:          LOG.debug("Unknown expression that should be a constant: {}", rexNode);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSemiJoinRule.java:      LOG.debug("Matched HiveSemiJoinRule");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSemiJoinRule.java:      LOG.debug("All conditions matched for HiveSemiJoinRule. Going to apply transformation.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HivePointLookupOptimizerRule.java:          LOG.debug("unexpected situation; giving up on this branch");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRelFieldTrimmer.java:        LOG.debug("Got col stats for {} in {}", iRefSet,
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/RelFieldTrimmer.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCSortPushDownRule.java:    LOG.debug("JDBCSortPushDownRule has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCAggregationPushDownRule.java:    LOG.debug("JDBCAggregationPushDownRule has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCJoinPushDownRule.java:    LOG.debug("JDBCJoinPushDownRule has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCExpandExpressionsRule.java:      LOG.debug("JDBCExpandExpressionsRule.FilterCondition has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCExpandExpressionsRule.java:      LOG.debug("JDBCExpandExpressionsRule.JoinCondition has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCExpandExpressionsRule.java:      LOG.debug("JDBCExpandExpressionsRule.ProjectionExpressions has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCProjectPushDownRule.java:    LOG.debug("JDBCProjectPushDownRule has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCFilterPushDownRule.java:    LOG.debug("JDBCFilterPushDown has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCRexCallValidator.java:        LOG.debug("RexOver operator push down is not supported for now with the following operator:" + call);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCUnionPushDownRule.java:    LOG.debug("JDBCUnionPushDown has been called");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveIntersectRewriteRule.java:        LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveIntersectRewriteRule.java:      LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveIntersectRewriteRule.java:        LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveIntersectRewriteRule.java:        LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAntiSemiJoinRule.java:    LOG.debug("Start Matching HiveAntiJoinRule");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAntiSemiJoinRule.java:    LOG.debug("Matched HiveAntiJoinRule");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExceptRewriteRule.java:      LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExceptRewriteRule.java:      LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExceptRewriteRule.java:      LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExceptRewriteRule.java:        LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExceptRewriteRule.java:        LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExceptRewriteRule.java:        LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExceptRewriteRule.java:        LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveMaterializedViewUtils.java:          LOG.debug("Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveMaterializedViewUtils.java:          LOG.debug("Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveMaterializedViewUtils.java:            LOG.debug("Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveMaterializedViewUtils.java:            LOG.debug("Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java:      LOG.debug("Trigger countDistinct rewrite. numCountDistinct is " + numCountDistinct);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java:        LOG.debug(e.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/PartitionPrune.java:              LOG.debug("Exception in closing " + hiveUDF, e);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java:        LOG.debug("Identified Primary - Foreign Key relation from constraints:\n {} {} Row count for join: {}\n" +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java:        LOG.debug("Identified Primary - Foreign Key relation: {} {}", RelOptUtil.toString(join), pkfk);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java:      LOG.debug("No Primary - Foreign Key relation: \n{} Row count for join: {}\n",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java:        LOG.debug("Identified Primary - Foreign Key relation: {} {}", RelOptUtil.toString(rel), pkfk);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/QueryPlanPostProcessor.java:        LOG.debug("Found " + work.getClass().getName() + " - no FileSinkOperation can be present.  executionId=" + executionId);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/QueryPlanPostProcessor.java:            LOG.debug("Found Acid Sink: " + fsd.getDirName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("Bailing out of sort dynamic partition optimization as dynamic partitioning context is null");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("Bailing out of sort dynamic partition optimization as list bucketing is enabled");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("Bailing out of sort dynamic partition optimization as destination table is null");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("Bailing out of sort dynamic partition optimization as destination is a materialized view"
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("Bailing out of sorted dynamic partition optimizer as all dynamic partition" +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("Bailing out of sort dynamic partition optimization as some partition columns " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:      LOG.debug("Got sort order");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("sort position " + i);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("sort order " + i);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("sort null order " + i);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java:        LOG.debug("Memory info during SDPO opt: {}", memoryInfo);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcCtx.java:    LOG.debug("Getting constants of op:" + op + " with rs:" + rs);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcCtx.java:        LOG.debug("Constant of Op " + parent.getOperatorId() + " is not found");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcCtx.java:        LOG.debug("Constant of Op " + parent.getOperatorId() + " " + constMap);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcCtx.java:    LOG.debug("Offering constants " + constants.keySet() + " to operator " + op.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java:        LOG.debug(err);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/OrderlessLimitPushDownOptimizer.java:        LOG.debug("Combining two limits child={}, parent={}, newLimit={}", childLimit, parentLimit, min);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/OrderlessLimitPushDownOptimizer.java:      LOG.debug("Pushing {} through {}", limit.getName(), parent.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/PointLookupOptimizer.java:          LOG.debug("Generated new predicate with IN clause: " + newPredicate);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:          LOG.debug("FULL OUTER MapJoin: only column expressions are supported " + expr.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:            LOG.debug("FULL OUTER MapJoin not enabled: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:        LOG.debug("FULL OUTER MapJoin not enabled: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:        LOG.debug("FULL OUTER MapJoin not enabled: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:        LOG.debug("FULL OUTER MapJoin not enabled: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:        LOG.debug("FULL OUTER MapJoin not enabled: multiple JOIN conditions not supported");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:            LOG.debug("FULL OUTER MapJoin not enabled: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:        LOG.debug("FULL OUTER MapJoin not enabled: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:        LOG.debug("FULL OUTER MapJoin is enabled: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:        LOG.debug("FULL OUTER MapJoin not enabled: Only Tez engine supported");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:        LOG.debug("FULL OUTER MapJoin not enabled: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java:      LOG.debug("FULL OUTER MapJoin enabled");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java:          LOG.debug("Generated new predicate with IN clause: " + newPredicate);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java:          LOG.debug("Partition columns not separated for " + fd + ", is not IN operator : ");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java:          LOG.debug("Partition columns not separated for " + fd + ", children size " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java:          LOG.debug("Partition columns not separated for " + fd +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java:          LOG.debug("Partition columns not separated for " + fd +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MemoryDecider.java:            LOG.debug("MapJoin: " + mj + ", size: " + size + ", remaining: " + remainingSize);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/AnnotateRunTimeStatsOptimizer.java:        LOG.debug("skip setRuntimeStatsDir for " + op.getOperatorId()
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/AnnotateRunTimeStatsOptimizer.java:      LOG.debug("skip annotateRuntimeStats for " + op.getOperatorId());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java:      LOG.debug("Looking at: {}", mapWork.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java:        LOG.debug("No top operators");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java:      LOG.debug("Looking for table scans where optimization is applicable");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java:      LOG.debug("Found {} null table scans", scanTableSize);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SerializeFilter.java:            LOG.debug("Serializing: " + ts.getConf().getFilterExpr().getExprString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SerializeFilter.java:            LOG.debug("Serializing: " + ts.getConf().getFilterObject());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LlapDecider.java:      LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LlapDecider.java:          LOG.debug("Disabling hybrid grace hash join in case of LLAP "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LlapDecider.java:          LOG.debug(String.format("Checking '%s'",expr.getExprString()));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LlapDecider.java:        LOG.debug(String.format("Checking '%s'", agg.getExprString()));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LlapDecider.java:            LOG.debug("Cannot run operator [" + n + "] in llap mode.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SkewJoinResolver.java:          LOG.debug("Not using skew join because the destination table "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SkewJoinResolver.java:          LOG.debug("Not using skew join because the destination table is an insert_only table");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:          LOG.debug("Ignoring vectorization of " + currTask.getClass().getSimpleName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:        LOG.debug(name + " dataColumnCount: " + batchContext.getDataColumnCount());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:          LOG.debug(name + " includeColumns: " + Arrays.toString(dataColumnNums));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:        LOG.debug(name + " partitionColumnCount: " + batchContext.getPartitionColumnCount());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:        LOG.debug(name + " dataColumns: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:        LOG.debug(name + " scratchColumnTypeNames: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:          LOG.debug(name + " neededVirtualColumns: " + Arrays.toString(neededVirtualColumns));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:          LOG.debug("Using reduce tag " + reduceWork.getTag());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:    LOG.debug("vectorizeOperator " + vectorOp.getClass().getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java:    LOG.debug("vectorizeOperator " + vectorOp.getConf().getClass().getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanOptimizer.java:            LOG.debug("Found where false TableScan. {}", op);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanOptimizer.java:      LOG.debug("Found Limit 0 TableScan. {}", nd);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkJoinDeDuplication.java:            LOG.debug("Set {} to forward data", cRS);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkJoinDeDuplication.java:            LOG.debug("Set {} to forward data", cRS);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkJoinDeDuplication.java:            LOG.debug("Set {} to forward data", cRS);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkJoinDeDuplication.java:          LOG.debug("Set {} to FIXED parallelism: {}", pRSOp, maxNumReducers);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkJoinDeDuplication.java:        LOG.debug("Set {} to FIXED parallelism: {}", rsOp, maxNumReducers);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java:          LOG.debug("Threshold " + splitSample.getTotalLength() + " exceeded for pseudoMR mode");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java:            LOG.debug("Threshold " + len + " exceeded for pseudoMR mode");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java:            LOG.debug("Threshold " + len + " exceeded for pseudoMR mode");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:      LOG.debug("Before SharedWorkOptimizer:\n" + Operator.toString(pctx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:    LOG.debug("Sorted tables by size: {}", sortedTables);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:        LOG.debug("After SharedWorkOptimizer:\n" + Operator.toString(pctx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:          LOG.debug("After SharedWorkExtendedOptimizer:\n" + Operator.toString(pctx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:          LOG.debug("After SharedWorkSJOptimizer:\n" + Operator.toString(pctx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:          LOG.debug("After SharedWorkOptimizer merging TS schema:\n" + Operator.toString(pctx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:          LOG.debug("After DPPUnion:\n" + Operator.toString(pctx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:          LOG.debug("Skip {} as it has already been removed", discardableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Skip {} as it has already been removed", retainableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:          LOG.debug("Can we merge {} into {} to remove a scan on {}?", discardableTsOp, retainableTsOp, tableName1);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("{} and {} cannot be merged", retainableTsOp, discardableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("{} and {} do not meet preconditions", retainableTsOp, discardableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("{} and {} cannot be merged", retainableTsOp, discardableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("{} and {} do not meet preconditions", retainableTsOp, discardableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("{} and {} cannot be merged", retainableTsOp, discardableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("{} and {} do not meet preconditions", retainableTsOp, discardableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Merging subtree starting at {} into subtree starting at {}", discardableTsOp, retainableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Merging {} into {}", discardableTsOp, retainableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("Input operator removed: {}", op);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Removing probeDecodeCntx for merged TS op {}", retainableTsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Operator removed: {}", op);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:          LOG.debug("downstream merge: from {} into {}", cJ, cI);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:      LOG.debug("Sorted operators by size: {}", sortedRSGroups);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Skip {} as it has already been removed", discardableRsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("Skip {} as it has already been removed", retainableRsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("{} and {} cannot be merged", retainableRsOp, discardableRsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Checking additional conditions for merging subtree starting at {}"
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("{} and {} do not meet preconditions", retainableRsOp, discardableRsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Merging subtree starting at {} into subtree starting at {}",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("Input operator removed: {}", op);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:            LOG.debug("Operator removed: {}", discardableRsOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:              LOG.debug("Operator removed: {}", op);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:    LOG.debug("DPP information stored in the cache: {}", optimizerCache.tableScanToDPPSource);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:      LOG.debug("Operators not equal: {} and {}", op1, op2);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:      LOG.debug("accumulated data size: {} / max size: {}", sr.dataSize, sr.maxDataSize);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java:      LOG.debug("merging {} and {} would violate dag properties", op1, op2);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SetHashGroupByMinReduction.java:        LOG.debug("Minimum reduction for hash group by operator {} set to {}", groupByOperator, minReductionHashAggrFactor);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/IdentityProjectRemover.java:        LOG.debug("Identity project remover optimization removed : " + sel);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SetReducerParallelism.java:      LOG.debug("Already processed reduce sink: " + sink.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/Generator.java:        LOG.debug("Not evaluating lineage");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/Generator.java:    LOG.debug("Time taken for lineage transform={}", (System.currentTimeMillis() - sTime));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:      LOG.debug("Parent: " + filter.getParentOperators().get(0));
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:      LOG.debug("Filter: " + desc.getPredicateString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:      LOG.debug("TableScan: " + ts);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:            LOG.debug("alias: " + alias);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:            LOG.debug("pruned partition list: ");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:                LOG.debug(p.getCompleteName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:            LOG.debug("No partition pruning necessary.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:          LOG.debug("Column " + column + " is not a partition column");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:            LOG.debug("Initiate semijoin reduction for " + column + " ("
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:        LOG.debug("Disabling semijoin optimzation on {} since it is an external table.",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:              LOG.debug("Join key {} is from {} which is an external table. Disabling semijoin optimization.",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:      LOG.debug("key expr: " + key);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:      LOG.debug("partition key expr: " + partKey);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:      LOG.debug("No ColumnInfo found in {} for {}", parentOfRS.getOperatorId(), internalColName);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:        LOG.debug("Setting size for " + keyBaseAlias + " to " + sjHint.getNumEntries() + " based on the hint");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java:    LOG.debug("DynamicSemiJoinPushdown: Saving RS to TS mapping: " + rsOpFinal + ": " + ts);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java:        LOG.debug("connecting "+parentWork.getName()+" with "+myWork.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java:          LOG.debug("Cloning reduce sink " + parentRS + " for multi-child broadcast edge");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java:        LOG.debug("adding dummy op to work " + myWork.getName() + " from MJ work: " + dummyOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java:        LOG.debug("Operator tree could not be cloned");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java:        LOG.debug("Operator tree not properly cloned!");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionExpressionForMetastore.java:      LOG.debug("Pruning " + len + " partition names took " + timeMs + "ms");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java:      LOG.debug("Filter w/ compacting: " + compactExprString
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java:          LOG.debug("skipping default/bad partition: " + partName);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java:        LOG.debug("retained " + (isUnknown ? "unknown " : "") + "partition: " + partName);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/TablePropertyEnrichmentOptimizer.java:        LOG.debug("TablePropertyEnrichmentOptimizer considers these SerDe classes:");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/TablePropertyEnrichmentOptimizer.java:          LOG.debug(className);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/TablePropertyEnrichmentOptimizer.java:        LOG.debug("Original Table parameters: " + originalTableParameters);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/TablePropertyEnrichmentOptimizer.java:          LOG.debug("SerDe init succeeded for class: " + deserializerClassName);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/TablePropertyEnrichmentOptimizer.java:              LOG.debug("Resolving changed parameters! key=" + property.getKey() + ", value=" + property.getValue());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/TablePropertyEnrichmentOptimizer.java:            LOG.debug("Skipping prefetch for " + deserializerClassName);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketVersionPopulator.java:        LOG.debug("Bucketing version for {} is set to {}", operator, version);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:        LOG.debug("Unsupported types " + priti + "; " + descti);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:        LOG.debug("Unsupported cast " + priti + "; " + descti);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:      LOG.debug("Casting " + desc + " to type " + ti);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:          LOG.debug("Function " + udf.getClass() + " is undeterministic. Don't evaluate immediately.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:          LOG.debug("Folding expression:" + desc + " -> " + shortcut);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:          LOG.debug("Function " + udf.getClass() + " is undeterministic. Don't evaluate immediately.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:            LOG.debug("Folding expression:" + desc + " -> " + constant);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:              LOG.debug("Folding expression:" + desc + " -> " + shortcut);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:          LOG.debug("Folding expression:" + desc + " -> " + col);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:          LOG.debug("Filter {} is identified as a value assignment, propagate it.",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:          LOG.debug("Filter {} is identified as a value assignment, propagate it.",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:        LOG.debug(udf.getClass().getName() + "(" + exprs + ")=" + o);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:            LOG.debug("Replacing column " + col + " with constant " + constant + " in " + op);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:        LOG.debug("Old filter FIL[" + op.getIdentifier() + "] conditions:" + condn.getExprString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:            LOG.debug("Filter expression " + condn + " holds true. Will delete it.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:        LOG.debug("New filter FIL[" + op.getIdentifier() + "] conditions:" + newCondn.getExprString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:          LOG.debug("New column list:(" + StringUtils.join(colList, " ") + ")");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:        LOG.debug("Stop propagate constants on op " + op.getOperatorId());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:            LOG.debug("Skip folding in outer join " + op);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:          LOG.debug("Skip folding in distinct subqueries " + op);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java:            LOG.debug("Skip JOIN-FIL(*)-RS structure.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:    LOG.debug("Pushing {} through {}", topNKey.getName(), select.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:    LOG.debug("Pushing {} through {}", topNKey.getName(), parent.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:    LOG.debug("Pushing a copy of {} through {}", topNKey.getName(), groupBy.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:      LOG.debug("Removing {} above {}", topNKey.getName(), groupBy.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:    LOG.debug("Pushing a copy of {} through {}", topNKey.getName(), reduceSink.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:      LOG.debug("Removing {} above {}", topNKey.getName(), reduceSink.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:    LOG.debug("Pushing a copy of {} through {} and {}",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:      LOG.debug("Removing {} above {}", topNKey.getName(), join.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:      LOG.debug("Not pushing {} through {} as non FK side of the join is filtered", topNKey.getName(), join.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:    LOG.debug("Pushing a copy of {} through {} and {}",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:      LOG.debug("Removing {} above {}", topNKey.getName(), join.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:      LOG.debug("Removing {} above same operator: {}", topNKey.getName(), parent.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:          LOG.debug("Removing {} since child {} supersedes it", parent.getName(), topNKey.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:        LOG.debug("Removing parent of {} since it supersedes", topNKey.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:        LOG.debug("Removing {}. Parent {} has same keys but lower topN {} > {}",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyPushdownProcessor.java:        LOG.debug("Removing parent {}. {} has same keys but lower topN {} < {}",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:    LOG.debug("Number of nodes = " + numNodes + ". Number of Executors per node = " + numExecutorsPerNode);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:      LOG.debug("Conversion to bucket map join failed.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:        LOG.debug("External table {} found in join - disabling SMB join.", sb.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:        LOG.debug("Found correlation optimizer operators. Cannot convert to SMB at this time.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:          LOG.debug("SMB Join can't be performed due to bucketing version mismatch");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:          LOG.debug("External table {} found in join - disabling bucket map join.", sb.toString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:      LOG.debug("No big table selected, no MapJoin");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:      LOG.debug("Conditions to convert to MapJoin are not met");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:          LOG.debug("Found semijoin optimization from the big table side of a map join, which will cause a task cycle. "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:      LOG.debug("Real big table reducers = " + numReducers);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:    LOG.debug("Estimated NDV for input {}: {}; Max NDV for MapJoin conversion: {}",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:      LOG.debug("Number of different entries for HashTable is greater than the max; "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:    LOG.debug("Estimated size for input {}: {}; Max size for DPHJ conversion: {}",
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java:      LOG.debug("Size of input is greater than the max; "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java:      LOG.debug("Reduce Sink Operator " + op.getIdentifier() + " key:" + keys);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("[0] STATS-" + tsop.toString() + " (" + table.getTableName() + "): " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:        LOG.debug("Failed to retrieve stats ", e);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("[0] STATS-" + sop.toString() + ": " + stats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:            LOG.debug("[1] STATS-" + sop.toString() + ": " + parentStats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:            LOG.debug("[0] STATS-" + fop.toString() + ": " + st.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:            LOG.debug("[1] STATS-" + fop.toString() + ": " + st.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("Estimating row count for " + pred + " Original num rows: " + currNumRows +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:        LOG.debug("Estimating row count for " + pred + " Original num rows: " + stats.getNumRows() +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:        LOG.debug("STATS-" + gop.toString() + ": inputSize: " + inputSize + " maxSplitSize: " +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("STATS-" + gop.toString() + " hashAgg: " + hashAgg);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:            LOG.debug("STATS-" + gop.toString() + ": ndvProduct became 0 as some column does not" +
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:                LOG.debug("[Case 4] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:                LOG.debug("[Case 3] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:                LOG.debug("[Case 6] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:                LOG.debug("[Case 5] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:              LOG.debug("[Case 8] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:              LOG.debug("[Case 9] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:                LOG.debug("[Case 2] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:                LOG.debug("[Case 1] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:              LOG.debug("[Case 7] STATS-" + gop.toString() + ": cardinality: " + cardinality);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:        LOG.debug("[0] STATS-" + gop.toString() + ": " + stats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("[0] STATS-" + jop.toString() + ": " + stats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("[1] STATS-" + jop.toString() + ": " + wcStats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:        LOG.debug("STATS-" + jop.toString() + ": detects none/multiple PK parents.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("STATS-" + jop.toString() + ": PK parent id(s) - " + parentIds);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("STATS-" + jop.toString() + ": FK parent id(s) - " + parentIds);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:        LOG.debug("STATS-" + jop.toString() + ": Overflow in number of rows. "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:        LOG.debug("STATS-" + jop.toString() + ": Equals 0 in number of rows. "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("Unhandled join type in stats estimation: " + joinCond.getType());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("[0] STATS-" + lop.toString() + ": " + stats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:            LOG.debug("[1] STATS-" + lop.toString() + ": " + wcStats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("[0] STATS-" + rop.toString() + ": " + outStats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:          LOG.debug("[0] STATS-" + uop.toString() + ": " + st.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:                LOG.debug("[0] STATS-" + op.toString() + ": " + stats.extendedToString());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java:    LOG.debug("using runtime stats for {}; {}", op, os.get());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkReduceSinkMapJoinProc.java:    LOG.debug("Mapjoin "+mapJoinOp+", pos: "+pos+" --> "+parentWork.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkReduceSinkMapJoinProc.java:        LOG.debug("connecting "+parentWork.getName()+" with "+myWork.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java:      LOG.debug("Checking map join optimization for operator {} using TS stats", joinOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java:            LOG.debug("Found a big table branch with parent operator {} and position {}", parentOp, pos);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java:            LOG.debug("Cannot enable map join optimization for operator {}", joinOp);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java:        LOG.debug("Skipping sink " + sink + " for now as we haven't seen all its parents.");
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java:      LOG.debug("Already processed reduce sink: " + sink.getName());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java:                  LOG.debug("Sibling " + sibling + " has stats: " + sibling.getStatistics());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java:                    LOG.debug("Table source " + source + " has stats: " + source.getStatistics());
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java:            LOG.debug("Gathered stats for sink " + sink + ". Total size is "
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java:          LOG.debug("Set parallelism for sink " + sink + " to " + numberOfReducers
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java:          LOG.debug("Adding " + path + " of table " + alias_id);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java:          LOG.debug("Information added for path " + path);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java:          LOG.debug("can't pre-create table for CTAS", e);
./ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java:          LOG.debug("can't pre-create table for MV", e);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/fallback/FallbackHiveAuthorizer.java:      LOG.debug("Configuring hooks : " + hooks);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java:      LOG.debug("Configuring hooks : " + hooks);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidator.java:      LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidator.java:      LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java:        LOG.debug("Checking fs privileges for multiple files that matched {}",
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java:          LOG.debug("Checking fs privileges for parent path {} for nonexistent {}",
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java:          LOG.debug("Checking fs privileges for path itself {}, originally specified as {}",
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java:    LOG.debug("addPrivilegesFromFS:[{}] asked for privileges on [{}] with recurse={} and obtained:[{}]",
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("==> HiveMetaStoreAuthorizer.onEvent(): EventType=" + preEventContext.getEventType());
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("<== HiveMetaStoreAuthorizer.onEvent(): EventType=" + preEventContext.getEventType());
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("HiveMetaStoreAuthorizer.filterDatabases()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("HiveMetaStoreAuthorizer.filterDatabases() :" + filteredDatabases);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("==> HiveMetaStoreAuthorizer.filterTableNames()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("<== HiveMetaStoreAuthorizer.filterTableNames() : " + filteredTableNames);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("==> HiveMetaStoreAuthorizer.filterTables()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("<== HiveMetaStoreAuthorizer.filterTables(): " + filteredTables);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("==> HiveMetaStoreAuthorizer.filterDatabaseObjects()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("<== HiveMetaStoreAuthorizer.filterDatabaseObjects() :" + ret );
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("==> HiveMetaStoreAuthorizer.buildAuthzContext(): EventType=" + preEventContext.getEventType());
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("<== HiveMetaStoreAuthorizer.buildAuthzContext(): EventType=" + preEventContext.getEventType() + "; ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("==> HiveMetaStoreAuthorizer.checkPrivileges(): authzContext=" + authzContext + ", authorizer=" + authorizer);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("<== HiveMetaStoreAuthorizer.checkPrivileges(): authzContext=" + authzContext + ", authorizer=" + authorizer);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("==> HiveMetaStoreAuthorizer.skipAuthorization()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java:    LOG.debug("<== HiveMetaStoreAuthorizer.skipAuthorization(): " + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/filtercontext/DatabaseFilterContext.java:    LOG.debug("==> DatabaseFilterContext.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/filtercontext/DatabaseFilterContext.java:    LOG.debug("<== DatabaseFilterContext.getOutputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/filtercontext/TableFilterContext.java:    LOG.debug("==> TableFilterContext.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/filtercontext/TableFilterContext.java:    LOG.debug("<== TableFilterContext.getOutputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterDatabaseEvent.java:      LOG.debug("==> AlterDatabaseEvent.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterDatabaseEvent.java:        LOG.debug("<== AlterDatabaseEvent.getOutputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/DropTableEvent.java:      LOG.debug("==> DropTableEvent.getInputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/DropTableEvent.java:      LOG.debug("<== DropTableEvent.getInputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AddPartitionEvent.java:      LOG.debug("==> AddPartitionEvent.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AddPartitionEvent.java:      LOG.debug("<== AddPartitionEvent.getOutputHObjs(): ret=" + ret );
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterPartitionEvent.java:      LOG.debug("==> AlterPartitionEvent.getInputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterPartitionEvent.java:      LOG.debug("<== AlterPartitionEvent.getInputHObjs()" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterPartitionEvent.java:      LOG.debug("==> AlterPartitionEvent.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterPartitionEvent.java:      LOG.debug("<== AlterPartitionEvent.getOutputHObjs()" + ret );
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/CreateTableEvent.java:      LOG.debug("==> CreateTableEvent.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/CreateTableEvent.java:      LOG.debug("<== CreateTableEvent.getOutputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/LoadPartitionDoneEvent.java:      LOG.debug("==> DropTableEvent.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/LoadPartitionDoneEvent.java:      LOG.debug("<== DropTableEvent.getOutputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/DropDatabaseEvent.java:      LOG.debug("==> DropDatabaseEvent.getInputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/DropDatabaseEvent.java:      LOG.debug("<== DropDatabaseEvent.getInputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/ReadDatabaseEvent.java:    LOG.debug("==> ReadDatabaseEvent.getInputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/ReadDatabaseEvent.java:      LOG.debug("<== ReadDatabaseEvent.getInputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterTableEvent.java:      LOG.debug("==> AlterTableEvent.getInputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterTableEvent.java:      LOG.debug("<== AlterTableEvent.getInputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterTableEvent.java:      LOG.debug("==> AlterTableEvent.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterTableEvent.java:      LOG.debug("<== AlterTableEvent.getOutputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/ReadTableEvent.java:    LOG.debug("==> ReadTableEvent.getInputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/ReadTableEvent.java:    LOG.debug("<== ReadTableEvent.getInputHObjs()" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/DropPartitionEvent.java:      LOG.debug("==> DropPartitionEvent.getInputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/DropPartitionEvent.java:      LOG.debug("<== DropPartitionEvent.getInputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/CreateDatabaseEvent.java:      LOG.debug("==> CreateDatabaseEvent.getOutputHObjs()");
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/CreateDatabaseEvent.java:        LOG.debug("<== CreateDatabaseEvent.getOutputHObjs(): ret=" + ret);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/PrivilegeSynchronizer.java:            LOG.debug("processing " + dbName);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/PrivilegeSynchronizer.java:              LOG.debug("processing " + dbName + "." + tblName);
./ql/src/java/org/apache/hadoop/hive/ql/security/authorization/PrivilegeSynchronizer.java:                LOG.debug("Unable to synchronize " + tblName + ":" + e.getMessage());
./ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java:      LOG.debug("Configuring JobConf for table {}.{}", fs.getConf().getTableInfo().getDbName(),
./ql/src/java/org/apache/hadoop/hive/ql/plan/mapper/StatsSources.java:        LOG.debug(sb.toString());
./ql/src/java/org/apache/hadoop/hive/ql/plan/mapper/StatsSources.java:        LOG.debug("Ignoring {}, marked with OperatorStats.IncorrectRuntimeStatsMarker", sig.get(0));
./ql/src/java/org/apache/hadoop/hive/ql/plan/mapper/MetastoreStatsConnector.java:      LOG.debug(msg, e);
./ql/src/java/org/apache/hadoop/hive/ql/Context.java:      LOG.debug("Created staging dir = " + dir + " for path = " + inputPath);
./ql/src/java/org/apache/hadoop/hive/ql/Context.java:        LOG.debug("Deleting result cache dir: {}", p);
./ql/src/java/org/apache/hadoop/hive/ql/Context.java:          LOG.debug("Skip deleting stagingDir: " + p);
./ql/src/java/org/apache/hadoop/hive/ql/Context.java:          LOG.debug("Deleting result dir: {}", resDir);
./ql/src/java/org/apache/hadoop/hive/ql/Context.java:        LOG.debug("Deleting result file: {}",  resFile);
./ql/src/java/org/apache/hadoop/hive/ql/ddl/database/show/ShowDatabasesOperation.java:      LOG.debug("pattern: {}", desc.getPattern());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/database/alter/AbstractAlterDatabaseOperation.java:      LOG.debug("DDLTask: Alter Database {} is skipped as database is newer than update", dbName);
./ql/src/java/org/apache/hadoop/hive/ql/ddl/function/drop/DropFunctionOperation.java:        LOG.debug("FunctionTask: Drop Function {} is skipped as database {} is newer than update", functionName,
./ql/src/java/org/apache/hadoop/hive/ql/ddl/function/create/CreateFunctionOperation.java:        LOG.debug("FunctionTask: Create Function {} is skipped as database {} is newer than update",
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/misc/truncate/TruncateTableOperation.java:      LOG.debug("DDLTask: Truncate Table/Partition is skipped as table {} / partition {} is newer than update",
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AbstractAlterTableOperation.java:      LOG.debug("DDLTask: Alter Table is skipped as table {} is newer than update", desc.getDbTableName());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/DescTableOperation.java:      LOG.debug("DDLTask: got data for {}", dbTableName);
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/DescTableOperation.java:      LOG.debug("DDLTask: written data for {}", dbTableName);
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesOperation.java:    LOG.debug("Found {} table(s) matching the SHOW TABLES statement.", tableNames.size());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesOperation.java:    LOG.debug("Found {} table(s) matching the SHOW EXTENDED TABLES statement.", tableObjects.size());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/ShowTableStatusOperation.java:      LOG.debug("pattern: {}", desc.getPattern());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/DropTableOperation.java:        LOG.debug("DDLTask: Drop Table is skipped as table {} is newer than update", desc.getTableName());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/PartitionUtils.java:          LOG.debug("Wrong specification" + StringUtils.stringifyException(e));
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/rename/AlterTableRenamePartitionOperation.java:      LOG.debug("DDLTask: Rename Partition is skipped as table {} / partition {} is newer than update", tableName,
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/CreateTableOperation.java:    LOG.debug("creating table {} on {}", tbl.getFullyQualifiedName(), tbl.getDataLocation());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/CreateTableOperation.java:          LOG.debug("DDLTask: Create Table is skipped as table {} is newer than update", desc.getDbTableName());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/drop/AlterTableDropConstraintOperation.java:      LOG.debug("DDLTask: Alter Table is skipped as table {} is newer than update", desc.getDbTableName());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintOperation.java:      LOG.debug("DDLTask: Alter Table is skipped as table {} is newer than update", desc.getDbTableName());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintOperation.java:            LOG.debug("InvalidObjectException: ", e);
./ql/src/java/org/apache/hadoop/hive/ql/ddl/DDLUtils.java:        LOG.debug("Ignoring request to add {} because {} is present", newWriteEntity.toStringDetail(),
./ql/src/java/org/apache/hadoop/hive/ql/ddl/DDLUtils.java:        LOG.debug("Found class for {}", serdeName);
./ql/src/java/org/apache/hadoop/hive/ql/ddl/view/show/ShowViewsOperation.java:    LOG.debug("Found {} view(s) matching the SHOW VIEWS statement.", viewNames.size());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/view/create/CreateViewOperation.java:          LOG.debug("DDLTask: Create View is skipped as view {} is newer than update", desc.getViewName());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/update/MaterializedViewUpdateOperation.java:      LOG.debug("Exception during materialized view cache update", e);
./ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/show/ShowMaterializedViewsOperation.java:    LOG.debug("Found {} materialized view(s) matching the SHOW MATERIALIZED VIEWS statement.", viewObjects.size());
./ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rebuild/AlterMaterializedViewRebuildAnalyzer.java:    LOG.debug("Rebuilding materialized view " + tableName.getNotEmptyDbTable());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/SimplePredicatePushDown.java:      LOG.debug("After PPD:\n" + Operator.toString(pctx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/ppd/SyntheticJoinPredicate.java:              LOG.debug("Synthetic predicate in " + join + ": " + srcPos + " --> " + targetPos + " (" + syntheticInExpr + ")");
./ql/src/java/org/apache/hadoop/hive/ql/ppd/SyntheticJoinPredicate.java:                    LOG.debug("Additional synthetic predicate in " + join + ": " + srcPos + " --> " + targetPos + " (" + expr + ")");
./ql/src/java/org/apache/hadoop/hive/ql/ppd/SyntheticJoinPredicate.java:              LOG.debug(" Non-Equi Join Predicate " + funcExpr);
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:      LOG.debug("Processing for {}", nd.toString());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:      LOG.debug("Processing for {}", nd.toString());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:      LOG.debug("Processing for {}", nd.toString());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:      LOG.debug("Processing for {}", nd.toString());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:      LOG.debug("Processing for {}", nd.toString());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:      LOG.debug("Processing for {}", nd.toString());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:      LOG.debug("Processing for {}", nd.toString());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:        LOG.debug(sb.toString());
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:        LOG.debug("No pushdown possible for predicate:  "
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:      LOG.debug("Original predicate:  "
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicatePushDown.java:      LOG.debug("After PPD:\n" + Operator.toString(pctx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java:        LOG.debug("Replacing SETCOLREF with ALLCOLREF because we couldn't find the root INSERT");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java:        LOG.debug("Replacing SETCOLREF with ALLCOLREF because we couldn't find the FROM");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java:        LOG.debug("Replacing SETCOLREF with ALLCOLREF because we couldn't find the SELECT");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java:          LOG.debug("Replacing SETCOLREF with ALLCOLREF because we couldn't find the QUERY");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java:          LOG.debug("Replacing SETCOLREF with ALLCOLREF because of nested ALLCOLREF");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java:          LOG.debug("Replacing SETCOLREF with ALLCOLREF because of the nested node "
./ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java:        LOG.debug("Skip creating child column reference because of regexp used as alias: " + colAlias);
./ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java:        LOG.debug("Replacing SETCOLREF with ALLCOLREF because of duplicate alias " + colAlias);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:              LOG.debug("Propagating hints to QB: " + oldHints);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:                LOG.debug("Propagating hints to QB: " + oldHints);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:      STATIC_LOG.debug("Replacing " + colName + " (produced by CBO) by " + nqColumnName);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:    LOG.debug("Translating the following plan:\n" + RelOptUtil.toString(modifiedOptimizedOptiqPlan));
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("Plan before removing subquery:\n" + RelOptUtil.toString(calciteGenPlan));
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("Plan just after removing subquery:\n" + RelOptUtil.toString(calciteGenPlan));
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("Plan after decorrelation:\n" + RelOptUtil.toString(calciteGenPlan));
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("CBO Planning details:\n");
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("Original Plan:\n" + RelOptUtil.toString(calciteGenPlan));
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("Plan After PPD, PartPruning, ColumnPruning:\n"
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("Plan After Join Reordering:\n"
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:              LOG.debug("User does not have privilege to use materialized view {}",
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:          LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:              LOG.debug("Dialect for table {}: {}", tableName, jdbcDialect.getClass().getName());
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:          LOG.debug("Can not find column in " + ref.getText() + ". The error msg is "
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:      LOG.debug("Handling query hints: " + hint);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:          LOG.debug("Find UDTF " + funcName);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("UDTF table alias is " + udtfTableAlias);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("UDTF col aliases are " + udtfColAliases);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:      LOG.debug("Table alias: " + outputTableAlias + " Col aliases: " + colAliases);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:          LOG.debug(msg + " because it: " + reason);
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:        LOG.debug("Created Plan for Query Block " + qb.getId());
./ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java:          LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java:          LOG.debug("Task: " + mrtask.getId() + ", Summary: " +
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:    LOG.debug("ReplicationSemanticAanalyzer: analyzeInternal");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:    LOG.debug(ast.getName() + ":" + ast.getToken().getText() + "=" + ast.getText());
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:        LOG.debug("ReplicationSemanticAnalyzer: analyzeInternal: dump");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:        LOG.debug("ReplicationSemanticAnalyzer: analyzeInternal: load");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:        LOG.debug("ReplicationSemanticAnalyzer: analyzeInternal: status");
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:          LOG.debug("{} contains an incremental dump", loadPath);
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:          LOG.debug("{} contains an bootstrap dump", loadPath);
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:    LOG.debug("ReplicationSemanticAnalyzer.analyzeReplStatus: writing repl.last.id={} out to {} using configuration {}",
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:    LOG.debug("prepareReturnValues : " + schema);
./ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java:      LOG.debug("    > " + s);
./ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java:      STATIC_LOG.debug("table {} location is {}", tblDesc.getTableName(), parsedLocation);
./ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java:      LOG.debug("partition {} has data location: {}", partition, location);
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/CopyUtils.java:          LOG.debug(" file " + newDestFile + " is deleted before renaming");
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java:    LOG.debug("Modified encoded uri {}, to {} ", cmEncodedURI, modifiedURI);
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java:      LOG.debug("scheduling partition dump {}", partition.getName());
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java:        LOG.debug("Thread: {}, start partition dump {}", threadName, partitionName);
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java:          LOG.debug("Thread: {}, finish partition dump {}", threadName, partitionName);
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddForeignKeyHandler.java:    LOG.debug("Processing#{} ADD_FOREIGNKEY_MESSAGE message : {}", fromEventId(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddNotNullConstraintHandler.java:    LOG.debug("Processing#{} ADD_NOTNULLCONSTRAINT_MESSAGE message : {}", fromEventId(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CreateTableHandler.java:      LOG.debug("Event#{} was a CREATE_TABLE_EVENT with no table listed", fromEventId());
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddPrimaryKeyHandler.java:    LOG.debug("Processing#{} ADD_PRIMARYKEY_MESSAGE message : {}", fromEventId(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddPartitionHandler.java:      LOG.debug("Event#{} was a ADD_PTN_EVENT with no table listed", fromEventId());
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddPartitionHandler.java:      LOG.debug("Event#{} was an ADD_PTN_EVENT with no partitions", fromEventId());
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/UpdatePartColStatHandler.java:      LOG.debug("Event#{} was an event of type {} with no table listed", fromEventId(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AlterTableHandler.java:        LOG.debug("Table " + before.getTableName() + " does not satisfy the policy");
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddDefaultConstraintHandler.java:    LOG.debug("Processing#{} ADD_DEFAULTCONSTRAINT_MESSAGE message : {}", fromEventId(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddUniqueConstraintHandler.java:    LOG.debug("Processing#{} ADD_UNIQUECONSTRAINT_MESSAGE message : {}", fromEventId(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CommitTxnHandler.java:        LOG.debug("writeEventsInfoList will be removed from commit message because we are " +
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CommitTxnHandler.java:        LOG.debug("writeEventsInfoList will be removed from commit message because we are " +
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CommitTxnHandler.java:        LOG.debug("payload for commit txn event : " + eventMessageAsJSON);
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddCheckConstraintHandler.java:    LOG.debug("Processing#{} ADD_CHECKCONSTRAINT_MESSAGE message : {}", fromEventId(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java:      LOG.debug("Stage Started {}, {}, {}", stageName, metricMap.size(), metricMap );
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java:      LOG.debug("Stage ended {}, {}, {}", stageName, status, lastReplId );
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java:      LOG.debug("Stage Ended {}, {}", stageName, status );
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java:      LOG.debug("Stage Ended {}, {}", stageName, status );
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java:      LOG.debug("Stage progress {}, {}, {}", stageName, metricName, count );
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/MetricSink.java:      LOG.debug("Metrics Sink Initialised with frequency {} ", getFrequencyInSecs());
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/MetricSink.java:        LOG.debug("Updating metrics to DB");
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/MetricSink.java:          LOG.debug("Converting metrics to thrift metrics {} ", metrics.size());
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/MetricSink.java:            LOG.debug("Metric to be persisted {} ", persistentMetric);
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/MetricSink.java:              LOG.debug("Persisting metrics to DB {} ", metricList.getReplicationMetricListSize());
./ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/MetricSink.java:          LOG.debug("No Metrics to Update ");
./ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java:    LOG.debug(fromScheme + "@" + fromAuthority + "@" + path);
./ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java:          LOG.debug("bucket ID for file " + oneSrc.getPath() + " = " + bucketId
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:    LOG.debug("Root operator: " + root);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:    LOG.debug("Leaf operator: " + operator);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:        LOG.debug("Processing map join: " + mj);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:                LOG.debug("Adding dummy ops to work: " + work.getName() + ": " + context.linkChildOpWithDummyOp.get(mj));
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:              LOG.debug("connecting "+parentWork.getName()+" with "+work.getName());
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:                  LOG.debug("Cloning reduce sink " + r + " for multi-child broadcast edge");
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:        LOG.debug("Removing " + parent + " as parent from " + root);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:      LOG.debug("Second pass. Leaf operator: "+operator
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:      LOG.debug("First pass. Leaf operator: "+operator);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java:    LOG.debug("Connecting union work (" + unionWork + ") with work (" + work + ")");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:          LOG.debug("Component: ");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:            LOG.debug("Operator: " + co.getName() + ", " + co.getIdentifier());
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:        LOG.debug("Cycle found. Removing semijoin "
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Adding special edge: " + o.getName() + " --> " + ts.toString());
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:        LOG.debug("Adding special edge: From terminal op to semijoin edge "  + o.getName() + " --> " + rs.toString());
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:          LOG.debug("Adding special edge: " + o.getName() + " --> " + ts.toString());
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:    LOG.debug("There are " + procCtx.eventOperatorSet.size() + " app master events.");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Handling AppMasterEventOperator: " + event);
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Skipping null scan query optimization");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Skipping metadata only query optimization");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Skipping cross product analysis");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Skipping llap pre-vectorization pass");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Skipping vectorization");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Skipping stage id rearranger");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Skipping llap decider");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:            LOG.debug("Semijoin optimization found going to SMB join. Removing semijoin "
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:                LOG.debug("expectedEntries=" + expectedEntries + ". "
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:                LOG.debug("Insufficient rows (" + numRows + ") to justify semijoin optimization. Removing semijoin "
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:        LOG.debug("Removing redundant " + OperatorUtils.getOpNamePretty(p.getKey()) + " - " + OperatorUtils.getOpNamePretty(p.getValue()));
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:          LOG.debug("Semijoin optimization with Union operator. Removing semijoin "
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:            LOG.debug("Semijoin optimization with parallel edge to map join. Removing semijoin " +
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:            LOG.debug("ProbeDecode MJ for TS {}  with CacheKey {} MJ Pos {} ColName {} with Ratio {}",
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:        LOG.debug("ProbeDecode MJ {} with Ratio {}", selectedMJOp, selectedMJOpRatio);
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:          LOG.debug("ProbeDecode MJ {} Ratio {} is lower than existing MJ {} with Ratio {}",
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("Computing key domain cardinality, keyDomainCardinality=" + keyDomainCardinality
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("BloomFilter selectivity for " + selCol + " to " + tsCol + ", selKeyCardinality=" + selKeyCardinality
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:      LOG.debug("No stats available to compute BloomFilter benefit");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:        LOG.debug("BloomFilter benefit=" + benefit
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:    LOG.debug("netBenefit=" + netBenefit);
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:          LOG.debug("Computing BloomFilter cost/benefit for " + OperatorUtils.getOpNamePretty(rs)
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:                LOG.debug("Unwrapped column expression from ExprNodeFieldDesc");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:                  LOG.debug("Adding " + OperatorUtils.getOpNamePretty(prevResult.rsOperator)
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:                  LOG.debug("Adding " + OperatorUtils.getOpNamePretty(rs) + " for re-iteration");
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:          LOG.debug("Old stats for {}: {}", roi.filterOperator, roi.filterStats);
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:          LOG.debug("Number of rows reduction: {}/{}", newNumRows, roi.filterStats.getNumRows());
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:          LOG.debug("New stats for {}: {}", roi.filterOperator, roi.filterStats);
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:        LOG.debug("Reduction factor not satisfied for " + OperatorUtils.getOpNamePretty(rs)
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:                  LOG.debug("nDVs = " + nDVs + ", nDVsOfTS = " + nDVsOfTS + " and nDVsOfTSFactored = " + nDVsOfTSFactored
./ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java:              LOG.debug("Caught NPE in markSemiJoinForDPP from ReduceSink " + rs + " to TS " + sjInfo.getTsOp());
./ql/src/java/org/apache/hadoop/hive/ql/parse/type/TypeCheckProcFactory.java:            LOG.debug(ErrorMsg.INVALID_TABLE_OR_COLUMN.toString() + ":"
./ql/src/java/org/apache/hadoop/hive/ql/parse/type/HiveFunctionHelper.java:        LOG.debug("CBO: Couldn't Obtain UDAF evaluators for " + aggregateName
./ql/src/java/org/apache/hadoop/hive/ql/parse/type/TypeCheckCtx.java:      LOG.debug("Setting error: [" + error + "] from "
./ql/src/java/org/apache/hadoop/hive/ql/parse/TableMask.java:    LOG.debug("TableMask creates `" + sb.toString() + "`");
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:    LOG.debug("Adding reduce work (" + reduceWork.getName() + ") for " + root);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:    LOG.debug("Setting up reduce sink: " + reduceSink
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:    LOG.debug("Adding map work (" + mapWork.getName() + ") for " + root);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:    LOG.debug("Setting dummy ops for work " + work.getName() + ": " + dummyOps);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:    LOG.debug("ResduceSink " + rs + " to TableScan " + ts);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:    LOG.debug("Connecting Baswork - " + parentWork.getName() + " to " + childWork.getName());
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:    LOG.debug("Removing ReduceSink " + rs + " and TableScan " + ts);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:    LOG.debug("Removing AppMasterEventOperator " + eventOp + " and TableScan " + ts);
./ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java:          LOG.debug("Buffer size for output from operator {} can be set to {}Mb", rsOp, result);
./ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java:      LOG.debug("Skipping optimize operator plan for analyze command.");
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:              LOG.debug("DEFAULT keyword replacement - Inserted {} for table: {}", newNode.getText(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:          LOG.debug("DEFAULT keyword replacement - Inserted {} for table: {}", newNode.getText(),
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("QUERY HINT: {} ", queryHintStr);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Created Filter Plan for {} row schema: {}", qb.getId(), inputRR.toString());
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Created Filter Plan for {} row schema: {}", qb.getId(), inputRR);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:            LOG.debug("Translated [" + oldCol + "] to [" + newCol + "]");
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Created Select Plan for clause: {}", dest);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("tree: {}", selExprList.toStringTree());
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("UDTF table alias is {}", udtfTableAlias);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("UDTF col aliases are {}", udtfColAliases);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("genSelectPlan: input = {} starRr = {}", inputRR, starRR);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Created Select Plan row schema: {}", out_rwsch);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:          LOG.debug("Added default value from metastore: {}", exp);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:        LOG.debug("numBuckets is {} and maxReducers is {}", numBuckets, maxReducers);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Created FileSink Plan for clause: {}dest_path: {} row schema: {}", dest, destinationPath, inputRR);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("Direct insert for ACID tables is enabled.");
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("Set stats collection dir : " + statsTmpLoc);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("Created LimitOperator Plan for clause: " + dest
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Table alias: {} Col aliases: {}", outputTableAlias, colAliases);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("Generate JOIN with post-filtering conditions");
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("Semijoin hint parsed: " + result);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:              LOG.debug("RR before GB " + opParseCtx.get(gbySource).getRowResolver()
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Created Body Plan for Query Block {}", qb.getId());
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Created Table Plan for {} {}", alias, op);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("Set stats collection dir : " + statsTmpLoc);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:      LOG.debug("Created Plan for Query Block " + qb.getId());
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:          STATIC_LOG.debug("Table " + tabIdName + " is not found in walkASTMarkTABREF.");
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:        LOG.debug("Before logical optimization\n" + Operator.toString(pCtx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:        LOG.debug("After logical optimization\n" + Operator.toString(pCtx.getTopOps().values()));
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:          LOG.debug("validated " + usedp.getName());
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:          LOG.debug(usedp.getTable().getTableName());
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:        LOG.debug("not validating writeEntity, because entity is neither table nor partition");
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:            LOG.debug("Exception when validate folder ",ioE);
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:    LOG.debug("Created PTF Plan ");
./ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java:      LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:    LOG.debug("Root operator: " + root);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:    LOG.debug("Leaf operator: " + operator);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:        LOG.debug("Processing map join: " + mj);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:              LOG.debug("connecting " + parentWork.getName() + " with " + work.getName());
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:                  LOG.debug("Cloning reduce sink for multi-child broadcast edge");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:        LOG.debug("Removing " + parent + " as parent from " + root);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:      LOG.debug("Second pass. Leaf operator: " + operator + " has common downstream work:" + childWork);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:        LOG.debug("work " + work.getName() + " is already connected to " + childWork.getName() + " before");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java:      LOG.debug("First pass. Leaf operator: " + operator);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java:    LOG.debug("Adding reduce work (" + reduceWork.getName() + ") for " + root);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java:    LOG.debug("Setting up reduce sink: " + reduceSink
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java:    LOG.debug("Adding map work (" + mapWork.getName() + ") for " + root);
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:          LOG.debug("Component: ");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:            LOG.debug("Operator: " + co.getName() + ", " + co.getIdentifier());
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:        LOG.debug("Adding special edge: " + o.getName() + " --> " + ts.toString());
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:      LOG.debug("Skipping runtime skew join optimization");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:      LOG.debug("Skipping null scan query optimization");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:      LOG.debug("Skipping metadata only query optimization");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:      LOG.debug("Skipping cross product analysis");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:      LOG.debug("Skipping vectorization");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:      LOG.debug("Skipping stage id rearranger");
./ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:      LOG.debug("Skipping combine equivalent work optimization");
./ql/src/java/org/apache/hadoop/hive/ql/Driver.java:      LOG.debug("Exception while shutting down the task runner", e);
./ql/src/java/org/apache/hadoop/hive/ql/Driver.java:      LOG.debug("Exception while clearing the Fetch task", e);
./ql/src/java/org/apache/hadoop/hive/ql/Driver.java:      LOG.debug("Exception while clearing the context ", e);
./ql/src/java/org/apache/hadoop/hive/ql/Driver.java:      LOG.debug(" Exception while closing the resStream ", e);
./ql/src/java/org/apache/hadoop/hive/ql/Driver.java:      LOG.debug(" Exception while clearing the FetchTask ", e);
./ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveRecordReader.java:      LOG.debug("Found spec for " + path + " " + otherPart + " from " + pathToPartInfo);
./ql/src/java/org/apache/hadoop/hive/ql/io/TeradataBinaryFileOutputFormat.java:        LOG.debug(format("The table property %s is: %s", TeradataBinaryRecordReader.TD_ROW_LENGTH, rowLength));
./ql/src/java/org/apache/hadoop/hive/ql/io/TeradataBinaryRecordReader.java:    LOG.debug("initialize the TeradataBinaryRecordReader");
./ql/src/java/org/apache/hadoop/hive/ql/io/TeradataBinaryRecordReader.java:      LOG.debug("No table property in JobConf. Try to recover the table directly");
./ql/src/java/org/apache/hadoop/hive/ql/io/TeradataBinaryRecordReader.java:        LOG.debug(format("the current alias: %s", alias));
./ql/src/java/org/apache/hadoop/hive/ql/io/TeradataBinaryRecordReader.java:    LOG.debug(format("The start of the file split is: %s", start));
./ql/src/java/org/apache/hadoop/hive/ql/io/TeradataBinaryRecordReader.java:    LOG.debug(format("The end of the file split is: %s", end));
./ql/src/java/org/apache/hadoop/hive/ql/io/BatchToRowReader.java:      LOG.debug("Including the columns " + DebugUtils.toString(included));
./ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java:          LOG.debug("CombineHiveInputSplit: pool is already created for " + path +
./ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java:    LOG.debug("Number of splits " + result.size());
./ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java:      LOG.debug("The received input paths are: [" + oldPaths +
./ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java:                LOG.debug("Exception in closing " + in, e);
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:    LOG.debug("in directory " + candidateDirectory.toUri().toString() + " base = " + basePath + " deltas = " +
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("getChildState() ignoring(" + aborted + ") " + child);
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:        LOG.debug("Failed to get files with ID", ioe);
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:    LOG.debug("Setting ValidWriteIdList: " + validWriteIds.toString()
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:            LOG.debug("Stats updater for {}.{} doesn't have a write ID ({})", dbName, tblName, writeId);
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:        LOG.debug("isRawFormat() called on " + dataFile + " which is not an ORC file: " +
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("Cannot extract write ID for a MM table: " + file
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("Cannot extract write ID for a MM table: " + file
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("Adding lock component to lock request {} ", comp);
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("output is null " + (output == null));
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("Adding lock component to lock request " + comp.toString());
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("DirCache got initialized already");
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("Cache entries: {}", Arrays.toString(dirCache.asMap().keySet().toArray()));
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:      LOG.debug("dirCache is not enabled");
./ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java:          LOG.debug("writeIdList: {} from cache: {} is not matching for key: {}",
./ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java:    LOG.debug("Processing file " + inputPath); // Logged at INFO in multiple other places.
./ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java:      LOG.debug(genericUDFClassName + " is not the name of a supported class.  " +
./ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java:      LOG.debug("Avro schema: {}", s);
./ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java:      LOG.debug(e.getMessage(), e);
./ql/src/java/org/apache/hadoop/hive/ql/io/RecordReaderWrapper.java:      LOG.debug("Using {} to read data with skip.header.line.count {} and skip.footer.line.count {}",
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java:        LOG.debug("Serialized tail " + (tfd == null ? "not " : "") + "cached for path: " + path);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java:        LOG.debug("Meta-Info for : " + path + " changed. CachedModificationTime: "
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileFormatProxy.java:        LOG.debug("PPD is adding a split " + i + ": " + si.getOffset() + ", " + si.getLength());
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:      LOG.debug("No ORC pushdown predicate - no column names");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:      LOG.debug("No ORC pushdown predicate");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:      LOG.debug("No ORC pushdown predicate - no column names");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:      LOG.debug("No ORC pushdown predicate");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:              LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:              LOG.debug("Blob storage detected for BI split strategy. Splitting files at boundary {}..", splitSize);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:          LOG.debug("Updated split from {" + index + ": " + si.getOffset() + ", "
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:      LOG.debug("Generate splits schema evolution property " + isSchemaEvolution +
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:            LOG.debug("Split strategy: {}", splitStrategy);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:        LOG.debug(split + " projected_columns_uncompressed_size: "
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:      LOG.debug("getReader:: Read ValidWriteIdList: " + validWriteIdList.toString()
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:      LOG.debug("Creating merger for {} and {}", split.getPath(), Arrays.toString(deltas));
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:        LOG.debug("Eliminating ORC stripe-" + i + " of file '" + filePath
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:            LOG.debug("Dynamic values are not available here {}", dve.getMessage());
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:        LOG.debug("Using schema evolution configuration variables schema.evolution.columns " +
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java:        LOG.debug("Using column configuration variables columns " +
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java:          LOG.debug("key " + getKey() + " > maxkey " + getMaxKey());
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java:            LOG.debug("key " + key + " > maxkey " + getMaxKey());
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java:          LOG.debug("Looking at delta file {}", deltaFile);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java:    LOG.debug("Final reader map {}", readers);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:      LOG.debug("ORC schema = " + schema);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:          LOG.debug("Close on abort for path: {}.. Deleting..", path);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:              LOG.debug("Closing writer for path: {} acid stats: {}", path, indexBuilder.acidStats);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:            LOG.debug("Empty file has been created for overwrite: {}", path);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:            LOG.debug("No insert events in path: {}.. Deleting..", path);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:          LOG.debug("Initializing writer before close (to create empty buckets) for path: {}", path);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:            LOG.debug("Closing delete event writer for path: {} acid stats: {}", path, indexBuilder.acidStats);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:            LOG.debug("No delete events in path: {}.. Deleting..", path);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java:        LOG.debug("Closing and deleting flush length file for path: {}", path);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewInputFormat.java:      LOG.debug("getSplits started");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewInputFormat.java:      LOG.debug("getSplits finished");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java:      LOG.debug("SARG translated into " + sarg);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java:          LOG.debug("ORC schema = " + schema);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:      LOG.debug("findMinMaxKeys() " + ConfVars.FILTER_DELETE_EVENTS + "=false");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:      LOG.debug("findMinMaxKeys(original split)");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:      LOG.debug("findMinMaxKeys() No ORC column stats");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:        LOG.debug("Using SortMergedDeleteEventRegistry");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:        LOG.debug("Num events stats({},x,x)", numEvents);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:        LOG.debug("Num events stats({},{},{})",
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:      LOG.debug("Using ColumnizedDeleteEventRegistry");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:                LOG.debug("Skipping delete delta dir {}", deleteDeltaDir);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:          LOG.debug("Number of delete events(limit, actual)=({},{})",
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java:        LOG.debug("findMinMaxKeys() No ORC column stats");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java:        LOG.debug("Batch has no data for " + columnIndex + ": " + batch);
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java:      LOG.debug("columnIndex: {} columnType: {} streamBuffers.length: {} vectors: {} columnEncoding: {}" +
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java:      LOG.debug("Resulting disk ranges to read (file " + fileKey + "): "
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java:        LOG.debug("Disk ranges after cache (found everything " + isAllInCache.value + "; file "
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java:        LOG.debug("Nothing to read for stripe [" + stripe + "]");
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java:        LOG.debug("Disk ranges after pre-read (file " + fileKey + ", base offset "
./ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java:        LOG.debug("Using LLAP memory manager for orc writer. memPerExecutor: {} maxLoad: {} totalMemPool: {}",
./ql/src/java/org/apache/hadoop/hive/ql/io/CodecPool.java:      LOG.debug("Got recycled compressor");
./ql/src/java/org/apache/hadoop/hive/ql/io/CodecPool.java:      LOG.debug("Got recycled decompressor");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java:      LOG.debug("get override parquet.block.size property via tblproperties");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java:      LOG.debug("get override parquet.enable.dictionary property via tblproperties");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java:      LOG.debug("get override compression properties via tblproperties");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java:        LOG.debug("Turn off filtering due to " + ex);
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java:      LOG.debug("Not pushing filters because FilterExpr is null");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ParquetRecordReaderBase.java:          LOG.debug("All row groups are dropped due to filter predicates");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ParquetRecordReaderBase.java:          LOG.debug("Dropping " + droppedBlocks + " row groups that do not pass filter predicate");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ParquetRecordReaderBase.java:      LOG.debug("PARQUET predicate push down generated.");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ParquetRecordReaderBase.java:      LOG.debug("No PARQUET predicate push down is generated.");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/LeafFilterFactory.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/LeafFilterFactory.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java:          LOG.debug("Using vectorized record reader");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java:          LOG.debug("Using row-mode record reader");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/BaseVectorizedColumnReader.java:      LOG.debug("page size " + bytes.size() + " bytes and " + pageValueCount + " records");
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/BaseVectorizedColumnReader.java:      LOG.debug("reading repetition levels at " + in.position());
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/BaseVectorizedColumnReader.java:      LOG.debug("reading definition levels at " + in.position());
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/BaseVectorizedColumnReader.java:      LOG.debug("reading data at " + in.position());
./ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/BaseVectorizedColumnReader.java:      LOG.debug("page data size " + page.getData().size() + " bytes and " + pageValueCount + " records");
./ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java:        LOG.debug(getClass().getSimpleName() + " in "
./ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java:      LOG.debug("Processing " + ifName);
./ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java:      LOG.debug("Checking " + ifName + " against " + formatList);
./ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java:      LOG.debug("Found spec for " + hsplit.getPath() + " " + part + " from " + pathToPartitionInfo);
./ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java:          LOG.debug("aliases: {} pathToAliases: {} dir: {}", aliases, mrwork.getPathToAliases(), dir);
./ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java:      LOG.debug("Pushdown initiated with filterText = " + filterText + ", filterExpr = "
./ql/src/java/org/apache/hadoop/hive/ql/io/BucketizedHiveInputFormat.java:    LOG.debug("Matches for " + dir + ": " + result);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:      LOG.debug("No locks needed for queryId=" + queryId);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:      LOG.debug("No locks needed for queryId=" + queryId);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:      LOG.debug("Adding global lock: " + lockName);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:      LOG.debug("Committing txn " + JavaUtils.txnIdToString(txnId));
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:      LOG.debug("Rolling back " + JavaUtils.txnIdToString(txnId));
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:        LOG.debug("No need to send heartbeat as there is no transaction and no locks.");
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:      LOG.debug("Started heartbeat with delay/interval = " + initialDelay + "/" + heartbeatInterval +
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:      LOG.debug("Allocated write ID {} for {}.{}", writeId, dbName, tableName);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:      LOG.debug("Started heartbeat for materialization rebuild lock for {} with delay/interval = {}/{} {} for query: {}",
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:        LOG.debug("Heartbeating...for currentUser: " + currentUser);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java:          LOG.debug("Stopped heartbeat for materialization rebuild lock for {} for query: {}",
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java:      LOG.debug("Requested lock= " + lock);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java:        LOG.debug("Starting retry attempt:#{} to acquire locks for lockId={}. QueryId={}",
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java:      LOG.debug("Unlocking " + hiveLock);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java:      LOG.debug("Removed a lock " + removed);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java:        LOG.debug("Removed a lock " + hiveLock);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java:          LOG.debug("About to release lock for {}",
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java:    LOG.debug("Acquiring lock for {} with mode {} {}", key.getName(), mode,
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java:            LOG.debug("Possibly transient ZooKeeper exception: ", e);
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java:      LOG.debug("Requested lock " + key.getDisplayName()
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java:        LOG.debug("Conflicting lock to " + key.getDisplayName()
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java:      LOG.debug("Node " + zLock.getPath() + " or its parent has already been deleted.");
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java:      LOG.debug("Node " + name + " to be deleted is not empty.");
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java:      LOG.debug("Adding " + input.getName() + " to list of lock inputs");
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java:      LOG.debug("Adding " + output.getName() + " to list of lock outputs");
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/EmbeddedLockManager.java:    LOG.debug("Acquiring lock for {} with mode {} {}", key.getName(), mode,
./ql/src/java/org/apache/hadoop/hive/ql/lockmgr/EmbeddedLockManager.java:        LOG.debug("Acquiring lock for {} with mode {}", obj.getObj().getName(),
./ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java:        LOG.debug(LOCK_ACQUIRED_MSG);
./ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java:        LOG.debug("Interrupted Exception ignored", e);
./ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java:      LOG.debug(WAIT_LOCK_ACQUIRE_MSG + command);
./ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java:          LOG.debug("Interrupted Exception ignored", e);
./ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java:    LOG.debug(LOCK_ACQUIRED_MSG);
./ql/src/java/org/apache/hadoop/hive/ql/Compiler.java:    LOG.debug("Setting " + ReplUtils.LAST_REPL_ID_KEY + " = " + lastReplId);
./ql/src/java/org/apache/hadoop/hive/ql/Compiler.java:    LOG.debug("Encoding valid txns info " + txnStr + " txnid:" + txnMgr.getCurrentTxnId());
./ql/src/java/org/apache/hadoop/hive/ql/cleanup/EventualCleanupService.java:      LOG.debug("EventualCleanupService is already running.");
./ql/src/java/org/apache/hadoop/hive/ql/cleanup/EventualCleanupService.java:          LOG.debug("PathCleaner was interrupted");
./ql/src/java/org/apache/hadoop/hive/ql/hooks/RuntimeStatsPersistenceCheckerHook.java:    LOG.debug("signature checked: " + sigs.size());
./ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java:        LOG.debug("Received null query plan.");
./ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java:        LOG.debug("Not logging events of operation type : {}", plan.getOperationName());
./ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java:            LOG.debug("Event per file enabled. New proto event file: {}", writer.getPath());
./ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java:      LOG.debug("{}.shouldReExecuteAfterCompile = {}", p, shouldReExecute);
./ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java:      LOG.debug("{}.shouldReExecute = {}", p, shouldReExecute);
./ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java:        LOG.debug("mapJoinTable is null");
./ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java:          LOG.debug("Aborting, skip dumping side-table for tag: " + tag);
./ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java:    LOG.debug("Cleaning up the operator state");
./ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java:      LOG.debug("Looking up GenericUDAF: " + functionName);
./ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java:          LOG.debug("Task: " + getId() + ", Summary: " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/mr/Throttle.java:        LOG.debug("Throttle: URL " + tracker);
./ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ObjectCache.java:      LOG.debug(key + " no longer needed");
./ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ObjectCache.java:        LOG.debug("Creating " + key);
./ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java:      LOG.debug("setting HADOOP_USER_NAME\t" + endUserName);
./ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java:        LOG.debug("Setting env: " + name + "=" + LogUtils.maskIfPassword(name, value));
./ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java:      LOG.debug("initializeOperators: " +  entry.getKey() + ", children = "  + entry.getValue().getChildOperators());
./ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java:        LOG.debug("Created slots for  " + numFiles);
./ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java:                LOG.debug("The following path has been added to the deleteDeltas list: "
./ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java:            LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java:            LOG.debug("Aborted: closing: " + outWriters[idx].toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java:            LOG.debug("Aborted: closing: " + updaters[idx].toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java:            LOG.debug("Created updater for bucket number " + bucketNum + " using file " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java:    LOG.debug("Parsed the attemptId " + attemptId.toString() + " from the task ID " + taskId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java:          LOG.debug("Adding alias " + alias + " to work list for file "
./ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java:          LOG.debug("dump " + context.op + " " + context.rowObjectInspector.getTypeName());
./ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java:        LOG.debug("Processing alias(es) " + builder.toString() + " for file " + fpath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/AppMasterEventOperator.java:      LOG.debug("AppMasterEvent: " + row);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:    LOG.debug("PLAN PATH = {}", path);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:      LOG.debug("Found plan in cache for name: {}", name);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:      LOG.debug("local path = {}", localPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:        LOG.debug("Loading plan from string: {}", planStringPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:        LOG.debug("Open file to read in plan: {}", localPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:      LOG.debug("No plan file found: {}", path, fnf);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:    LOG.debug("TaskId for {} = {}", filename, taskId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:        LOG.debug("shouldAvoidRename is false therefore moving/renaming " + tmpPathOriginal + " to " + tmpPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:          LOG.debug("Skipping rename/move files. Files to be kept are: " + filesKept.toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:          LOG.debug("CTAS/Create MV: Files being renamed:  " + filesKept.toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:          LOG.debug("Final renaming/moving. Source: " + tmpPath + " .Destination: " + specPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:              LOG.debug("Moving from {} to {} ", fileStatus.getPath(), dst);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:    LOG.debug("FS {}", fs);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:    LOG.debug("Filename: {} TaskId: {} CopySuffix: {}", filename, taskId, copyFileSuffix);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:      LOG.debug("Hive Conf not found or Session not initiated, use thread based class loader instead");
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:    LOG.debug("Session specified class loader not found, use thread based class loader");
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:            LOG.debug("Failed to close filesystem", ignore);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:              LOG.debug("Cannot get size of {}. Safely ignored.", path, e);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:            LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:          LOG.debug("Content Summary cached for {} length: {} num files: {} " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:        LOG.debug("Content Summary not cached for {}", dirPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:          LOG.debug("Adding input file {}", file);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:    LOG.debug("Create dirs {} with permission {} recursive {}",
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:      LOG.debug("HDFS dir: " + rootHDFSDirPath + " with schema " + schema + ", permission: " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java:      LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:      LOG.debug("Initialization Done: {}", this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:    LOG.debug("Initialization Done - Reset: {}", this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:    LOG.debug("Operator Initialized: {}", this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:    LOG.debug("Initializing Children: {}", this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:    LOG.debug("Initializing Child : {}", this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:        LOG.debug("allInitializedParentsAreClosed? parent.state = {}",
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:    LOG.debug("Close Called for Operator: {}", this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:      LOG.debug("Not all parent operators are closed. Not closing.");
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:        LOG.debug("Closing Child: {} ", op);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:      LOG.debug("Close Done: {}", this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:    LOG.debug("Setting traits ({}) on: {}", metaInfo, this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java:    LOG.debug("Setting stats ({}) on: {}", stats, this);
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java:        LOG.debug("keys size is " + keys.size());
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java:          LOG.debug("Key exprNodeDesc " + k.getExprString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/ExportTask.java:      LOG.debug("Exporting data to: {}", exportPaths.metadataExportRootDir());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ranger/RangerRestClientImpl.java:    LOG.debug("Url to export policies from source Ranger: {}", finalUrl);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ranger/RangerRestClientImpl.java:        LOG.debug("Response received for ranger export {} ", response);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ranger/RangerRestClientImpl.java:        LOG.debug("Ranger policy export request returned empty list");
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ranger/RangerRestClientImpl.java:      LOG.debug("Ranger policy export request returned empty list or failed, Please refer Ranger admin logs.");
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ranger/RangerRestClientImpl.java:    LOG.debug("URL to import policies on target Ranger: {}", finalUrl);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ranger/RangerRestClientImpl.java:          LOG.debug("Ranger policy import finished successfully");
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/RangerDumpTask.java:      LOG.debug("Ranger policy export filePath:" + filePath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/util/PathInfo.java:        LOG.debug("Created staging dir = " + dir + " for path = " + inputPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/table/LoadTable.java:      LOG.debug("adding dependent ReplTxnTask/CopyWork/MoveWork for table");
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/table/LoadTable.java:      LOG.debug("external table {} data location is: {}", tblDesc.getTableName(), newLocation);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/table/LoadTable.java:    LOG.debug("adding dependent CopyWork/AddPart/MoveWork for table "
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/table/LoadPartitions.java:      LOG.debug("adding dependent CopyWork for partition "
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/DatabaseEventsIterator.java:      LOG.debug("functions directory: {}", next.toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/DatabaseEventsIterator.java:    LOG.debug("processing " + previous);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/FileList.java:      LOG.debug("File list backed by {} can be used for write operation.", backingFile);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/TaskTracker.java:    LOG.debug("{} event with total / root number of tasks:{}/{}", forEventType, numberOfTasks,
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/FileListStreamer.java:          LOG.debug("Writing entry {} to file list backed by {}", nextEntry, backingFile);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/FileListStreamer.java:      LOG.debug("Closed the file list backing file: {}", backingFile);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/AtlasDumpTask.java:      LOG.debug("Finished dumping atlas metadata, total:{} bytes written", numBytesWritten);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/AtlasDumpTask.java:    LOG.debug("Current timestamp is: {}", ret);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/AtlasDumpTask.java:    LOG.debug("Stored metadata for Atlas dump at:", dumpFile.toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:    LOG.debug("prepareReturnValues : " + dumpSchema);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:      LOG.debug("    > " + s);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:            LOG.debug(te.getMessage());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:              LOG.debug(te.getMessage());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:      LOG.debug("Table list file is not created for db level replication.");
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:        LOG.debug("Dumping db: " + dbName);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:              LOG.debug("Dumping table: " + tblName + " to db root " + dbRoot.toUri());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:                LOG.debug("Adding table {} to external tables list", tblName);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:              LOG.debug(te.getMessage());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java:      LOG.debug(e.getMessage());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java:            LOG.debug(e.getMessage());
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java:        LOG.debug("error while closing reader ", e);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpWork.java:      LOG.debug("added task for {}", dirCopyWork);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpWork.java:      LOG.debug("added task for {}", managedTableCopyPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpWork.java:        LOG.debug("added task for {}", binaryCopyPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java:        LOG.debug("Current incremental dump have tables to be bootstrapped. Switching to bootstrap "
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java:        LOG.debug("Added task to set last repl id of db " + dbName + " to " + lastEventid);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/atlas/AtlasRequestBuilder.java:    LOG.debug("createRequest: {}" + request);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/atlas/AtlasRequestBuilder.java:    LOG.debug("Atlas getQualifiedName: {}", qualifiedName);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/atlas/AtlasRequestBuilder.java:    LOG.debug("Atlas metadata import request: {}" + request);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/atlas/AtlasRestClientImpl.java:    LOG.debug("exportData: {}" + request);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/atlas/AtlasRestClientImpl.java:    LOG.debug("Atlas import data request: {}" + request);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/atlas/RetryingClientTimeBased.java:        LOG.debug("Retrying method: {}", func.getClass().getName(), null);
./ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java:      LOG.debug("Added task for {}", dirCopyWork);
./ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java:      LOG.debug("No locks to release because Hive concurrency support is not enabled");
./ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java:      LOG.debug("No locks found to release");
./ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java:              LOG.debug("No files found to move from " + sourcePath + " to " + targetPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java:            LOG.debug("The statementId used when loading the dynamic partitions is " + statementId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java:      LOG.debug("The statementId used when loading the dynamic partitions is " + statementId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/DemuxOperator.java:        LOG.debug(id + " (newTag, childIndex, oldTag)=(" + tag + ", " + currentChildIndex + ", "
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicValueRegistryTez.java:        LOG.debug("No input rows from " + inputSourceName + ", filling dynamic values with nulls");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java:      LOG.debug("No local resources to process (other than hive-exec)");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java:        LOG.debug("Adding local resource: " + lr.getResource());
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java:      LOG.debug("DagInfo: " + dagInfo);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java:      LOG.debug("Setting Tez DAG access for queryId={} with viewAclString={}, modifyStr={}",
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java:            LOG.debug("Published tez counters: " + tezCounters);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java:      LOG.debug("Adding split " + path + " to src new group? " + retval);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPool.java:      LOG.debug("Closing an unneeded returned session " + session);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPool.java:          LOG.debug("Closing an unneeded session " + newSession
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java:          LOG.debug("Not adding " + serviceInstance.getWorkerIdentity() + " with hostname=" +
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java:          LOG.debug("Adding " + serviceInstance.getWorkerIdentity() + " with hostname=" +
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManagerFederation.java:      LOG.debug("Using unmanaged session - WM is not initialized");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/InterruptibleProcessing.java:        LOG.debug("Adjusting abort check rows to " + newRows
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java:        LOG.debug("Additional work {}", mergeWork.getName());
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java:    LOG.debug("Setting the LLAP fragment ID for OF to {}", attemptId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java:      LOG.debug("Starting Output: " + outputEntry.getKey());
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java:      LOG.debug("There are {} key-value readers for input {}", kvReaders.size(), inputName);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java:        LOG.debug("ProgressHelper initialized!");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java:        LOG.debug("scheduleProgressTaskService() called!");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java:        LOG.debug("shutDownProgressTaskService() called!");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java:      LOG.debug("Running task: " + getContext().getUniqueIdentifier());
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TriggerValidatorRunnable.java:              LOG.debug("Validating trigger: {} against currentCounters: {}", currentTrigger, currentCounters);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java:            LOG.debug("Marking MapWork input URI as needing credentials: " + uri);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java:      LOG.debug("Kafka credentials already added, skipping...");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java:    LOG.debug("Getting kafka credentials for brokers: {}", kafkaBrokers);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java:    LOG.debug("Jaas config for requesting kafka credentials: {}", jaasConfig);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java:    LOG.debug("Collecting file sink uris for {} topnodes: {}", baseWork.getClass(), topNodes);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java:        LOG.debug("Marking {} output URI as needing credentials (filesink): {}",
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java:      LOG.debug("TezDir path set " + tezDir + " for user: " + userName);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/LlapObjectCache.java:        LOG.debug("Found " + key + " in cache");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/LlapObjectCache.java:          LOG.debug("Found " + key + " in cache");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/LlapObjectCache.java:            LOG.debug("Found " + key + " in cache");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/LlapObjectCache.java:          LOG.debug("Caching new object for key: " + key);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/LlapObjectCache.java:      LOG.debug("Removing key: " + key);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java:              LOG.debug("Killed " + queryId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java:      LOG.debug("Processing KillQuery {} for {}",
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java:        LOG.debug("Kill query succeeded; returning to the pool: {}", ctx.session);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java:        LOG.debug("Kill query failed; restarting: {}", ctx.session);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java:        LOG.debug("Processing changes for pool " + poolName + ": " + pools.get(poolName));
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java:      LOG.debug("Will destroy {} instead of restarting", session);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ColumnarSplitSizeEstimator.java:        LOG.debug("Estimated column projection size: " + colProjSize);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ColumnarSplitSizeEstimator.java:          LOG.debug("Estimated column projection size: " + colProjSize);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java:      LOG.debug(sb.toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java:        LOG.debug("Converted partition value: " + partValue + " original (" + partValueString + ")");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java:        LOG.debug("part key expr applied: " + partValue);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java:      LOG.debug("Type of obj insp: " + inspector.getTypeName());
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java:              LOG.debug("Adding: " + value + " to list of required partitions");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/TezJobMonitor.java:              LOG.debug("Requested DAG status. checkInterval: {}. currentCounters: {}", checkInterval, currentCounters);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SessionExpirationTracker.java:      LOG.debug("Session expiration is enabled; session lifetime is "
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SessionExpirationTracker.java:            LOG.debug("Seeing if we can expire [" + nextToExpire + "]");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SessionExpirationTracker.java:          LOG.debug("[" + nextToExpire + "] is not ready to expire; adding it back");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SessionExpirationTracker.java:              LOG.debug("Waiting for ~" + timeToWaitMs + "ms to expire [" + nextToExpire + "]");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SessionExpirationTracker.java:            LOG.debug("Expiration queue is empty");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SessionExpirationTracker.java:      LOG.debug("Adding a pool session [" + this + "] to expiration queue");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java:      LOG.debug("Closed a pool session [" + this + "]");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HashTableLoader.java:          LOG.debug("Failed to get value for counter APPROXIMATE_INPUT_RECORDS", e);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java:        LOG.debug("Split: " + split + " is not a FileSplit. Using default locations");
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java:      LOG.debug(desc + " mapped to index=" + index + ", location=" + location);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java:        LOG.debug(desc + " remapped to index=" + index + ", location=" + location);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java:      LOG.debug("Path file splits map for input name: " + inputName + " is " + pathFileSplitsMap);
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java:        LOG.debug("Grouping splits. " + availableSlots + " available slots, " + waves
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java:      LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java:      LOG.debug("The destination file name for [" + localJarPath + "] is " + destFileName);
./ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java:      LOG.debug("Got stats through replication for " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java:      LOG.debug("Reading date value as days since epoch: {}", dateStr);
./ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java:      LOG.debug("Reading timestamp value as seconds since epoch: {}", timestampStr);
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplTxnTask.java:          LOG.debug("ReplTxnTask: Event is skipped as it is already replayed. Event Id: " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplTxnTask.java:            LOG.debug("ReplTxnTask: Event is skipped as it is already replayed. Event Id: " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java:        LOG.debug("This is not bucket map join, so cache");
./ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java:          LOG.debug(s + "]");
./ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java:        LOG.debug("Partition key " + current + "th :" + new BytesWritable(sorted[current]));
./ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/FlatRowContainer.java:    LOG.debug("Add is called with " + t.size() + " objects");
./ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/FlatRowContainer.java:    LOG.debug("Add is called with " + value.length + " objects");
./ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java:        LOG.debug("Probed " + i + " slots (the longest so far) to find space");
./ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java:        LOG.debug("Task getting executed using mapred tag : " + conf.get(MRJobConfig.JOB_TAGS));
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:    LOG.debug("ReplCopyTask.execute()");
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:          LOG.debug("ReplCopyTask _files contains: {}", (srcFiles == null ? "null" : srcFiles.size()));
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:          LOG.debug("ReplCopyTasks srcs= {}", (srcs == null ? "null" : srcs.length));
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:          LOG.debug("ReplCopyTask :cp:{}=>{}", oneSrc.getPath(), toPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:      LOG.debug("ReplCopyTask numFiles: {}", srcFiles.size());
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:        LOG.debug(" path " + toPath + " is cleaned before renaming");
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:    LOG.debug("ReplCopyTask filesInFileListing() reading {}", fileListing.toUri());
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:      LOG.debug("ReplCopyTask : _files does not exist");
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:        LOG.debug("ReplCopyTask :_filesReadLine: {}", line);
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:    LOG.debug("ReplCopyTask:getLoadCopyTask: {}=>{}", srcPath, dstPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:      LOG.debug("ReplCopyTask:\trcwork");
./ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java:      LOG.debug("ReplCopyTask:\tcwork");
./ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java:    LOG.debug("FetchOperator set writeIdStr: " + writeIdStr);
./ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java:        LOG.debug("Creating fetchTask with deserializer typeinfo: "
./ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java:        LOG.debug("deserializer properties:\ntable properties: " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java:        LOG.debug("No valid directories for " + currPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java:        LOG.debug("No splits for " + currPath);
./ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java:      LOG.debug("FetchOperator get writeIdStr: " + txnString);
./ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java:        LOG.debug("mapJoinTables is null");
./ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java:          LOG.debug(message);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorTopNKeyOperator.java:        LOG.debug("Closing TopNKeyFilter: {}", each);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyMultiKeyOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinStringOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterGenerateResultOperator.java:        LOG.debug("finishOuter spillCount > 0" +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterGenerateResultOperator.java:          LOG.debug("finishOuter spillCount > 0" +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterGenerateResultOperator.java:          LOG.debug("finishOuter spillCount == 0" +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterGenerateResultOperator.java:        LOG.debug("finishOuter allMatchCount > 0" +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterGenerateResultOperator.java:          LOG.debug("finishOuter noMatchCount > 0 && batch.size > 0" +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterGenerateResultOperator.java:          LOG.debug("finishOuter noMatchCount > 0 && batch.size == 0" +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterGenerateResultOperator.java:    // LOG.debug("finishOuterRepeated batch #" + batchCounter + " " + joinResult.name() + " batch.size " + batch.size + " someRowsFilteredOut " + someRowsFilteredOut);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiMultiKeyOperator.java:          // LOG.debug(CLASS_NAME + " processOp all " + displayBytes(keyBytes, 0, keyLength));
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiMultiKeyOperator.java:          // LOG.debug(CLASS_NAME + " currentKey " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiMultiKeyOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java:    // LOG.debug("VectorMapJoinFastTableContainer load newThreshold " + newThreshold);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashTable.java:            LOG.debug("Probed " + i + " slots (the longest so far) to find space");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashSet.java:        LOG.debug("Probed " + i + " slots (the longest so far) to find space");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java:        // LOG.debug("VectorMapJoinFastLongHashTable add key " + key + " slot " + slot + " pairIndex " + pairIndex + " empty slot (i = " + i + ")");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java:        // LOG.debug("VectorMapJoinFastLongHashTable add key " + key + " slot " + slot + " pairIndex " + pairIndex + " found key (i = " + i + ")");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java:        LOG.debug("Probed " + i + " slots (the longest so far) to find space");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java:    // LOG.debug("VectorMapJoinFastLongHashTable add slot " + slot + " hashCode " + Long.toHexString(hashCode));
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java:            LOG.debug("Probed " + i + " slots (the longest so far) to find space");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java:        // LOG.debug("VectorMapJoinFastLongHashTable expandAndRehash key " + tableKey + " slot " + newSlot + " newPairIndex " + newPairIndex + " empty slot (i = " + i + ")");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTableLoader.java:          LOG.debug("Failed to get value for counter APPROXIMATE_INPUT_RECORDS", e);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMultiSet.java:        LOG.debug("Probed " + i + " slots (the longest so far) to find space");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMap.java:        LOG.debug("Probed " + i + " slots (the longest so far) to find space");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastKeyStore.java:    // LOG.debug("VectorMapJoinFastKeyStore add keyLength " + keyLength + " absoluteKeyOffset " + absoluteKeyOffset + " keyRefWord " + Long.toHexString(keyRefWord));
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastKeyStore.java:    // LOG.debug("VectorMapJoinFastKeyStore equalKey match on bytes");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java:      LOG.debug(getLoggingPrefix() + " outputColumnNames " + outputColumnNames);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java:        LOG.debug(getLoggingPrefix() + " addProjectionColumn " + i + " columnName " + columnName +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java:      LOG.debug(getLoggingPrefix() + " VectorMapJoinCommonOperator initializeOp currentScratchColumns " + Arrays.toString(currentScratchColumns));
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java:        LOG.debug(getLoggingPrefix() + " VectorMapJoinCommonOperator initializeOp " + i + " field " + field.getFieldName() + " type " + field.getFieldObjectInspector().getTypeName());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java:        LOG.debug(getLoggingPrefix() + " VectorMapJoinCommonOperator initializeOp overflowBatch outputColumn " + outputColumn + " class " + overflowBatch.cols[outputColumn].getClass().getSimpleName());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java:    LOG.debug(getLoggingPrefix() + " VectorMapJoinCommonOperator commonSetup " + batchName + " column count " + batch.numCols);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java:      LOG.debug(getLoggingPrefix() + " VectorMapJoinCommonOperator commonSetup " + batchName + "     column " + column + " type " + (batch.cols[column] == null ? "NULL" : batch.cols[column].getClass().getSimpleName()));
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerStringOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java:      LOG.debug(CLASS_NAME + " reloadHashTable!");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java:      LOG.debug(CLASS_NAME + " reProcessBigTable enter...");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java:      LOG.debug(CLASS_NAME + " reProcessBigTable exit! " + rowCount + " row processed and " + batchCount + " batches processed");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java:      LOG.debug("VectorMapJoinInnerLongOperator closeOp " + batchCounter + " batches processed");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyGenerateResultOperator.java:    // LOG.debug("generateHashMultiSetResultSingleValue enter...");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyGenerateResultOperator.java:    // LOG.debug("generateHashMultiSetResultSingleValue with big table...");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyGenerateResultOperator.java:    // LOG.debug("generateHashMultiSetResultMultiValue allMatchesIndex " + allMatchesIndex + " duplicateCount " + duplicateCount + " count " + count);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerMultiKeyOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiStringOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java:    // LOG.debug("VectorMapJoinOptimizedLongCommon serialize key " + key + " hashTableKeyType " + hashTableKeyType.name() + " hex " + Hex.encodeHexString(bytes));
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinLeftSemiLongOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java:          // LOG.debug(CLASS_NAME + " processOp all " + displayBytes(keyBytes, 0, keyLength));
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java:          // LOG.debug(CLASS_NAME + " currentKey " +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyStringOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyStringOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyLongOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " repeated joinResult " + joinResult.name());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerLongOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " non-repeated");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerLongOperator.java:          LOG.debug(CLASS_NAME +
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/reducesink/VectorReduceSinkUniformHashOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " empty");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/reducesink/VectorReduceSinkObjectHashOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " empty");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/reducesink/VectorReduceSinkEmptyKeyOperator.java:          LOG.debug(CLASS_NAME + " batch #" + batchCounter + " empty");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java:      LOG.debug("getVectorExpressionClass udf " + udf.getSimpleName() + " descriptor: " + descriptor.toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java:          LOG.debug("getVectorExpressionClass doesn't match " + ve.getSimpleName() + " " + ve.newInstance().getDescriptor().toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/rowbytescontainer/VectorRowBytesContainer.java:    LOG.debug("BytesContainer created temp file " + tmpFile.getAbsolutePath());
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java:              LOG.debug("We will try to use the VectorUDFAdaptor for " + exprDesc.toString()
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java:      LOG.debug("Input Expression = " + exprDesc.toString()
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java:        LOG.debug("No vector udf found for "+udfClass.getSimpleName() + ", descriptor: "+descriptor);
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java:        LOG.debug("Casting constant scalar " + scalar + " to HiveDecimal resulted in null");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java:      LOG.debug("using hash aggregation processing mode");
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java:            LOG.debug(String.format("Flush did not progress: %d entries before, %d entries after",
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java:        LOG.debug("GBY memory limits - isLlap: {} maxMemory: {} ({} * {}) fixSize:{} (key:{} agg:{})",
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java:      LOG.debug("totalAccessCount:{}, numEntries:{}, avgAccess:{}",
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java:        LOG.debug(String.format(
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java:        LOG.debug(String.format("GC canary caused %d flushes", gcCanaryFlushes));
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java:          LOG.debug(String.format("checkHashModeEfficiency: HT:%d RC:%d MIN:%d",
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java:            LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorLimitOperator.java:      LOG.debug("Limit reached: currCount: {}, currentCountForAllTasks: {}", currCount,
./ql/src/java/org/apache/hadoop/hive/ql/exec/TopNKeyOperator.java:        LOG.debug("Closing TopNKeyFilter: {}", each);
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java:          LOG.debug("Shutting down the SparkContext");
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkDynamicPartitionPruner.java:        LOG.debug("Converted partition value: " + partValue + " original (" + partValueString + ")");
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkDynamicPartitionPruner.java:        LOG.debug("part key expr applied: " + partValue);
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkDynamicPartitionPruner.java:        LOG.debug("Type of obj insp: " + inspector.getTypeName());
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java:      LOG.debug("Starting Spark job with job handle id " + sparkJobHandleId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java:        LOG.debug("Failed to submit Spark job with job handle id " + sparkJobHandleId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java:    LOG.debug("Killing Spark job with job handle id " + sparkJobHandleId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SmallTableCache.java:            LOG.debug("Cleaned up small table cache for query {}", queryId);
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java:        LOG.debug("Logging job configuration: ");
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java:        LOG.debug(outWriter.toString());
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlan.java:        LOG.debug(explainOutput);
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveKVResultCache.java:          LOG.debug("Retry creating tmp result-cache directory...");
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java:            LOG.debug(String.format(
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java:          LOG.debug("Failed to close inputstream.", e);
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java:        LOG.debug(String.format(
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java:        LOG.debug(String.format(
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java:        LOG.debug(String.format(
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java:        LOG.debug(String.format(
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java:        LOG.debug(String.format("load RPC property from hive configuration (%s -> %s).", propertyName,
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/session/SparkSessionManagerImpl.java:      LOG.debug(String.format("New session (%s) is created.", sparkSession.getSessionId()));
./ql/src/java/org/apache/hadoop/hive/ql/exec/spark/session/SparkSessionManagerImpl.java:      LOG.debug(String.format("Closing Spark session (%s).", sparkSession.getSessionId()));
./ql/src/java/org/apache/hadoop/hive/ql/log/syslog/SyslogInputFormat.java:      LOG.debug("Returning record reader for path {}", finalPath);
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java:      LOG.debug(functionName);
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java:      LOG.debug("Count of True Values: {}", myagg.countTrues);
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java:      LOG.debug("Count of False Values: {}", myagg.countFalses);
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java:      LOG.debug("Count of Null Values: {}", myagg.countNulls);
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits2.java:    LOG.debug("initializing GenericUDFGetSplits2");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits2.java:    LOG.debug("done initializing GenericUDFGetSplits2");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSQLSchema.java:    LOG.debug("Getting schema for Query: {}", query);
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSQLSchema.java:    LOG.debug("initializing GenericUDTFGetSQLSchema");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSQLSchema.java:    LOG.debug("Initialized conf, jc and metastore connection");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSQLSchema.java:    LOG.debug("done initializing GenericUDTFGetSQLSchema");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/BaseMaskUDF.java:    LOG.debug("==> BaseMaskUDF.initialize()");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/BaseMaskUDF.java:    LOG.debug("<== BaseMaskUDF.initialize()");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java:    LOG.debug("initializing GenericUDFGetSplits");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java:    LOG.debug("done initializing GenericUDFGetSplits");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java:    LOG.debug("Initialized conf, jc and metastore connection");
./ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java:        LOG.debug("NumEvents=" + eventList.size() + ", NumSplits=" + result.length);
./ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java:    LOG.debug("Encoding valid txn write ids info {} txnid: {}", txnWriteIds.toString(),
./ql/src/java/org/apache/hadoop/hive/ql/TaskQueue.java:    LOG.debug("Shutting down query " + ctx.getCmd());
./ql/src/java/org/apache/hadoop/hive/ql/TaskQueue.java:        LOG.debug("There is no correspoing statTask for: " + statKey);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:        LOG.debug("Creating new db. db = " + db + ", needsRefresh = " + needsRefresh +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Closing current thread's connection to Hive Metastore.");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:            LOG.debug("Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:            LOG.debug("Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:          LOG.debug("Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:          LOG.debug("Unable to determine if Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:          LOG.debug("Materialized view " + materializedViewTable.getFullyQualifiedName() +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:        LOG.debug("No new files were created, and is not a replace, or we're inserting into a "
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Adding new partition " + newTPart.getSpec());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Caught AlreadyExistsException, trying to alter partition instead");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:        LOG.debug(debugMsg.toString());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Caught AlreadyExistsException, trying to add partitions one by one.");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("No files added by this query in: " + acidDir);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:    LOG.debug("Listing files under " + acidDir);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:    LOG.debug("Altering existing partition " + newTPart.getSpec());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug(sb.toString());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Number of partitionsToAdd to be added is " + futures.size());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Cancelling " + futures.size() + " dynamic loading tasks");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Clear table column statistics and set basic statistics to false for " + tbl.getCompleteName());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:          LOG.debug("creating partition for table " + tbl.getTableName()
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:            LOG.debug("Caught already exists exception, trying to alter partition instead");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:              LOG.debug("Caught JDO exception, trying to alter partition instead");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:    LOG.debug("altering partition for table " + tbl.getTableName() + " with partition spec : "
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("write notification log is ignored as dml event logging is disabled");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("write notification log is ignored as " + tbl.getTableName() + " is temporary : " + writeId);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("write notification log is ignored as file list is empty");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:    LOG.debug("adding write notification log for operation " + writeId + " table " + tbl.getCompleteName() +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Firing dml insert event");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:        LOG.debug("Not firing dml insert event as " + tbl.getTableName() + " is temporary");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:          LOG.debug("Moved src: {}, to dest: {}", pair.getLeft().toString(), pair.getRight().toString());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("The source path is null for isSubDir method.");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:    LOG.debug("The source path is " + fullF1 + " and the destination path is " + fullF2);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("The source file is in the local while the dest not.");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("The source path's schema is " + schemaSrcf +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:    LOG.debug("The source path is " + fullF1 + " and the destination path is " + fullF2);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:            LOG.debug("The path " + destf.toString() + " is deleted");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:          LOG.debug("Copying source " + srcf + " to " + destf + " because HDFS encryption zones are different.");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Acid move Looking for original buckets in " + srcPath);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug("Acid move found " + origBucketStats.length + " original buckets");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:    LOG.debug("Acid move looking for " + deltaFileType + " files in bucket " + origBucketPath);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:    LOG.debug("Acid move found " + deltaStats.length + " " + deltaFileType + " files");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:        LOG.debug("Acid move found " + bucketStats.length + " bucket files");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug(StringUtils.stringifyException(e));
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug(StringUtils.stringifyException(e));
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug(StringUtils.stringifyException(e));
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug(StringUtils.stringifyException(e));
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:      LOG.debug(StringUtils.stringifyException(e));
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java:        LOG.debug(StringUtils.stringifyException(e));
./ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java:      LOG.debug("Materialized view " + materializedViewTable.getCompleteName() +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java:      LOG.debug("Created materialized view for rewriting: " + materializedViewTable.getFullyQualifiedName());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java:      LOG.debug("Materialized view " + materializedViewTable.getCompleteName() +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java:      LOG.debug("Materialized view refreshed: " + materializedViewTable.getFullyQualifiedName());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java:    LOG.debug("sd is " + tPartition.getSd().getClass().getName());
./ql/src/java/org/apache/hadoop/hive/ql/metadata/MaterializedViewsCache.java:    LOG.debug("Materialized view {}.{} added to registry",
./ql/src/java/org/apache/hadoop/hive/ql/metadata/MaterializedViewsCache.java:    LOG.debug("Refreshed materialized view {}.{} -> {}.{}",
./ql/src/java/org/apache/hadoop/hive/ql/metadata/MaterializedViewsCache.java:    LOG.debug("Materialized view {}.{} removed from registry",
./ql/src/java/org/apache/hadoop/hive/ql/metadata/MaterializedViewsCache.java:      LOG.debug("Materialized view {}.{} removed from registry", dbName, tableName);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/MaterializedViewsCache.java:      LOG.debug("Found materialized view {}.{} in registry", dbName, viewName);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/MaterializedViewsCache.java:    LOG.debug("Materialized view {}.{} not found in registry", dbName, viewName);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/MaterializedViewsCache.java:      LOG.debug("No materialized view with similar query text found in registry.");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/MaterializedViewsCache.java:    LOG.debug("{} materialized view(s) found with similar query text found in registry",
./ql/src/java/org/apache/hadoop/hive/ql/metadata/events/NotificationEventPoll.java:      LOG.debug("Non-positive poll interval configured, notification event polling disabled");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/events/NotificationEventPoll.java:      LOG.debug("No event consumers configured, notification event polling disabled");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/events/NotificationEventPoll.java:      LOG.debug("Polling for notification events");
./ql/src/java/org/apache/hadoop/hive/ql/metadata/events/NotificationEventPoll.java:          LOG.debug("Event: " + event);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/events/NotificationEventPoll.java:      LOG.debug("Processed {} notification events", eventsProcessed);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:      LOG.debug("No current SessionState, skipping temp tables for " + msg);
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:      LOG.debug("No current SessionState, skipping temp tables for " +
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java:        LOG.debug(
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:        LOG.debug(msg);
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:      LOG.debug("Stats collection waiting for threadpool to shutdown..");
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:      LOG.debug("Stats collection threadpool shutdown successful.");
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:          LOG.debug("Stats requested to be reliable. Empty stats found: {}", statsCollection.partish.getSimpleName());
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:    LOG.debug("Collectors.size(): {}", collectorsByTable.keySet());
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:    LOG.debug("Updating stats for: {}", tableFullName);
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:        LOG.debug("Updated stats for {}.", tableFullName);
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:          LOG.debug("Bulk updated {} partitions of {}.", results.size(), tableFullName);
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:    LOG.debug("Updated stats for: {}", tableFullName);
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:        LOG.debug("Waiting for all stats tasks to finish...");
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java:        LOG.debug("Stats collection thread pool did not terminate");
./ql/src/java/org/apache/hadoop/hive/ql/stats/OperatorStatsReaderHook.java:        LOG.debug("Reading runtime statistics for tez vertex task: {}", vertexName);
./ql/src/java/org/apache/hadoop/hive/ql/stats/OperatorStatsReaderHook.java:              LOG.debug("Unable to get statistics for vertex: {} opId: {} groupName: {}", vertexName, operatorId,
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStats.java:          LOG.debug("Estimated average row size: " + avgRowSize);
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java:          LOG.debug("Cancelling " + futures.size() + " file stats lookup tasks");
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java:          LOG.debug("Finished getting file stats of all partitions!");
./ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java:    LOG.debug("Getting file stats of all partitions. threadpool size:" + poolSize);
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:            LOG.debug("Column stats requested for : {} columns. Able to retrieve for {} columns",
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:            LOG.debug("Column stats requested for : {} partitions. Able to retrieve for {} partitions",
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:        LOG.debug("Stats for column {} in table {} could not be retrieved from cache", colName,
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:        LOG.debug("Stats for column {} in table {} retrieved from cache", colName, table.getCompleteName());
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:            LOG.debug("Partition path : " + path);
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:      LOG.debug("Materialized table does not contain table statistics");
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:            LOG.debug("Stats for column " + cs.getColumnName() +
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:      LOG.debug("STATS-" + op.toString() + ": Overflow in number of rows. "
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java:      LOG.debug("STATS-" + op.toString() + ": Equals 0 in number of rows. "
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:    LOG.debug("Processing {}", fullTableNames);
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:    LOG.debug("Processing table {}", table);
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:      LOG.debug("Skipping table {} since it is being replicated into", table);
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:      LOG.debug("Columns to update are {}; existing only: {}, out of: {} based on {}",
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:      LOG.debug("Columns to update are {} for all partitions; {} individual partitions."
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:        LOG.debug("Adding analyze work for {}", e.getKey());
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:      LOG.debug("Processing partition ({} in batch), {}", currentIxInBatch, partName);
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:      LOG.debug("Updating {} based on {} and {}", colsToUpdate, colsToMaybeUpdate, params);
./ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java:      LOG.debug("Running {} based on {}", cmd, req);
./ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java:        LOG.debug("Initing FSStatsPublisher with : " + statsDir);
./ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java:    LOG.debug("Connecting to : " + statsDir);
./ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java:      LOG.debug("Created file : " + statsFile);
./ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java:      LOG.debug("Writing stats in it : " + statsMap);
./ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsAggregator.java:    LOG.debug("About to delete stats tmp dir :" + statsDir);
./ql/src/java/org/apache/hadoop/hive/ql/stats/ColStatsProcessor.java:            LOG.debug("Because {} is infinite or NaN, we skip stats.", columnName, e);
./ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:      LOG.debug("SessionState user: " + userName);
./ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:      LOG.debug("Updating thread name to {}", newThreadName);
./ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:      LOG.debug("Resetting thread name to {}", names[names.length - 1]);
./ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:      LOG.debug("Session is using authorization class " + authorizationClass.getClass());
./ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:    LOG.debug("Removing resource dir " + resourceDir);
./ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:        LOG.debug("Cleared Hadoop ReflectionUtils CONSTRUCTOR_CACHE");
./serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java:    LOG.debug(serdeConstants.LIST_COLUMN_TYPES + ": " + columnTypeProperty);
./serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java:    LOG.debug(format("The Null Bytes for each record will have %s bytes", byteNumForNullArray));
./serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonWriter.java:    LOG.debug("Create JSON tree from Object tree: {}", rootNode);
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java:      LOG.debug("Resetting already initialized AvroSerDe");
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java:    LOG.debug("AvroSerde::initialize(): Preset value of avro.schema.literal == "
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java:      LOG.debug("Avro schema is " + schema);
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java:      LOG.debug("Configuration null, not inserting schema");
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java:        LOG.debug("columnComments is " + columnCommentProperty);
./serde/src/java/org/apache/hadoop/hive/serde2/avro/InstanceCache.java:    if(LOG.isDebugEnabled()) LOG.debug("Checking for hv: " + hv.toString());
./serde/src/java/org/apache/hadoop/hive/serde2/avro/InstanceCache.java:      if(LOG.isDebugEnabled()) LOG.debug("Returning cache result.");
./serde/src/java/org/apache/hadoop/hive/serde2/avro/InstanceCache.java:      if(LOG.isDebugEnabled()) LOG.debug("Creating new instance and storing in cache");
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java:        LOG.debug("Adding new valid RRID :" +  recordReaderId);
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroLazyObjectInspector.java:      LOG.debug("Getting struct field data for field: [" + f.getFieldName() + "] on data ["
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroLazyObjectInspector.java:          LOG.debug("Deserializing struct [" + rowField.getClass() + "]");
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroLazyObjectInspector.java:          LOG.debug("Returning a lazy map for field [" + f.getFieldName() + "]");
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroLazyObjectInspector.java:          LOG.debug("Returning [" + rowField + "] for field [" + f.getFieldName() + "]");
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroLazyObjectInspector.java:        LOG.debug("Retrieved writer Schema: " + ws.toString());
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroLazyObjectInspector.java:        LOG.debug("Retrieved reader Schema: " + rs.toString());
./serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java:        LOG.debug(msg, ioe);
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFloat.java:      LOG.debug("Data not in the Float data type range so converted to null. Given data is :"
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFloat.java:      LOG.debug("Data not in the Float data type range so converted to null.", e);
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyPrimitive.java:        LOG.debug("Data not in the " + dataType
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyPrimitive.java:      LOG.debug("Data not in the " + dataType + " data type range so converted to null.", e1);
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyDouble.java:      LOG.debug("Data not in the Double data type range so converted to null. Given data is :"
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyDouble.java:      LOG.debug("Data not in the Double data type range so converted to null.", e);
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySerDeParameters.java:    LOG.debug(serdeName + " initialized with: columnNames="
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java:              LOG.debug("Data not in the HiveDecimal data type range so converted to null. Given data is :"
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java:        LOG.debug("Data not in the " + dataType
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java:      LOG.debug("Data not in the " + dataType + " data type range so converted to null.", e1);
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyBinary.java:      LOG.debug("Data does not contain only Base64 characters so return original byte array", e);
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveVarchar.java:        LOG.debug("Data not in the HiveVarchar data type range so converted to null.", e);
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java:      LOG.debug("Data not in the HiveDecimal data type range so converted to null. Given data is :"
./serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveChar.java:        LOG.debug("Data not in the HiveChar data type range so converted to null.", e);
./serde/src/java/org/apache/hadoop/hive/serde2/MetadataTypedColumnsetSerDe.java:    LOG.debug(getClass().getName() + ": initialized with columnNames: "
./testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/context/CloudComputeService.java:        LOG.debug("Found node: " + node + ", Result: " + result);
./metastore/src/java/org/apache/hadoop/hive/metastore/HiveClientCache.java:      LOG.debug("Using a version of guava <12.0. Stats collection is enabled by default.");
./metastore/src/java/org/apache/hadoop/hive/metastore/HiveClientCache.java:              LOG.debug("Evicting client: " + Integer.toHexString(System.identityHashCode(hiveMetaStoreClient)));
./metastore/src/java/org/apache/hadoop/hive/metastore/HiveClientCache.java:        LOG.debug("Cleaning up hive client cache in ShutDown hook");
./metastore/src/java/org/apache/hadoop/hive/metastore/HiveProtoEventsCleanerTask.java:    LOG.debug("Trying to delete expired proto events from " + eventsBasePath);
