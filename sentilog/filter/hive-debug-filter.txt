get valid challenge for host
no auth cookie find for uri exist cookies authenticating
the user credential
find auth cookie find for uri cookie
prepare for retry boolean result object
unauthorizedresponsehandler get response status
send request query for segment
run with data schema
pre create data source with name
no kafka supervisor find for datasource
check load status from coordinator
check segment response
commit insert into table overwrite
set enable split generation on hs
supply sql connector with db type uri user
request receive redirect response location
retrieve data from druid location use query
add table
mutation written value written
add hadoop token for accumulo job credentials
add hadoop token for accumulo job credential
all job tokens
search tab column range
use column family
find index match
find index match
index lookup fail for table
consumer poll timeout
optimize scan
no optimization thus use full scan
consumer poll timeout
list stats column before analyze part
list stats column after analyze part
list stats column before analyze part
list stats column after analyze part
rowidx
executing
executing
get privilege object
get privilege object
verifier function
verifier function
return list size bitvalue
return list size bitvalue
return list size bitvalue
return list size bitvalue
return list size bitvalue
return list size
return list size bitvalue
return list size bitvalue
return list size bitvalue
return list size bitvalue
return list size bitvalue
catalog do not exist create new one
catalog create
will initialize metastore database in class rule
will destroy metastore database in class rule if not derby
will initialize metastore database in test rule
will destroy metastore database in test rule if not derby
execute
look for token with service
token service
create chunk input for
close chunk input
chunk size
hit end data
create chunk input stream
close underlie output stream
write chunk size
get host proxy for
create client without token for
create client for the token
transfer messages
error while connect port retrying
error while connect port retry
transfer outbound client messages
transfer outbound server messages
remote spark driver configure with
send error client
send error client
send job result client
send job submit client
send task metric client
shut down due endsession request
receive client job request
client job spark job finished
discard fail rpc
handle sasl challenge message
send sasl challenge response
sasl negotiation finish with qop
interrupt before driver thread be finished
send jobrequest
receive result for client job
generate columns
generate column types
hbaseserde initialize with
probably do not support table snapshots return null instance
ignore residual predicate
use tablesnapshotinputformat
use hivehbasetableinputformat
configure jobconf for table
check node with data
find slowest node with data
try blacklist node
can not blacklist node
get result for lock check
skip check current time be wait for
no op launch for container
no op stopcontainer invocation for containerid
try get kerberosinfo for
try get tokeninfo for
gettotalresources numinstancesfound totalmem totalvcores
getavailableresources numinstancesfound totalmem totalvcores
process deallocatetask for task tasksucceeded endreason
ignore deallocatecontainer for containerid
selectinghost for task on hosts
shoulddelayforlocality for task on hosts
host will not become available within request timeout
delay local allocation for
skip local allocation for
no locality requested select random host for task
request node in consistent order do not exist fall back random selection for
assign in consistent order when look for first request host from hosts
node
ignore disablenode invocation for null nodeinfo
schedulerun
scheduleresult for task
attempt preempt on request host for task potentialhosts
attempt preempt for on potential hosts totalpendingpreemptions
not preempt for on potential hosts an exist preemption request exist
attempt preempt on any host for task pendingpreemptions
no task qualify killable schedule task at priority current priority
receive heartbeat from
register for node
register for node
add path host instance cache
remove path host from cache
return host for locality allocation on
error deserializing row from data
interrupt reader thread due reader event with error
receive terminate response for
receive heartbeat from container request
heartbeat from
task complete event for
task fail event for
task update event for
node heartbeat from
can not get the current configuration lock time
can not lock window current value
can not lock window current value change
set attempt number to task number to from give taskattemptid in conf
execute session query
close connection for handle id
no connection find for handle id
close all handle
send queue event record reader
no register record reader queue event
hive conf
error close metastore client ignore the error
use version guava stats collection enable by default
evict client
clean hive client cache in shutdown hook
get ask for oi for
get ask for oi for
movetaskoutputs
test if move file
move file
move directory
finalpath file
can not suppress data schema partition schema table schema seem differ
partition storagehandler
partition inputformat
partition outputformat
partition serde
getstatus
getstatus
listdatabases
listdatabases
http status code
response
get token for
start new job request with time out
complete job request
user authorize do doas any user
user
user
user authorize do doas from any host
user
user
authorization check proxyuser host doas
libdir
load conf
xsrf filter enable
receive callback
resolve remote hostname
queue job in
launch job
prepare submit job
add delegation token for jobid
create hive metastore delegation token for user
create hiveserver delegation token for user
do not exist
cancel token for jobid status from jt
could not de serialize from
could not de serialize partition from
file
cpr respcode cpr errmsg for table
dump row via sql from
class
file
cpr respcode cpr errmsg
dump row via sql from
dump raw file
executing
executing
file
convertpigschematohcatschema pigschema tblschema
convertpigschematohcatschema computed
hcatfieldschema null for fschema
outputschema
set
go execute query
go execute query
update row for sequnce
lock derby table
go execute query
go execute query
update row for notification sequence table
dbnotificationlistener add write notification log for
go execute query
dbnotificationlistener add notification log for
go execute insert with parameter
dbnotificationlistener process
cleaner thread run
cleaner thread do
sleep ms
clidriver inited with classpath java class path
parse command
parse complete
parse hint
parse complete
parse command
parse complete
parse expression
parse complete
run hive query
create partition for table
write notification log ignore dml event log disabled
txnid writeid
txnid writeid
override hiveconf set
override hiveconf set
override hiveconf set
orc memory pressure notified usedmemory maxmemory
create new filesystem instance
flush record updater for partitions
close updater for partitions
low memory canary set ingestion size buffered threshold exceeded
record updaters partitions buffered records total records
fetch delegation token from session
server in zookeeper after remove rejected
configuration apply jdbc connection params
resolve authority
resolve authority
progress bar complete
operation log show the user
hivequeryresultsfetchreq
create new range last range which can include some previous adds be
error while retrieve literallist return null
disable ssl protocol
skip child access check since the directory already remove
copy srcpaths destpath with doas
delete
purge set true not move trash
invalid string for timestamp with time zone
attempt write output directory
attempt create output directory
create flat output file
attempt create output directory
create output file
set job conf credstore location
use the default value pas in for log id
find metastore uri
valid non mapreduce execution engines
perflog method from
could not format millis
could not parse timestamp text
version
logout
list bean for
get attribute unsupported
get attribute throw an exception
request logger not found add new logger with level
update logger level
the detail are
exception while inherit permission
return value
the detail are
filter found use as is
unable use get exception use internal shim impl filter
look for database
find database process
will wait for
require compaction succeeded
still wait on
still work on
find table in
find in
process table
write compaction command
set enable split generation on hs
not import credential for service
import credential for service
fail get sql key word from database metadata due the exception
go run command
method do do not match pas
unable shutdown metastore client will try close transport directly
get back event
column empty or partnames empty short circuit stats eval on client side
column empty or partnames empty short circuit stats eval on client side
ignore sql error
successfully truncate table
not truncate because do not exist
test filter
misbehave
numdistinctvalueestimator
number vectors
vector size
serialize vectors
perflog method from
create hikari connection pool for the metastore
create dbcp connection pool for the metastore
fail get object from metastore
access metastore fail due invalid input
object not find in metastore
do not find object in metastore
could not find db entry
receive follow output from sqlline
utf
go invoke file contains
script
go run
external table property execute sql
external table purge execute sql
tabletype external table execute sql
catalog name execute sql
go run
find oracle hack our way through rather than use sqlline
fail determine schema version from hive metastore db
validate table in the schema for version
schema not support
find table in hm dbstore
parse schema script
parse subscript
schema tables
db tables
schema subscript find
find table in the schema
go run
go run
go run
go run
go run
go run
go execute query
go execute query
find potential compaction
go execute query
no compaction find ready compact
go execute update
another worker pick
go rollback
go execute update
go rollback
go commit
go rollback
go execute query
find ready clean
go rollback
run markcleaned with compactioninfo
go execute update for cq id
go execute update
go rollback
go execute update
remove record from completed txn components
go execute update
remove record from txn components
go commit
go rollback
go execute query
go execute delete
go commit
go rollback
go execute query
go execute update
remove empty abort commit transaction from txns
go commit
go rollback
go execute update
set compaction queue entry state for host
go commit
go rollback
go execute update
go commit
go rollback
find column with statistic info for compactioninfo
go execute
find column update stats on
about execute
about execute
go execute update
remove record from completed compactions
go rollback
mark failed compactioninfo
go execute update
the failure occur before even make an entry in compaction queue generate id so
insert entry into completed compactions
go commit
go rollback
go execute with jobid cq id
go commit
go rollback
total time acidhousekeeperservice took seconds
take seconds
add query batch
go execute query in oracle anonymous statement
go execute query in oracle anonymous statement
add query batch
go execute query in batch batch size
go execute query in batch batch size
txn cleaner service took seconds
catch sql exception in min history level check
go execute query
open transaction add for miss value in txns
get opentxnlist with hwm opentxnlist size
go commit
go rollback
go execute insert
go execute update
go rollback
go execute query
go execute query
go execute select
targettxnid for srctxnid
go rollback
go commit
go rollback
go execute query
go commit
go rollback
go execute query
update repl id for db
go execute query
update repl id for table
go execute query with partition
update repl id for part
execute batch queries batch size
execute batch queries batch size
go commit
go rollback
go execute query
go execute query
go execute insert
go execute update
go execute query
go execute insert
go commit
go rollback
go execute query
go rollback
go rollback
go execute query
go execute query
go execute query
go execute query
go execute query
go execute insert
go execute update
go execute delete
execute batch queries
execute batch queries
go execute query
go execute insert
go commit
go commit
go rollback
minopentxnidwatermark calculate with minopentxn lowwatermark
minimum open write id do not match for table
go execute query
acquire lock for materialization rebuild with for
go execute query
go execute update
go commit
go execute update
go rollback
go commit
go rollback
go execute query
go execute update
go commit
go rollback
go retry enqueuelock for request after catch retryexception with message
go execute query
go execute update in batch
execute batch queries batch size
execute batch queries batch size
go execute query
execute batch queries batch size
execute batch queries batch size
go retry checklock for extlockid    txnid after catch retryexception with message
go retry checklock for request after catch retryexception with message
go execute update
successfully unlock at least lock with extlockid
go execute query
go rollback
go rollback
go execute update
go rollback
go execute query
go execute update
go execute query
go rollback
go execute query
go execute update
go commit
go rollback
go execute query
go rollback
go rollback
execute batch queries batch size
execute batch queries batch size
go commit
go rollback
skip cleanup because db belong catalog
skip cleanup because table belong catalog
skip cleanup because partition belong catalog
go execute update
go commit
go rollback
go execute update
go commit
go rollback
go execute query
go execute query
go execute query
failure acquire lock intlockid block by
go execute query
go execute query
successfully acquire locks
go execute update
go execute query
go execute update
successfully heartbeated for extlockid
go execute update
successfully heartbeated for txnid
go execute query
go execute query
go execute query
go execute query
go execute query
go execute query
go rollback
go rollback
go execute query for extlockid
gettxnidfromlockid return
go execute query for extlockid
find lock for extlockid locks
go execute query
do not find any timed out locks therefore retuning
go execute update
go execute query
go execute query
go rollback
go execute insert
remove commit transaction txnid from min history level
about execute sql
lock by
txn lock lock by in mode
unlock by
update cache use notification event start from event id
no event process
num event process
event process
process database table cache table so far
process database cache database so far
cachedstore update cache objects share cache have be update time so far
cachedstore update cache objects share cache update count is
cachedstore update cache database object for catalog
cachedstore update cache database object for catalog
cachedstore update cache table object for catalog database
cachedstore update cache table object for catalog database
unable refresh cache table for database
cachedstore update cache table col stats object for catalog database
cachedstore update cache table col stats object for catalog database
cachedstore update cache foreign key object for catalog database table
cachedstore update cache foreign key object for catalog database table
cachedstore update cache not null constraint for catalog database table
cachedstore update cache not null constraint for catalog database table
cachedstore update cache unique constraint for catalog database table
cachedstore update cache unique constraint for catalog database table
cachedstore update cache primary key object for catalog database table
cachedstore update cache primary key object for catalog database table
cachedstore update cache default constraint object for catalog database table
cachedstore update cache default constraint object for catalog database table
cachedstore update cache check constraint object for catalog database table
cachedstore update cache check constraint object for catalog database table
cachedstore update cache partition object for catalog database table
cachedstore update cache partition object for catalog database table
cachedstore update cache partition col stats object for catalog database table
cachedstore update cache partition col stats object for catalog database table
cachedstore update cache aggregate partition col stats object for catalog
do not find aggr stats in cache merge them tblname parts cols
stats not find in cachedstore for dbname tblname partname colname
no stats data find for dbname tblname partnames colnames
try match against blacklist pattern
find matcher group at start index end index
try match against whitelist pattern
find matcher group at start index end index
in blacklist skip
not in whitelist skip
eviction happen for table
current table cache contain entry
partition not present in the cache
constraint do not exist in cache
skip primary key cache update for table
primary key refresh in cache be successful for
skip foreign key cache update for table
foreign key refresh in cache be successful for
skip not null constraint cache update for table
not null constraint refresh in cache be successful for
skip unique constraint cache update for table
unique constraint refresh in cache be successful for
skip default constraint cache update for table
default constraint refresh in cache be successful for
skip check constraint cache update for table
check constraint refresh in cache be successful for
skip partition cache update for table
skip table col stats cache update for table
write id list not compatible with write id
the current cache store transactional partition column statistic for
skip partition column stats cache update for table
skip partition column stats cache update for table
unable cache partition column stats for table
skip aggregate stats cache update for table
skip aggregate stats cache update for table
skip database cache update the database list have dirty
unable cache partition column stats for table
skip table cache update the table list have dirty
find be different old val new val
find mask property be different
close persistencemanagerfactory
persistencemanagerfactory close
remove cache classloaders from datanucleus nucleuscontext
override value from jpox properties with
override value
interrupt while sleep before retrying
non retriable exception
role already exist
role already exist
fail while grant global privs admin
already in admin role
source
create database path
id should not be set but table have the id set id ignored
get tables ext gettables return
get tables ext gettableobjectsbyname return
table require
some the partition miss stats
all the column stats
all the column stats be not accurate merge
threadpool initialize
use direct sql underlie db
getdatabase directsql return db
column empty or partnames empty short circuit stats eval
usedensityfunctionforndvestimation
getdefaultconstraints directsql
getcheckconstraints directsql
run
run
add server definition for plain sasl with authentication
hive home not set use current directory instead
partition expr
match partition
initialize objectstore
objectstore initialize call
create catalog
fetch catalog
fetch all catalog name
drop catalog
fail get database return nosuchobjectexception
type not find
gettablemeta with filter params
execute getmtable for
execute getpartitions
execute getpartitionlocations
do execute query for getpartitionlocations
execute getpartitionnames
execute getpartitionnamesbyfilter
filter specify jdoql filter
parms
do execute query for getpartitionnamesbyfilter
do retrieve all object for getpartitionnamesbyfilter size
execute listpartitionnamespswithauth
execute listpartitionnamesps
execute listmpartitions
do execute query for listmpartitions
do retrieve all object for listmpartitions
execute listmpartitionswithprojection
do retrieve object for listmpartitionswithprojection
do execute query for getpartitionsviaormfilter
do retrieve all object for getpartitionsviaormfilter
delete partition from store
jdoql filter
use direct sql optimization
not use direct sql optimization
full directsql callstack for debug not an error
retrieve use in ms
jdo filter pushdown cannot be used
jdofilter
jdo filter pushdown cannot be used
jdofilter
execute listtablenamesbyfilter
filter specify jdoql filter
key value class
do execute query for listtablenamesbyfilter
do retrieve all object for listtablenamesbyfilter
execute removeunusedcolumndescriptor
successfully delete cd in removeunusedcolumndescriptor
attempt add guid for the metastore db
find guid
return guid metastore db
execute listroles
do retrieve all object for listroles
execute listmsecurityprincipalmembershiprole
retrieve all object for listmsecurityprincipalmembershiprole
do retrieve all object for listmsecurityprincipalmembershiprole
execute listallrolenames
find new revoke privilege be synced
no new revoke privilege be require be synced
find new grant privilege be synced
no new grant privilege be require be synced
execute listrolemembers
do retrieve all object for listrolemembers
execute listprincipaldbgrants
do retrieve all object for listprincipaldbgrants
execute listprincipalalldbgrant
do retrieve all object for listprincipalalldbgrant
do retrieve all object for listprincipalalldbgrant
execute listalltablegrants
do execute query for listalltablegrants
do retrieve all object for listalltablegrants
execute listtableallpartitiongrants
do retrieve all object for listtableallpartitiongrants
execute listtableallcolumngrants
query obtain object for listtableallcolumngrants finish
retrieveall on all the object for listtableallcolumngrants finish
transaction run query obtain object for listtableallcolumngrants
do retrieve object for listtableallcolumngrants
execute listtableallpartitioncolumngrants
do retrieve all object for listtableallpartitioncolumngrants
execute listpartitionallcolumngrants
do execute query for listpartitionallcolumngrants
do retrieve all object for listpartitionallcolumngrants
execute listdatabasegrants
do retrieve all object for listdatabasegrants
execute listpartitiongrants
do execute query for listpartitiongrants
do retrieve all object for listpartitiongrants
execute listalltablegrants
do retrieve all object for listalltablegrants
execute listprincipalpartitiongrants
do retrieve all object for listprincipalpartitiongrants
execute listprincipaltablecolumngrants
do retrieve all object for listprincipaltablecolumngrants
execute listprincipalpartitioncolumngrants
do retrieve all object for listprincipalpartitioncolumngrants
execute listprincipalpartitioncolumngrantsall
do execute query for listprincipalpartitioncolumngrantsall
do retrieve all object for listprincipalpartitioncolumngrantsall
execute listpartitioncolumngrantsall
do execute query for listpartitioncolumngrantsall
do retrieve all object for listpartitioncolumngrantsall
execute listprincipalalltablegrants
do retrieve all object for listprincipalalltablegrants
execute listprincipalalltablegrants
do execute query for listprincipalalltablegrants
do retrieve all object for listprincipalalltablegrants
execute listtablegrantsall
do execute query for listtablegrantsall
do retrieve all object for listprincipalalltablegrants
execute listprincipalallpartitiongrants
do retrieve all object for listprincipalallpartitiongrants
execute listprincipalpartitiongrantsall
do execute query for listprincipalpartitiongrantsall
do retrieve all object for listprincipalpartitiongrantsall
execute listprincipalpartitiongrantsall
do execute query for listprincipalpartitiongrantsall
do retrieve all object for listprincipalpartitiongrantsall
execute listprincipalalltablecolumngrants
do retrieve all object for listprincipalalltablecolumngrants
execute listprincipaltablecolumngrantsall
do execute query for listprincipaltablecolumngrantsall
do retrieve all object for listprincipaltablecolumngrantsall
execute listprincipaltablecolumngrantsall
do execute query for listprincipaltablecolumngrantsall
do retrieve all object for listprincipaltablecolumngrantsall
execute listprincipalalltablecolumngrants
do retrieve all object for listprincipalalltablecolumngrants
begin execute ispartitionmarkedforevent
do execute ispartitionmarkedforevent
begin execute markpartitionforevent
do execute markpartitionforevent
do execute gettablecolumnstatistics with status
do execute gettablecolumnstatistics with status
partnames and or colnames be empty
partnames and or colnames be empty
the current metastore transactional partition column statistic for
the current metastore transactional partition column
begin execute cleanupevents
do execute cleanupevents
begin execute addtoken
do execute addtoken with status
begin execute removetoken
do execute removetoken with status
begin execute gettoken
do execute gettoken with status
begin execute getalltokenidentifiers
do execute getalltokenidentifers with status
begin execute addmasterkey
do execute addmasterkey with status
begin execute updatemasterkey
do execute updatemasterkey with status
begin execute removemasterkey
do execute removemasterkey with status
begin execute getmasterkeys
do execute getmasterkeys with status
find expect hm version
debug dump stack trace not an exception
getschemaversionsbycolumns go execute query
with parameter
runtimestat
iscurrentstatsvalidforthequery with stats write id query writer params
catch jdo exception exclusive
replication metric deletion disable
schedule execution retention disable
schedule execution time out mark disable
find include within give projection field
version
no stats data find for tblname partnames colnames
aggregate column stats thread used
time for aggr col stats in seconds thread used
new columnstats size but old columnstats size
tablepath partcols
currpath
convert type
merge statistics aggregatecolstats newcolstats
use bitvector merge column ndvs be
merge statistics aggregatecolstats newcolstats
merge statistics aggregatecolstats newcolstats
use bitvector merge column ndvs be
merge statistics aggregatecolstats newcolstats
use bitvector merge column ndvs be
merge statistics aggregatecolstats newcolstats
use bitvector merge column ndvs be
merge statistics aggregatecolstats newcolstats
use bitvector merge column ndvs be
merge statistics aggregatecolstats newcolstats
merge statistics aggregatecolstats newcolstats
use bitvector merge column ndvs
all the bite vector can merge for
start extrapolation for
all the bite vector can merge for
start extrapolation for
all the bite vector can merge for
start extrapolation for
all the bite vector can merge for
start extrapolation for
all the bite vector can merge for
start extrapolation for
all the bite vector can merge for
start extrapolation for
no aggregate stats cache for
interrupt exception ignore
interrupt exception ignore
interrupt exception ignore
interrupt exception ignore
direct sql query in
value type
expect blob type but get
seed table with writeid
seed global txns sequence with
generate partexpr for partname
clean materialization rebuild lock
number materialization lock deleted
expired createdat current age expiry
partitionname
number partition not in metastore
max writeid max txnid find in partition
use single threaded version msck getpaths
use multi threaded version msck getpaths with number thread
filter specify
table have no specific require capability
external bucket table with hb capability rw
external bucket table without hb capability ro
bucket table without hivebucket capability remove bucket info from table
external unbucketed table with extread write capability rw
external unbucketed table with extread capability ro
external unbucketed table without extread write capability none
manage non acid table rw
manage acid table with insertwrite or connectorwrite capability rw
manage acid table with insertread or connectorread capability ro
full acid table with acidwrite or connectorwrite capability rw
full acid table with acidread or connectorread capability ro
full acid table without acidread write or connectorread write capability none
bucket table without hivebucket capability remove bucket info from table
add hivebucket requiredwrites
no matches accesstype
manage acid table with connectorwrite capability rw
manage acid table with managedread capability ro
manage acid table with connectorread capability ro
manage acid table without any read capability none
full acid table with connectorwrite capability rw
full acid table with connectorread acidread capability ro
full acid table without read capability ro
table belong non default catalog skip translation
number original part buckets
table have no specific require capability
table belong non default catalog skip
table managed table
processor have require capability be able create insert only table
processor have require capability be able create fullacid tables
table be create type
table belong non default catalog skip translation
transformer return table
database belong non default catalog skip translation
processor have atleast one acid write capabilities set current locationuri managedlocationuri
processor have atleast one acid write capabilities set default manage path
difflist master list
difflist
list  size
diff return full list
validatetablepaths whroot dblocation tablelocation
base encode string
message
create client
close connection metastore
abort all open txns
open txns count
open txn id
abort all open txns
send lock request
drop table
execute getvalidwriteids
create directory
create partition
start benchmark
create partition
start benchmark
add configuration resource
configuration do not exist
open kerberos connection hm
connect framedtransport
connect metastore use compact protocol
close thrift transport
not set ugi conf passed in authmethod current
set ugi conf passed in authmethod current
not set ugi conf passed in authmethod current
set ugi conf passed in authmethod current
current authmethod
sasl client callback set username
sasl client callback set userpassword
sasl client callback set realm
sasl server digest md callback set password
sasl server digest md callback set
sasl server auth id
set remoteuser
set remoteuser from enduser
could not connect the ldap server authentication fail for
connect use principal ldap url
match users
execute query with base dns
exception happen for query
user member
groupmembershipkeyfilter passes user member group
cannot find dn for group
usermembershipkeyfilter passes user member group
cache entry weight key value total
caffeine be remove
delete
purge set true not move trash
disable ssl protocol
user user belong super group supergroupname
fail get ez for non existent path
the detail are
the detail are
exception while inherit permission
return value
reconnection status for method
hmsc  open create plain authentication thrift connection
hmsc  open find delegation token create digest based thrift connection
hmsc  open could not find delegation token create kerberos based thrift connection
fail connect the metastore server uri
unable shutdown metastore client will try close transport directly
column empty or partnames empty short circuit stats eval on client side
class method duration comments getaggrcolstatsfor
select dropdatabase method for tables
drop database in per table batch manner
drop database in per db manner
class method duration comments listpartitionswithauthinfo
class method duration comments listpartitionswithauthinfo
class method duration comments listpartitionsbyexpr
class method duration comments listpartitionsspecbyexpr
class method duration comments getdatabase
class method duration comments gettable
class method duration comments gettable
class method duration comments getprimarykeys
class method duration comments getforeignkeys
class method duration comments getuniqueconstraints
class method duration comments getnotnullconstraints
class method duration comments getalltableconstraints
class method duration comments gettablecolumnstatistics
class method duration comments gettablecolumnstatistics
class method duration comments getconfigvalue
get back event
get event with id
column empty or partnames empty short circuit stats eval on client side
class method duration comments getaggrcolstatsfor
file with the same content already exists ignore
encode uri
read encode uri
move trash
remove
repl policy for database
repl policy not set for database
invalid user
invalid password for user
user successfully authenticated
set conf value use value
pick system property with value
exception when check if path
register watch for appdirs appid dagid
pathcacheeviction reason
load via loader
register via watcher
keepaliveparam
recv
retrieve pathinfo for check for correspond
jobid mapid datafile
content length in shuffle
verify request enc str hash
fetcher request verified enc str reply
ignore close channel error
ignore client socket close
shufflebuffersize path
indexcache hit mapid find
indexcache hit mapid find
indexcache miss mapid not find
buffer incref deffering an interrupt
try get kerberosinfo for
try get tokeninfo for
create reader at
setdone called close interrupt err pending
seterror called close interrupt err pending
logical table includes
start proactive eviction
include for deserializer be
add batch
write batch
from the file include be
encode reader be stop
encode reader be stop
discard disk data if any be not cached
buffer be null for
diskdata
create slice cache in addition an exist slice
process slice have
use read data with skip header line count skip footer line count
use appname
final appstate
no information find in the llap registry
potential instance start up
instance likely shutdown soon
complete process exit with
copy
copy tez libs from
config json generation take
exit successfully
call package py via
start the cluster via service api
complete process exit with
wait for
remove from pool pool size
add new ugi pool for pool size
cannot finish due source
query complete receive for
remove follow am due query complete
canfinish
taskowner hashcode
report taskkilled for non started fragment
successfully finish
localizer thread interrupt
localizer thread interrupt
token
register request for with the shufflehandler
queue future cleanup for external queryid
delete path
send heartbeat am request
receive heartbeat response from am response
counters
invoke oob heartbeat for successful attempt istaskdone
receive dagid queryid instancetype
update stats instance thread name thread id scheme
local hostname is
start load generator process on
llap daemon log initialize from in ms async
attempt schedule task canfinish current state
grace period end for the previous kill preemtping more task
add wait queue current wait queue size
eviction
wait queue full
finishable state update during registration for state update
wait queue
fragment not find
fragment guarantee state change finishable in wait queue
remove from waitqueue
remove from preemptionqueue
schedule for execution canfinish isguaranteed
preemption queue
fragment guarantee state change finishable in wait queue
receive successful completion for
receive fail completion for
task complete waitqueuesize numslotsavailable preemptionqueuesize
awaittermination shut down task executor
awaittermination shut down task executor
no display object find for operation
could not connect the ldap server authentication fail for
connect use principal ldap url
match users
execute query with base dns
exception happen for query
user member
groupmembershipkeyfilter passes user member group
cannot find dn for group
usermembershipkeyfilter passes user member group
kill query request with id
confirm unknown kill query request with id
query find with id
query find with tag
query not find with tag id
kill query with zookeeper coordination
rawstore for the thread
thread local rawstore null for the thread
add rawstore for the thread
no function metadata have be return
unexpected display object value null for operation
operation already abort in state
attempt cancel from state
no schema metadata have be returned
no primary key metadata have be returned
no type info metadata have be returned
no column metadata have be returned
no table metadata have be returned
no table type metadata have be returned
opensession
opensessionwithimpersonation
opensession
opensession
opensession
createsessionwithsessionhandle
opensession
closesession
getinfo
executestatement
executestatement
executestatementasync
executestatementasync
gettypeinfo
getcatalogs
getschemas
gettables
gettabletypes
getcolumns
getfunctions
getprimarykeys
getcrossreference
getoperationstatus
time out hence return progress log null
canceloperation
closeoperation
getresultsetmetadata
fetchresults
getqueryid
use the random number the secret for cookie generation
client ip address
client username
invalid cookie
validate the cookie for user
no valid cooky associate with the request
receive cookies
cookie name value
http auth header
url query string
client ip address
client username
proxy user from query string
proxy user from thrift body
verify proxy user
xsrf filter enable
global init file do not exist
set proxy user name base on query param to
signature generate for
signature generate for inside verify
set output service
output service
tear down service
tear down complete
socket connect
data write
have record writer
have record reader
socket connect
data write
not found return default
read record from
start minortablewithbase
not the delta file you be look for
start minorwithopeninmiddle
start minorwithaborted
not the delta file you be look for
start minortablewithbase
not the delta file you be look for
start majortablewithbase
not the file you be look for
start majorpartitionwithbase
not the file you be look for
start majortablenobase
not the file you be look for
start majortablelegacy
not the file you be look for
start minortablelegacy
not the file you be look for
start majorpartitionwithbasemissingbuckets
not the file you be look for
start majorwithopeninmiddle
start majorwithaborted
read record
read fully costs millisecond
read record
read fully costs millisecond
join
join
start testunlockwithtxn
start deadlock test
no exception no deadlock
get an exception but not deadlock sqlstate
force deadlock sqlstate class
no exception no deadlock
get an exception but not deadlock sqlstate
force deadlock sqlstate class
fail bind zk server client port
unregistering
buffer after cache
register socket for
request proactive eviction for entity in database
proactively evict byte
proactive eviction free byte on llap daemon in total
initialize for input
user jar set
add original file dirs search
add base directory dirs search
add delta directory search
read length
read numelements
read file length
read bucket number
read base path length
return split
add list file for split
move content
remove
cleaner thread finish one loop
clean base on writeidlist
go delete path
go delete path
determine who run the job as
run job
unable stat file current user try table owner
run job
no exist stats found will not run analyze
heartbeating compaction transaction id for table
successfully stop the heartbeating the transaction
process compaction request
validcompactwriteidlist
find potential compactions
find too many abort transaction for
find an abort transaction for
not enough delta initiate compaction for table partition
request major compaction for mm table find deltas threshold
find delta files no have base
go delete directory for abort transaction for mm table
releasereader entry readercount
addreader entry readercount added
queryresultscache lookup for query
queryresultscache lookup result
table changed at
check writeids for table currentwriteidfortable cachedwriteidfortable
cache query no longer valid due table
cache entry size larger than max entry size
handle event on table
cache not instantiated skip event on
convert local
find table in
find in
process table
convert external table
set owner group
set perm
look for database
find database process
find table in
process table
rename
rename
write crud conversion command
write manage table conversion command
translate operator rel
generate with row schema
translate operator rel
generate with row schema
generate with row schema
translate operator rel
generate with row schema
generate with row schema
translate operator rel
generate with row schema
translate operator rel
generate with row schema
translate operator rel
generate with row schema
translate operator rel
genudtfplan col aliases
translate operator rel
operator rel
operator rel
operator rel
generate with row schema
old rexcall
new rexcall
original plan for planmodifier
plan after top level introducederivedtable
plan after nest convertoptree
plan after propagate order
plan after fixtopobschema
final plan after modifier
stats for column
stats for column
original plan cost optimize plan cost
plan after
only plan where root have one input be supported root
some project field lineage can not be determine
none the table have key projected unable join back
nothing be trim out
join back table
lineage expression in node can not be determined
unknown expression should be constant
match hivesemijoinrule
all condition match for hivesemijoinrule go apply transformation
unexpected situation give on branch
get col stats for in
jdbcsortpushdownrule have be call
jdbcaggregationpushdownrule have be call
jdbcjoinpushdownrule have be call
jdbcexpandexpressionsrule filtercondition have be call
jdbcexpandexpressionsrule joincondition have be call
jdbcexpandexpressionsrule projectionexpressions have be call
jdbcprojectpushdownrule have be call
jdbcfilterpushdown have be call
rexover operator push down not support for now with the follow operator
jdbcunionpushdown have be call
start match hiveantijoinrule
match hiveantijoinrule
materialize view
materialize view
materialize view
materialize view
trigger countdistinct rewrite numcountdistinct
exception in close
identify primary foreign key relation from constraints row count for join
identify primary foreign key relation
no primary foreign key relation row count for join
identify primary foreign key relation
find no filesinkoperation can be present executionid
find acid sink
bail out sort dynamic partition optimization dynamic partition context null
bail out sort dynamic partition optimization list bucket enable
bail out sort dynamic partition optimization destination table null
bail out sort dynamic partition optimization destination materialize view
bail out sort dynamic partition optimizer all dynamic partition
bail out sort dynamic partition optimization some partition column
get sort order
sort position
sort order
sort null order
memory info during sdpo opt
get constant op with rs
constant op not find
constant op
offer constant operator
combine two limit child parent newlimit
push through
generate new predicate with in clause
full outer mapjoin only column expression be support
full outer mapjoin not enabled
full outer mapjoin not enabled
full outer mapjoin not enabled
full outer mapjoin not enabled
full outer mapjoin not enabled multiple join condition not support
full outer mapjoin not enabled
full outer mapjoin not enabled
full outer mapjoin enabled
full outer mapjoin not enabled only tez engine support
full outer mapjoin not enabled
full outer mapjoin enable
generate new predicate with in clause
partition column not separate for not in operator
partition column not separate for child size
partition column not separate for
partition column not separate for
mapjoin size remaining
skip setruntimestatsdir for
skip annotateruntimestats for
look at
no top operator
look for table scan where optimization applicable
find null table scan
serializing
serializing
disable hybrid grace hash join in case llap
check
check
cannot run operator in llap mode
not use skew join because the destination table
not use skew join because the destination table an insert only table
ignore vectorization
datacolumncount
includecolumns
partitioncolumncount
datacolumns
scratchcolumntypenames
neededvirtualcolumns
use reduce tag
vectorizeoperator
vectorizeoperator
find where false tablescan
find limit tablescan
set forward data
set forward data
set forward data
set fix parallelism
set fix parallelism
threshold exceed for pseudomr mode
threshold exceed for pseudomr mode
threshold exceed for pseudomr mode
before sharedworkoptimizer
sort table by size
after sharedworkoptimizer
after sharedworkextendedoptimizer
after sharedworksjoptimizer
after sharedworkoptimizer merge schema
after dppunion
skip have already be remove
skip have already be remove
can merge into remove scan on
cannot be merge
do not meet precondition
cannot be merge
do not meet precondition
cannot be merge
do not meet precondition
merge subtree start at into subtree start at
merge into
input operator removed
remove probedecodecntx for merge op
operator removed
downstream merge from into
sort operator by size
skip have already be remove
skip have already be remove
cannot be merge
check additional condition for merge subtree start at
do not meet precondition
merge subtree start at into subtree start at
input operator removed
operator removed
operator removed
dpp information store in the cache
operator not equal
accumulate data size max size
merge would violate dag property
minimum reduction for hash group by operator set
identity project remover optimization remove
already process reduce sink
not evaluate lineage
time take for lineage transform
parent
filter
tablescan
alias
prune partition list
no partition prune necessary
column not partition column
initiate semijoin reduction for
disable semijoin optimzation on since an external table
join key from which an external table disable semijoin optimization
key expr
partition key expr
no columninfo find in for
set size for base on the hint
dynamicsemijoinpushdown save mapping
connect with
clone reduce sink for multi child broadcast edge
add dummy op work from mj work
operator tree could not be clone
operator tree not properly cloned
prune partition name take
filter compacting
skip default bad partition
retain unknown partition
tablepropertyenrichmentoptimizer consider these serde classes
original table parameters
serde init succeed for class
resolve change parameters key value
skip prefetch for
bucket version for set
unsupported type
unsupported cast
cast type
function undeterministic do not evaluate immediately
fold expression
function undeterministic do not evaluate immediately
fold expression
fold expression
fold expression
filter identify value assignment propagate it
filter identify value assignment propagate it
replace column with constant in
old filter fil conditions
filter expression hold true will delete it
new filter fil conditions
new column list
stop propagate constant on op
skip fold in outer join
skip fold in distinct subqueries
skip join fil    rs structure
push through
push through
push copy through
remove above
push copy through
remove above
push copy through
remove above
not push through non fk side the join filter
push copy through
remove above
remove above same operator
remove since child supersede
remove parent since supersede
remove parent have same key but lower topn
remove parent have same key but lower topn
number nod number executor per node
conversion bucket map join failed
external table find in join disable smb join
find correlation optimizer operators cannot convert smb at time
smb join can not be perform due bucket version mismatch
external table find in join disable bucket map join
no big table selected no mapjoin
condition convert mapjoin be not meet
find semijoin optimization from the big table side map join which will cause task cycle
real big table reducer
estimate ndv for input max ndv for mapjoin conversion
number different entry for hashtable greater than the max
estimate size for input max size for dphj conversion
size input greater than the max
reduce sink operator key
stats
fail retrieve stats
stats
stats
stats
stats
estimate row count for original num rows
estimate row count for original num rows
stats inputsize maxsplitsize
stats hashagg
stats ndvproduct become some column do not
case stats cardinality
case stats cardinality
case stats cardinality
case stats cardinality
case stats cardinality
case stats cardinality
case stats cardinality
case stats cardinality
case stats cardinality
stats
stats
stats
stats detect none multiple pk parents
stats pk parent id
stats fk parent id
stats overflow in number rows
stats equal in number rows
unhandled join type in stats estimation
stats
stats
stats
stats
stats
use runtime stats for
mapjoin pos
connect with
check map join optimization for operator use stats
find big table branch with parent operator position
cannot enable map join optimization for operator
skip sink for now have not see all it parents
already process reduce sink
sibling have stats
table source have stats
gather stats for sink total size
set parallelism for sink
add table
information add for path
can not pre create table for ctas
can not pre create table for mv
configure hook
configure hook
check privilege for multiple file match
check privilege for parent path for nonexistent
check privilege for path itself originally specify
addprivilegesfromfs ask for privilege on with recurse obtained
hivemetastoreauthorizer onevent eventtype
hivemetastoreauthorizer onevent eventtype
hivemetastoreauthorizer filterdatabases
hivemetastoreauthorizer filterdatabases
hivemetastoreauthorizer filtertablenames
hivemetastoreauthorizer filtertablenames
hivemetastoreauthorizer filtertables
hivemetastoreauthorizer filtertables
hivemetastoreauthorizer filterdatabaseobjects
hivemetastoreauthorizer filterdatabaseobjects
hivemetastoreauthorizer buildauthzcontext eventtype
hivemetastoreauthorizer buildauthzcontext eventtype ret
hivemetastoreauthorizer checkprivileges authzcontext authorizer
hivemetastoreauthorizer checkprivileges authzcontext authorizer
hivemetastoreauthorizer skipauthorization
hivemetastoreauthorizer skipauthorization
databasefiltercontext getoutputhobjs
databasefiltercontext getoutputhobjs ret
tablefiltercontext getoutputhobjs
tablefiltercontext getoutputhobjs ret
alterdatabaseevent getoutputhobjs
alterdatabaseevent getoutputhobjs ret
droptableevent getinputhobjs
droptableevent getinputhobjs ret
addpartitionevent getoutputhobjs
addpartitionevent getoutputhobjs ret
alterpartitionevent getinputhobjs
alterpartitionevent getinputhobjs
alterpartitionevent getoutputhobjs
alterpartitionevent getoutputhobjs
createtableevent getoutputhobjs
createtableevent getoutputhobjs ret
droptableevent getoutputhobjs
droptableevent getoutputhobjs ret
dropdatabaseevent getinputhobjs
dropdatabaseevent getinputhobjs ret
readdatabaseevent getinputhobjs
readdatabaseevent getinputhobjs ret
altertableevent getinputhobjs
altertableevent getinputhobjs ret
altertableevent getoutputhobjs
altertableevent getoutputhobjs ret
readtableevent getinputhobjs
readtableevent getinputhobjs
droppartitionevent getinputhobjs
droppartitionevent getinputhobjs ret
createdatabaseevent getoutputhobjs
createdatabaseevent getoutputhobjs ret
process
process
unable synchronize
configure jobconf for table
ignore mark with operatorstats incorrectruntimestatsmarker
create stag dir for path
delete result cache dir
skip delete stagingdir
delete result dir
delete result file
pattern
ddltask alter database skip database newer than update
functiontask drop function skip database newer than update
functiontask create function skip database newer than update
ddltask truncate table partition skip table partition newer than update
ddltask alter table skip table newer than update
ddltask get data for
ddltask write data for
find table match the show table statement
find table match the show extend table statement
pattern
ddltask drop table skip table newer than update
wrong specification
ddltask rename partition skip table partition newer than update
create table on
ddltask create table skip table newer than update
ddltask alter table skip table newer than update
ddltask alter table skip table newer than update
invalidobjectexception
ignore request add because present
find class for
find view match the show view statement
ddltask create view skip view newer than update
exception during materialize view cache update
find materialize view match the show materialize view statement
rebuild materialize view
after ppd
synthetic predicate in
additional synthetic predicate in
non equi join predicate
process for
process for
process for
process for
process for
process for
process for
no pushdown possible for predicate
original predicate
after ppd
replace setcolref with allcolref because could not find the root insert
replace setcolref with allcolref because could not find the from
replace setcolref with allcolref because could not find the select
replace setcolref with allcolref because could not find the query
replace setcolref with allcolref because nest allcolref
replace setcolref with allcolref because the nest node
skip create child column reference because regexp use alias
replace setcolref with allcolref because duplicate alias
propagate hint qb
propagate hint qb
replace produced by cbo by
translate the follow plan
plan before remove subquery
plan just after remove subquery
plan after decorrelation
cbo plan details
original plan
plan after ppd partpruning columnpruning
plan after join reordering
user do not have privilege use materialize view
dialect for table
can not find column in the error msg
handle query hints
find udtf
udtf table alias
udtf col alias be
table alias col aliases
because it
create plan for query block
task summary
replicationsemanticaanalyzer analyzeinternal
replicationsemanticanalyzer analyzeinternal dump
replicationsemanticanalyzer analyzeinternal load
replicationsemanticanalyzer analyzeinternal status
contain an incremental dump
contain an bootstrap dump
replicationsemanticanalyzer analyzereplstatus write repl last id out use configuration
preparereturnvalues
table location
partition have data location
file delete before rename
modify encode uri
schedule partition dump
thread start partition dump
thread finish partition dump
processing add foreignkey message message
processing add notnullconstraint message message
event be create table event with no table list
processing add primarykey message message
event be add ptn event with no table list
event be an add ptn event with no partition
event be an event type with no table list
table do not satisfy the policy
processing add defaultconstraint message message
processing add uniqueconstraint message message
writeeventsinfolist will be remove from commit message because be
writeeventsinfolist will be remove from commit message because be
payload for commit txn event
processing add checkconstraint message message
stage start
stage end
stage end
stage end
stage progress
metric sink initialise with frequency
update metric db
convert metric thrift metric
metric be persist
persist metric db
no metric update
bucket id for file
root operator
leaf operator
process map join
add dummy ops work
connect with
clone reduce sink for multi child broadcast edge
remove parent from
second pass leaf operator
first pass leaf operator
connect union work with work
component
operator
cycle found remove semijoin
add special edge
add special edge from terminal op semijoin edge
add special edge
there be app master events
handle appmastereventoperator
skip null scan query optimization
skip metadata only query optimization
skip cross product analysis
skip llap pre vectorization pas
skip vectorization
skip stage id rearranger
skip llap decider
semijoin optimization find go smb join remove semijoin
expectedentries
insufficient row justify semijoin optimization remove semijoin
remove redundant
semijoin optimization with union operator remove semijoin
semijoin optimization with parallel edge map join remove semijoin
probedecode mj for with cachekey mj po colname with ratio
probedecode mj with ratio
probedecode mj ratio lower than exist mj with ratio
compute key domain cardinality keydomaincardinality
bloomfilter selectivity for selkeycardinality
no stats available compute bloomfilter benefit
bloomfilter benefit
netbenefit
compute bloomfilter cost benefit for
unwrap column expression from exprnodefielddesc
add
add for re iteration
old stats for
number row reduction
new stats for
reduction factor not satisfy for
ndvs ndvsofts ndvsoftsfactored
catch npe in marksemijoinfordpp from reducesink
cbo could not obtain udaf evaluator for
set error from
tablemask create
add reduce work for
set reduce sink
add map work for
set dummy ops for work
resducesink tablescan
connect baswork
remove reducesink tablescan
remove appmastereventoperator tablescan
buffer size for output from operator can be set mb
skip optimize operator plan for analyze command
default keyword replacement insert for table
default keyword replacement insert for table
query hint
create filter plan for row schema
create filter plan for row schema
translate
create select plan for clause
tree
udtf table alias
udtf col alias be
genselectplan input starrr
create select plan row schema
add default value from metastore
numbuckets maxreducers
create filesink plan for clause dest path row schema
direct insert for acid table enabled
set stats collection dir
create limitoperator plan for clause
table alias col aliases
generate join with post filtering condition
semijoin hint parsed
rr before gb
create body plan for query block
create table plan for
set stats collection dir
create plan for query block
table not find in walkastmarktabref
before logical optimization
after logical optimization
validate
not validate writeentity because entity neither table nor partition
exception when validate folder
create ptf plan
root operator
leaf operator
process map join
connect with
clone reduce sink for multi child broadcast edge
remove parent from
second pass leaf operator have common downstream work
work already connect before
first pass leaf operator
add reduce work for
set reduce sink
add map work for
component
operator
add special edge
skip runtime skew join optimization
skip null scan query optimization
skip metadata only query optimization
skip cross product analysis
skip vectorization
skip stage id rearranger
skip combine equivalent work optimization
exception while shut down the task runner
exception while clear the fetch task
exception while clear the context
exception while close the resstream
exception while clear the fetchtask
find spec for from
the table property is
initialize the teradatabinaryrecordreader
no table property in jobconf try recover the table directly
the current alias
the start the file split is
the end the file split is
include the column
combinehiveinputsplit pool already create for
number split
the receive input path are
exception in close
in directory base delta
getchildstate ignoring
fail get file with id
set validwriteidlist
stats updater for do not have write id
israwformat call on which not an orc file
cannot extract write id for mm table
cannot extract write id for mm table
add lock component lock request
output null
add lock component lock request
dircache get initialize already
cache entries
dircache not enable
writeidlist from cache not match for key
process file
not the name support class
avro schema
use read data with skip header line count skip footer line count
serialize tail not cache for path
meta info for changed cachedmodificationtime
ppd add split
no orc pushdown predicate no column name
no orc pushdown predicate
no orc pushdown predicate no column name
no orc pushdown predicate
blob storage detect for bi split strategy split file at boundary
update split from
generate split schema evolution property
split strategy
projected columns uncompressed size
getreader read validwriteidlist
create merger for
eliminate orc stripe file
dynamic value be not available here
use schema evolution configuration variable schema evolution columns
use column configuration variable column
key maxkey
key maxkey
look at delta file
final reader map
orc schema
close on abort for path deleting
close writer for path acid stats
empty file have be create for overwrite
no insert event in path deleting
initialize writer before close to create empty buckets for path
close delete event writer for path acid stats
no delete event in path deleting
close delete flush length file for path
getsplits start
getsplits finish
sarg translate into
orc schema
findminmaxkeys false
findminmaxkeys original split
findminmaxkeys no orc column stats
use sortmergeddeleteeventregistry
num event stats
num event stats
use columnizeddeleteeventregistry
skip delete delta dir
number delete events limit actual
findminmaxkeys no orc column stats
batch have no data for
columnindex columntype streambuffers length vectors columnencoding
result disk range read file
disk range after cache found everything file
nothing read for stripe
disk range after pre read file base offset
use llap memory manager for orc writer memperexecutor maxload totalmempool
get recycle compressor
get recycle decompressor
get override parquet block size property via tblproperties
get override parquet enable dictionary property via tblproperties
get override compression property via tblproperties
turn off filter due
not push filter because filterexpr null
all row group be drop due filter predicate
drop row group do not pas filter predicate
parquet predicate push down generated
no parquet predicate push down generated
use vectorized record reader
use row mode record reader
page size byte record
read repetition level at
read definition level at
read data at
page data size byte record
in
process
check against
find spec for from
aliases pathtoaliases dir
pushdown initiate with filtertext filterexpr
match for
no lock need for queryid
no lock need for queryid
add global lock
commit txn
roll back
no send heartbeat there no transaction no locks
start heartbeat with delay interval
allocate write id for
start heartbeat for materialization rebuild lock for with delay interval for query
heartbeating   for currentuser
stop heartbeat for materialization rebuild lock for for query
request lock
start retry attempt acquire lock for lockid queryid
unlock
remove lock
remove lock
about release lock for
acquire lock for with mode
possibly transient zookeeper exception
request lock
conflict lock
node or it parent have already be deleted
node be delete not empty
add list lock input
add list lock output
acquire lock for with mode
acquire lock for with mode
interrupt exception ignore
interrupt exception ignore
set
encode valid txns info txnid
eventualcleanupservice already running
pathcleaner be interrupt
signature checked
receive null query plan
not log event operation type
event per file enabled new proto event file
shouldreexecuteaftercompile
shouldreexecute
mapjointable null
aborting skip dump side table for tag
clean the operator state
look genericudaf
task summary
throttle url
no longer need
create
set hadoop user name
set env
initializeoperators child
create slot for
the follow path have be add the deletedeltas list
aborted closing
aborted closing
create updater for bucket number use file
parse the attemptid from the task id
add alias work list for file
dump
process alias es for file
appmasterevent
plan path
find plan in cache for name
local path
load plan from string
open file read in plan
no plan file found
taskid for
shouldavoidrename false therefore moving renaming
skip rename move files file be keep are
ctas create mv file be renamed
final renaming moving source destination
move from
filename taskid copysuffix
hive conf not find or session not initiated use thread base class loader instead
session specify class loader not found use thread base class loader
fail close filesystem
cannot get size safely ignored
content summary cache for length num files
content summary not cache for
add input file
create dirs with permission recursive
hdfs dir with schema permission
initialization done
initialization do reset
operator initialized
initialize children
initialize child
allinitializedparentsareclosed parent state
close call for operator
not all parent operator be closed not closing
close child
close done
set trait on
set stats on
key size
key exprnodedesc
export data to
url export policy from source ranger
response receive for ranger export
ranger policy export request return empty list
ranger policy export request return empty list or failed please refer ranger admin logs
url import policy on target ranger
ranger policy import finish successfully
ranger policy export filepath
create stag dir for path
add dependent repltxntask copywork movework for table
external table data location is
add dependent copywork addpart movework for table
add dependent copywork for partition
function directory
process
file list back by can be use for write operation
event with total root number tasks
write entry file list back by
close the file list back file
finish dump atlas metadata total byte write
current timestamp is
store metadata for atlas dump at
preparereturnvalues
table list file not create for db level replication
dump db
dump table db root
add table external table list
error while close reader
add task for
add task for
add task for
current incremental dump have table be bootstrapped switch bootstrap
add task set last repl id db
createrequest
atlas getqualifiedname
atlas metadata import request
exportdata
atlas import data request
retry method
add task for
no lock release because hive concurrency support not enable
no lock find release
no file find move from
the statementid use when load the dynamic partition
the statementid use when load the dynamic partition
newtag childindex oldtag
no input row from fill dynamic value with null
no local resource process other than hive exec
add local resource
daginfo
set tez dag access for queryid with viewaclstring modifystr
publish tez counters
add split src new group
close an unneeded return session
close an unneeded session
not add with hostname
add with hostname
use unmanaged session wm not initialize
adjust abort check row
additional work
set the llap fragment id for
start output
there be key value reader for input
progresshelper initialized
scheduleprogresstaskservice called
shutdownprogresstaskservice called
run task
validate trigger against currentcounters
mark mapwork input uri need credentials
kafka credential already added skipping
get kafka credential for brokers
jaas config for request kafka credentials
collect file sink uris for topnodes
mark output uri need credential filesink
tezdir path set for user
find in cache
find in cache
find in cache
cache new object for key
remove key
kill
process killquery for
kill query succeeded return the pool
kill query failed restarting
process change for pool
will destroy instead restart
estimate column projection size
estimate column projection size
convert partition value original
part key expr applied
type obj insp
adding list require partition
request dag status checkinterval currentcounters
session expiration enabled session lifetime
see if can expire
not ready expire add back
wait for expire
expiration queue empty
add pool session expiration queue
close pool session
fail get value for counter approximate input records
split not filesplit use default location
map index location
remapped index location
path file split map for input name
group splits available slots
the destination file name for
get stats through replication for
read date value day since epoch
read timestamp value second since epoch
repltxntask event skip already replayed event id
repltxntask event skip already replayed event id
not bucket map join so cache
partition key th
add call with object
add call with object
probe slot the longest so far find space
task get execute use mapred tag
replcopytask execute
replcopytask files contains null
replcopytasks srcs null
replcopytask cp
replcopytask numfiles
path clean before rename
replcopytask filesinfilelisting read
replcopytask files do not exist
replcopytask filesreadline
replcopytask getloadcopytask
replcopytask  trcwork
replcopytask  tcwork
fetchoperator set writeidstr
create fetchtask with deserializer typeinfo
deserializer properties  ntable properties
no valid directory for
no split for
fetchoperator get writeidstr
mapjointables null
close topnkeyfilter
batch repeat joinresult
batch non repeated
batch repeat joinresult
batch non repeated
batch repeat joinresult
batch
batch
batch
finishouter spillcount
finishouter spillcount
finishouter spillcount
finishouter allmatchcount
finishouter nomatchcount batch size
finishouter nomatchcount batch size
finishouterrepeated batch batch size somerowsfilteredout
processop all
batch repeat joinresult
batch non repeated
currentkey
vectormapjoinfasttablecontainer load newthreshold
probe slot the longest so far find space
probe slot the longest so far find space
vectormapjoinfastlonghashtable add key slot pairindex empty slot
vectormapjoinfastlonghashtable add key slot pairindex find key
probe slot the longest so far find space
vectormapjoinfastlonghashtable add slot hashcode
probe slot the longest so far find space
vectormapjoinfastlonghashtable expandandrehash key slot newpairindex empty slot
fail get value for counter approximate input records
probe slot the longest so far find space
probe slot the longest so far find space
vectormapjoinfastkeystore add keylength absolutekeyoffset keyrefword
vectormapjoinfastkeystore equalkey match on byte
batch repeat joinresult
batch non repeated
outputcolumnnames
addprojectioncolumn columnname
vectormapjoincommonoperator initializeop currentscratchcolumns
vectormapjoincommonoperator initializeop field type
vectormapjoincommonoperator initializeop overflowbatch outputcolumn class
vectormapjoincommonoperator commonsetup column count
vectormapjoincommonoperator commonsetup column type null
batch repeat joinresult
batch non repeated
reloadhashtable
reprocessbigtable enter
reprocessbigtable exit row process batch process
vectormapjoininnerlongoperator closeop batch process
generatehashmultisetresultsinglevalue enter
generatehashmultisetresultsinglevalue with big table
generatehashmultisetresultmultivalue allmatchesindex duplicatecount count
batch repeat joinresult
batch non repeated
batch repeat joinresult
batch non repeated
vectormapjoinoptimizedlongcommon serialize key hashtablekeytype hex
batch repeat joinresult
batch non repeated
processop all
batch repeat joinresult
batch non repeated
currentkey
batch repeat joinresult
batch non repeated
batch repeat joinresult
batch non repeated
batch repeat joinresult
batch non repeated
batch empty
batch empty
batch empty
getvectorexpressionclass udf descriptor
getvectorexpressionclass do not match
bytescontainer create temp file
will try use the vectorudfadaptor for
input expression
no vector udf find for descriptor
cast constant scalar hivedecimal result in null
use hash aggregation process mode
flush do not progress entry before entry after
gby memory limit isllap maxmemory fixsize key agg
totalaccesscount numentries avgaccess
gc canary cause flush
checkhashmodeefficiency ht rc min
limit reached currcount currentcountforalltasks
close topnkeyfilter
shut down the sparkcontext
convert partition value original
part key expr applied
type obj insp
start spark job with job handle id
fail submit spark job with job handle id
kill spark job with job handle id
clean small table cache for query
log job configuration
retry create tmp result cache directory
fail close inputstream
load rpc property from hive configuration
new session created
close spark session
return record reader for path
count true values
count false values
count null values
initialize genericudfgetsplits
do initialize genericudfgetsplits
get schema for query
initialize genericudtfgetsqlschema
initialize conf jc metastore connection
do initialize genericudtfgetsqlschema
basemaskudf initialize
basemaskudf initialize
initialize genericudfgetsplits
do initialize genericudfgetsplits
initialize conf jc metastore connection
numevents numsplits
encode valid txn write id info txnid
shut down query
there no correspoing stattask for
create new db db needsrefresh
close current thread connection hive metastore
materialize view
materialize view
materialize view
unable determine if materialize view
materialize view
no new file be created not replace or be insert into
add new partition
catch alreadyexistsexception try alter partition instead
catch alreadyexistsexception try add partition one by one
no file add by query in
list file under
alter exist partition
number partitionstoadd be add
cancel dynamic load task
clear table column statistic set basic statistic false for
create partition for table
catch already exist exception try alter partition instead
catch jdo exception try alter partition instead
alter partition for table with partition spec
write notification log ignore dml event log disable
write notification log ignore temporary
write notification log ignore file list empty
add write notification log for operation table
fire dml insert event
not fire dml insert event temporary
move src dest
the source path null for issubdir method
the source path the destination path
the source file in the local while the dest not
the source path schema
the source path the destination path
the path delete
copy source because hdfs encryption zone be different
acid move look for original bucket in
acid move find original bucket
acid move look for file in bucket
acid move find file
acid move find bucket file
materialize view
create materialize view for rewriting
materialize view
materialize view refreshed
sd
materialize view add registry
refresh materialize view
materialize view remove from registry
materialize view remove from registry
find materialize view in registry
materialize view not find in registry
no materialize view with similar query text find in registry
materialize view find with similar query text find in registry
non positive poll interval configured notification event poll disable
no event consumer configured notification event poll disable
poll for notification event
event
process notification event
no current sessionstate skip temp table for
no current sessionstate skip temp table for
stats collection wait for threadpool shutdown
stats collection threadpool shutdown successful
stats request be reliable empty stats found
collectors size
update stats for
update stats for
bulk update partition
update stats for
wait for all stats task finish
stats collection thread pool do not terminate
read runtime statistic for tez vertex task
unable get statistic for vertex opid groupname
estimate average row size
cancel file stats lookup task
finish get file stats all partitions
get file stats all partitions threadpool size
column stats request for columns able retrieve for column
column stats request for partitions able retrieve for partition
stats for column in table could not be retrieve from cache
stats for column in table retrieve from cache
partition path
materialize table do not contain table statistic
stats for column
stats overflow in number rows
stats equal in number rows
process
process table
skip table since be replicate into
column update be exist only out of base on
column update be for all partitions individual partitions
add analyze work for
process partition in batch
update base on
run base on
initing fsstatspublisher with
connect
create file
write stats in
about delete stats tmp dir
because infinite or nan skip stats
sessionstate user
update thread name
reset thread name
session use authorization class
remove resource dir
clear hadoop reflectionutils constructor cache
the null byte for each record will have byte
create json tree from object tree
reset already initialize avroserde
avroserde  initialize preset value avro schema literal
avro schema
configuration null not insert schema
columncomments
check for hv
return cache result
create new instance store in cache
add new valid rrid
get struct field data for field on data
deserializing struct
return lazy map for field
return for field
retrieve writer schema
retrieve reader schema
data not in the float data type range so convert null give data
data not in the float data type range so convert null
data not in the
data not in the data type range so convert null
data not in the double data type range so convert null give data
data not in the double data type range so convert null
initialize with columnnames
data not in the hivedecimal data type range so convert null give data
data not in the
data not in the data type range so convert null
data do not contain only base character so return original byte array
data not in the hivevarchar data type range so convert null
data not in the hivedecimal data type range so convert null give data
data not in the hivechar data type range so convert null
initialize with columnnames
find node result
use version guava stats collection enable by default
evict client
clean hive client cache in shutdown hook
try delete expire proto event from
